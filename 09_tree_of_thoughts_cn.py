#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
# ğŸ“˜ æ™ºèƒ½ä½“æ¶æ„ 9ï¼šæ€ç»´æ ‘ï¼ˆTree-of-Thoughtsï¼‰è§„åˆ’

è¿™ç§æ¨¡å¼å°†æ™ºèƒ½ä½“çš„é—®é¢˜è§£å†³èƒ½åŠ›ä»çº¿æ€§æ€ç»´é“¾æå‡åˆ°å¤šè·¯å¾„æ¢ç´¢æœç´¢ã€‚

ä¸ç”Ÿæˆå•ä¸€ã€é¡ºåºçš„æ¨ç†è·¯çº¿ä¸åŒï¼Œæ€ç»´æ ‘æ™ºèƒ½ä½“åœ¨é—®é¢˜çš„æ¯ä¸ªé˜¶æ®µéƒ½ä¼šç”Ÿæˆå¤šä¸ªå€™é€‰"æ€è·¯"æˆ–ä¸‹ä¸€æ­¥ã€‚
ç„¶åå®ƒè¯„ä¼°è¿™äº›æ€è·¯ï¼Œä¿®å‰ªæ— æ•ˆæˆ–æ²¡æœ‰å‰é€”çš„åˆ†æ”¯ï¼Œå¹¶æ‰©å±•æœ€æœ‰å¸Œæœ›çš„åˆ†æ”¯ã€‚
è¿™åˆ›å»ºäº†ä¸€ä¸ªæœç´¢æ ‘ï¼Œæ™ºèƒ½ä½“å¯ä»¥å›æº¯ã€æ¢ç´¢æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ç³»ç»Ÿåœ°å¯¼èˆªå¤æ‚çš„é—®é¢˜ç©ºé—´ã€‚

ä¸ºäº†æ¼”ç¤ºè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†è®©æˆ‘ä»¬çš„æ™ºèƒ½ä½“è§£å†³ä¸€ä¸ªç®€å•ç›´è§‚çš„é€»è¾‘éš¾é¢˜ï¼š**æ•°å­—æ±‚å’Œè·¯å¾„é—®é¢˜**ã€‚
è¿™ä¸ªé—®é¢˜è¦æ±‚ä»æ•°å­—1å¼€å§‹ï¼Œæ¯æ¬¡åªèƒ½åŠ 1æˆ–ä¹˜ä»¥2ï¼Œåœ¨5æ­¥å†…åˆ°è¾¾æ•°å­—10ã€‚
è¿™ä¸ªä¾‹å­è™½ç„¶ç®€å•ï¼Œä½†æ¸…æ™°åœ°å±•ç¤ºäº†æ€ç»´æ ‘å¦‚ä½•å¹¶è¡Œæ¢ç´¢å¤šä¸ªè§£å†³æ–¹æ¡ˆè·¯å¾„ï¼Œå¹¶é€šè¿‡è¯„ä¼°å’Œä¿®å‰ªæ¥é«˜æ•ˆæ‰¾åˆ°æœ€ä¼˜è§£ã€‚

### å®šä¹‰
**æ€ç»´æ ‘ï¼ˆTree-of-Thoughtsï¼ŒToTï¼‰**æ˜¯ä¸€ç§æ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œå…¶ä¸­é—®é¢˜è§£å†³è¢«å»ºæ¨¡ä¸ºé€šè¿‡æ ‘çš„æœç´¢ã€‚
æ™ºèƒ½ä½“åŒæ—¶æ¢ç´¢å¤šä¸ªæ¨ç†è·¯å¾„ï¼ˆåˆ†æ”¯ï¼‰ã€‚åœ¨æ¯ä¸ªæ­¥éª¤ï¼Œå®ƒç”Ÿæˆæ½œåœ¨çš„ä¸‹ä¸€æ­¥æˆ–"æ€è·¯"ï¼Œè¯„ä¼°å®ƒä»¬çš„å¯è¡Œæ€§ï¼Œå¹¶å†³å®šç»§ç»­æ¢ç´¢å“ªäº›è·¯å¾„ï¼Œä»è€Œæœ‰æ•ˆåœ°ä¿®å‰ªæœç´¢ç©ºé—´ã€‚

### é«˜çº§å·¥ä½œæµç¨‹

1.  **åˆ†è§£**ï¼šå°†é—®é¢˜åˆ†è§£ä¸ºä¸€ç³»åˆ—æ­¥éª¤æˆ–æ€è·¯ã€‚
2.  **æ€è·¯ç”Ÿæˆ**ï¼šå¯¹äºé—®é¢˜çš„å½“å‰çŠ¶æ€ï¼Œæ™ºèƒ½ä½“ç”Ÿæˆå¤šä¸ªæ½œåœ¨çš„ä¸‹ä¸€æ­¥æˆ–æ€è·¯ã€‚è¿™ä¼šåœ¨æœç´¢æ ‘ä¸­åˆ›å»ºåˆ†æ”¯ã€‚
3.  **çŠ¶æ€è¯„ä¼°**ï¼šæ¯ä¸ªæ–°æ€è·¯ï¼ˆå¯¼è‡´æ–°çŠ¶æ€ï¼‰ç”±"è¯„è®ºå®¶"æˆ–éªŒè¯å‡½æ•°è¯„ä¼°ã€‚è¯¥è¯„ä¼°å¯ä»¥è¯„ä¼°ï¼š
    *   **æœ‰æ•ˆæ€§**ï¼šæ­¤ç§»åŠ¨æ˜¯å¦ç¬¦åˆé—®é¢˜è§„åˆ™ï¼Ÿ
    *   **è¿›å±•**ï¼šæ­¤ç§»åŠ¨æ˜¯å¦è®©æˆ‘ä»¬æ›´æ¥è¿‘è§£å†³æ–¹æ¡ˆï¼Ÿ
    *   **å¯å‘å¼**ï¼šè¿™æ¡è·¯å¾„æ˜¯å¦å¯èƒ½æˆåŠŸï¼Ÿ
4.  **ä¿®å‰ªä¸æ‰©å±•**ï¼šæ— æ•ˆæˆ–æ²¡æœ‰å‰é€”çš„åˆ†æ”¯è¢«ä¿®å‰ªã€‚ç„¶åæ™ºèƒ½ä½“ä»æœ€æœ‰å‰é€”çš„æ´»åŠ¨åˆ†æ”¯ç»§ç»­ï¼Œé‡å¤æ€è·¯ç”Ÿæˆè¿‡ç¨‹ã€‚
5.  **è§£å†³æ–¹æ¡ˆ**ï¼šè¯¥è¿‡ç¨‹ç»§ç»­ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡çŠ¶æ€ã€‚è§£å†³æ–¹æ¡ˆæ˜¯ä»æ ¹åˆ°ç›®æ ‡çš„æ€è·¯è·¯å¾„ã€‚

### ä½•æ—¶ä½¿ç”¨ / åº”ç”¨
*   **é€»è¾‘è°œé¢˜å’Œæ•°å­¦é—®é¢˜**ï¼šå…·æœ‰æ˜ç¡®è§„åˆ™å’Œç›®æ ‡çŠ¶æ€çš„é—®é¢˜ï¼Œéœ€è¦å¤šæ­¥éª¤ã€éçº¿æ€§æ¨ç†ï¼ˆå¦‚æ•°ç‹¬ã€è¿‡æ²³è°œé¢˜ï¼‰ã€‚
*   **å¤æ‚è§„åˆ’**ï¼šå½“ä»»åŠ¡éœ€è¦è¯¦ç»†è®¡åˆ’ï¼Œå…¶ä¸­æ“ä½œé¡ºåºå¾ˆé‡è¦ä¸”å¿…é¡»éµå®ˆçº¦æŸæ—¶ï¼ˆä¾‹å¦‚ï¼Œè®¡åˆ’å…·æœ‰å¤šæ®µè¡Œç¨‹å’Œé¢„ç®—çº¦æŸçš„å¤æ‚æ—…è¡Œï¼‰ã€‚
*   **åˆ›æ„å†™ä½œæˆ–ä»£ç ç”Ÿæˆ**ï¼šåœ¨æäº¤ä¹‹å‰æ¢ç´¢å¤šä¸ªæ•…äº‹åˆ†æ”¯æˆ–å®ç°ç­–ç•¥ã€‚

### ä¼˜åŠ¿å’ŒåŠ£åŠ¿
*   **ä¼˜åŠ¿**ï¼š
    *   **ç¨³å¥æ€§**ï¼šç³»ç»Ÿåœ°æ¢ç´¢é—®é¢˜ç©ºé—´ï¼Œä¸å•æ¬¡é€šè¿‡æ–¹æ³•ç›¸æ¯”ï¼Œä¸å¤ªå¯èƒ½å¡ä½æˆ–äº§ç”Ÿé”™è¯¯ç­”æ¡ˆã€‚
    *   **å¤„ç†ç»„åˆå¤æ‚æ€§**ï¼šéå¸¸é€‚åˆå¯èƒ½åºåˆ—æ•°é‡å·¨å¤§çš„é—®é¢˜ã€‚
*   **åŠ£åŠ¿**ï¼š
    *   **è®¡ç®—é‡å¤§**ï¼šä¸ç®€å•çš„æ€ç»´é“¾æç¤ºç›¸æ¯”ï¼Œéœ€è¦æ˜¾è‘—æ›´å¤šçš„LLMè°ƒç”¨å’ŒçŠ¶æ€ç®¡ç†ï¼Œå› æ­¤é€Ÿåº¦æ›´æ…¢ã€æˆæœ¬æ›´é«˜ã€‚
    *   **éœ€è¦è‰¯å¥½çš„è¯„ä¼°å™¨**ï¼šæœç´¢çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºçŠ¶æ€è¯„ä¼°é€»è¾‘çš„è´¨é‡ã€‚
"""

# ğŸ“‹ ç¯å¢ƒä¸è®¾ç½®
# æˆ‘ä»¬å°†å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼Œå¹¶é…ç½®æˆ‘ä»¬çš„APIå¯†é’¥ã€‚

# !pip install -q -U langchain langgraph rich python-dotenv langchain_community langchain-openai

import os
import re
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from collections import defaultdict

# Pydanticç”¨äºæ•°æ®å»ºæ¨¡
from pydantic import BaseModel, Field

# OpenAIç›¸å…³
from openai import OpenAI
from openai import RateLimitError, APIError

# LangChainç»„ä»¶
from langchain_core.prompts import ChatPromptTemplate

# LangGraphç»„ä»¶
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict

# ç”¨äºç¾åŒ–æ‰“å°
from rich.console import Console
from rich.markdown import Markdown
from rich.tree import Tree

# é…ç½®è°ƒè¯•æ¨¡å¼
DEBUG: bool = True

# åˆå§‹åŒ–æ—¥å¿—
import logging
from rich.logging import RichHandler

logger = logging.getLogger("tree_of_thoughts")
handler = RichHandler(console=Console(), rich_tracebacks=True, markup=True)
handler.setFormatter(logging.Formatter("%(message)s"))
logger.handlers = [handler]
logger.propagate = False
logger.setLevel(logging.DEBUG if DEBUG else logging.INFO)

# ModelScopeçš„OpenAIå…¼å®¹æ¥å£é€‚é…å™¨
class ModelScopeChat:
    """
    ModelScope çš„ OpenAI å…¼å®¹æ¥å£é€‚é…å™¨ï¼š
    - æä¾› invoke(prompt) åŸºæœ¬è°ƒç”¨
    - æä¾› with_structured_output(PydanticModel) çš„ç»“æ„åŒ–è¾“å‡ºåŒ…è£…
    - æ”¯æŒAPIé”™è¯¯æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹
    """
    def __init__(self, base_url: str, api_key: str, model: str, temperature: float = 0.2, extra_body: Optional[dict] = None):
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model = model
        # ä»ç¯å¢ƒå˜é‡è·å–å¤‡é€‰æ¨¡å‹ID
        self.fallback_model = os.environ.get("MODELSCOPE_MODEL_ID_R1")
        self.base_url = base_url
        self.temperature = temperature
        self.extra_body = extra_body or {}
        self.switched = False

    def invoke(self, prompt: str):
        try:
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                stream=False,
                extra_body=self.extra_body,
            )
            # éæµå¼è¿”å›ï¼šchoices[0].message.content
            return resp.choices[0].message.content
        except (RateLimitError, APIError) as e:
            if not self.switched and self.fallback_model:
                if DEBUG:
                    console.print(f"[bold yellow]âš ï¸ æ¨¡å‹ {self.model} è¯·æ±‚å¤±è´¥: {str(e)}ï¼Œå°è¯•åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹ {self.fallback_model}[/bold yellow]")
                # åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹
                self.model = self.fallback_model
                self.switched = True
                # é‡æ–°å°è¯•è¯·æ±‚
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    stream=False,
                    extra_body=self.extra_body,
                )
                return resp.choices[0].message.content
            else:
                # å¦‚æœæ²¡æœ‰å¤‡é€‰æ¨¡å‹æˆ–å·²ç»åˆ‡æ¢è¿‡ï¼Œé‡æ–°æŠ›å‡ºå¼‚å¸¸
                raise

    def with_structured_output(self, pyd_model: type[BaseModel]):
        class _StructuredWrapper:
            def __init__(self, outer: "ModelScopeChat"):
                self.outer = outer

            def invoke(self, prompt: str) -> BaseModel:
                # é€šè¿‡ç³»ç»Ÿæç¤ºçº¦æŸä»…è¾“å‡º JSONï¼ˆå°½é‡æé«˜è§£ææˆåŠŸç‡ï¼‰ï¼Œå¹¶æ˜ç¡®å­—æ®µ/ç±»å‹
                schema = pyd_model.model_json_schema()
                props = schema.get("properties", {})
                required = schema.get("required", [])
                schema_text_lines = []
                for k, v in props.items():
                    t = v.get("type", "string")
                    schema_text_lines.append(f"- {k}: {t}")
                schema_text = "\n".join(schema_text_lines) or "- è¯·æŒ‰æ¨¡å‹å®šä¹‰ç”Ÿæˆå­—æ®µ"
                required_text = ", ".join(required) if required else "æ‰€æœ‰å­—æ®µ"
                system_msg = (
                    "ä½ æ˜¯ä¸€ä¸ªç»“æ„åŒ–è¾“å‡ºç”Ÿæˆå™¨ã€‚åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸¥æ ¼åŒ¹é…ä»¥ä¸‹å­—æ®µä¸ç±»å‹ï¼š\n"
                    f"{schema_text}\n"
                    f"å¿…é¡»åŒ…å«å­—æ®µï¼š{required_text}\n"
                    "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæˆ–å¤šä½™æ–‡æœ¬ï¼ˆä¾‹å¦‚ä»£ç å—æ ‡è®°ã€å‰åç¼€ï¼‰ã€‚"
                )
                if DEBUG:
                    logger.debug("ğŸ”§ ç”ŸæˆåŠ¨æ€ç³»ç»Ÿæç¤ºï¼ˆåŒ…å«å­—æ®µä¸ç±»å‹è¦æ±‚ï¼‰")
                    logger.debug("ç»“æ„åŒ–è¾“å‡ºæç¤ºï¼ˆç³»ç»Ÿæ¶ˆæ¯ï¼‰ï¼š\n" + system_msg)
                messages = [
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt},
                ]
                try:
                    resp = self.outer.client.chat.completions.create(
                        model=self.outer.model,
                        messages=messages,
                        temperature=self.outer.temperature,
                        stream=False,
                        extra_body=self.outer.extra_body,
                    )
                except (RateLimitError, APIError) as e:
                    if not self.outer.switched and self.outer.fallback_model:
                        if DEBUG:
                            console.print(f"[bold yellow]âš ï¸ æ¨¡å‹ {self.outer.model} è¯·æ±‚å¤±è´¥: {str(e)}ï¼Œå°è¯•åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹ {self.outer.fallback_model}[/bold yellow]")
                        # åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹
                        self.outer.model = self.outer.fallback_model
                        self.outer.switched = True
                        # é‡æ–°å°è¯•è¯·æ±‚
                        resp = self.outer.client.chat.completions.create(
                            model=self.outer.model,
                            messages=messages,
                            temperature=self.outer.temperature,
                            stream=False,
                            extra_body=self.outer.extra_body,
                        )
                    else:
                        # å¦‚æœæ²¡æœ‰å¤‡é€‰æ¨¡å‹æˆ–å·²ç»åˆ‡æ¢è¿‡ï¼Œé‡æ–°æŠ›å‡ºå¼‚å¸¸
                        raise
                content = resp.choices[0].message.content or ""
                import json, re
                from pydantic import ValidationError
                def _extract_json(s: str) -> str:
                    m = re.search(r'(\{[\s\S]*\}|\[[\s\S]*\])', s)
                    return m.group(1) if m else "{}"
                raw = content.strip()
                if DEBUG:
                    console.print("[bold blue]ğŸ“¥ æ”¶åˆ°æ¨¡å‹è¿”å›ï¼Œå°è¯•è§£æä¸º JSON[/bold blue]")
                try:
                    data = json.loads(raw)
                except Exception:
                    data = json.loads(_extract_json(raw))
                if DEBUG:
                    console.print("[bold cyan]æ¨¡å‹åŸå§‹è¿”å›ï¼ˆæˆªæ–­å±•ç¤ºï¼‰[/bold cyan]:")
                    preview = json.dumps(data, ensure_ascii=False)[:400]
                    console.print(preview + ("..." if len(preview) == 400 else ""))
                # å…œåº•å­—æ®µæ˜ å°„ï¼šå°½é‡æŠŠå¸¸è§åˆ«åæ˜ å°„åˆ°ç›®æ ‡æ¨¡å‹å­—æ®µ
                try:
                    parsed = pyd_model.model_validate(data)
                    if DEBUG:
                        console.print(f"[bold green]âœ… ç»“æ„åŒ–è§£ææˆåŠŸ[/bold green]ï¼š{pyd_model.__name__}")
                    return parsed
                except ValidationError:
                    if DEBUG:
                        console.print("[bold yellow]âš ï¸ å­—æ®µä¸åŒ¹é…ï¼Œå°è¯•è‡ªåŠ¨æ˜ å°„å¸¸è§åˆ«å[/bold yellow]")
                    mappings_applied = []
                    # KnowledgeGraph: å¸¸è§è¿”å› 'relationships' å­—æ®µ
                    if "relationships" not in data and "relations" in data:
                        data["relationships"] = data.pop("relations")
                        mappings_applied.append("relations â†’ relationships")
                    if DEBUG and mappings_applied:
                        console.print("[bold cyan]å·²åº”ç”¨å­—æ®µæ˜ å°„ï¼š[/bold cyan] " + ", ".join(mappings_applied))
                    parsed = pyd_model.model_validate(data)
                    if DEBUG:
                        console.print(f"[bold green]âœ… ç»“æ„åŒ–è§£ææˆåŠŸ[/bold green]ï¼š{pyd_model.__name__}")
                    return parsed

        return _StructuredWrapper(self)


def init_llm() -> ModelScopeChat:
    """
    åˆå§‹åŒ– ModelScope LLMï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰ã€‚
    - å¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼š
      MODELSCOPE_BASE_URLï¼ˆé»˜è®¤ï¼šhttps://api-inference.modelscope.cn/v1ï¼‰
      MODELSCOPE_API_KEY
      MODELSCOPE_MODEL_IDï¼ˆé»˜è®¤ï¼šdeepseek-ai/DeepSeek-V3.2ï¼‰
    - é¢å¤–å‚æ•°ï¼šenable_thinking å¯é€‰
    """
    base_url = os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
    api_key = os.environ.get("MODELSCOPE_API_KEY", "")
    model_id = os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
    # ä¸ºé¿å…â€œæœªä¿¡ä»»çš„ chat templateâ€é”™è¯¯ï¼Œå¢åŠ ä¿¡ä»»å‚æ•°ï¼›å¹¶è¯·æ±‚ JSON è¾“å‡ºæ ¼å¼
    extra = {
        "enable_thinking": True,
        "trust_request_chat_template": True,
        "response_format": {"type": "json_object"},
    }
    return ModelScopeChat(base_url=base_url, api_key=api_key, model=model_id, temperature=0.4, extra_body=extra)


# --- APIå¯†é’¥å’Œè¿½è¸ªè®¾ç½® ---
load_dotenv()

# ç¦ç”¨LangSmithè¿½è¸ªï¼ˆå¦‚æœAPIå¯†é’¥å·²è¿‡æœŸï¼‰
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["LANGCHAIN_PROJECT"] = "æ™ºèƒ½ä½“æ¶æ„ - æ€ç»´æ ‘ï¼ˆModelScopeï¼‰"

# æ£€æŸ¥æ‰€éœ€çš„ç¯å¢ƒå˜é‡
required_vars = ["MODELSCOPE_API_KEY", "LANGCHAIN_API_KEY"]
missing_vars = []
for var in required_vars:
    if var not in os.environ:
        missing_vars.append(var)

if missing_vars:
    print(f"è­¦å‘Š: ä»¥ä¸‹ç¯å¢ƒå˜é‡æœªè®¾ç½®: {', '.join(missing_vars)}")

print("ç¯å¢ƒå˜é‡å·²åŠ è½½ï¼Œè¿½è¸ªå·²è®¾ç½®ã€‚")

# åˆå§‹åŒ–æ§åˆ¶å°
console = Console()

# åˆå§‹åŒ–LLM
llm = init_llm()

# --- ç¬¬1é˜¶æ®µï¼šå®šä¹‰é—®é¢˜ç¯å¢ƒ ---
# æ€ç»´æ ‘ç³»ç»Ÿéœ€è¦åœ¨ä¸€ä¸ªå®šä¹‰æ˜ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œã€‚å¯¹äºæˆ‘ä»¬çš„æ•°å­—æ±‚å’Œè·¯å¾„é—®é¢˜ï¼Œ
# è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦ä»¥ç¼–ç¨‹æ–¹å¼å®šä¹‰ï¼š
# 
# 1.  **çŠ¶æ€è¡¨ç¤º**ï¼šæè¿°å½“å‰æ•°å­—å’Œå·²èµ°è·¯å¾„çš„æ–¹å¼ã€‚
# 2.  **éªŒè¯è§„åˆ™**ï¼šæ£€æŸ¥çŠ¶æ€æ˜¯å¦æœ‰æ•ˆçš„å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œæ­¥æ•°é™åˆ¶ï¼‰ã€‚
# 3.  **ç›®æ ‡çŠ¶æ€**ï¼šæ£€æŸ¥è°œé¢˜æ˜¯å¦å·²è§£å†³çš„æ–¹å¼ï¼ˆæ˜¯å¦åˆ°è¾¾ç›®æ ‡æ•°å­—ï¼‰ã€‚
# 4.  **å¯èƒ½çš„ç§»åŠ¨**ï¼šç¡®å®šä»ç»™å®šçŠ¶æ€çš„æ‰€æœ‰åˆæ³•ç§»åŠ¨çš„å‡½æ•°ï¼ˆåŠ 1æˆ–ä¹˜2ï¼‰ã€‚

# é—®é¢˜é…ç½®å‚æ•°ï¼ˆæ›´å¤æ‚çš„ç‰ˆæœ¬ï¼šä½¿ç”¨+1ã€*3ã€-2æ“ä½œåˆ°è¾¾29ï¼‰
CONFIG = {
    "START_NUMBER": 1,
    "TARGET_NUMBER": 29,
    "MAX_STEPS": 8,  # å¢åŠ æ­¥æ•°é™åˆ¶ï¼Œå› ä¸ºæ“ä½œæ›´å¤æ‚
    "MOVE_OPTIONS": [
        ("+1", lambda x: x + 1),
        ("Ã—3", lambda x: x * 3),
        ("-2", lambda x: x - 2)  # æ–°æ·»åŠ çš„æ“ä½œ
    ]
}

class NumberPathState(BaseModel):
    """è¡¨ç¤ºæ•°å­—æ±‚å’Œè·¯å¾„é—®é¢˜çš„çŠ¶æ€ã€‚"""
    current_number: int = Field(default=CONFIG["START_NUMBER"], description="å½“å‰æ•°å­—")
    path: List[int] = Field(default_factory=lambda: [CONFIG["START_NUMBER"]], description="å·²èµ°è·¯å¾„")
    steps_taken: int = Field(default=0, description="å·²èµ°æ­¥æ•°")
    move_description: str = Field(default=f"åˆå§‹çŠ¶æ€ï¼šä»{CONFIG['START_NUMBER']}å¼€å§‹ã€‚", description="ç§»åŠ¨æè¿°")

    def is_valid(self) -> bool:
        """æ£€æŸ¥å½“å‰çŠ¶æ€æ˜¯å¦æœ‰æ•ˆï¼ˆæ­¥æ•°æ˜¯å¦åœ¨é™åˆ¶å†…ï¼‰ã€‚"""
        return self.steps_taken <= CONFIG["MAX_STEPS"]

    def is_goal(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡çŠ¶æ€ï¼ˆæ˜¯å¦åˆ°è¾¾ç›®æ ‡æ•°å­—ï¼‰ã€‚"""
        return self.current_number == CONFIG["TARGET_NUMBER"]
    
    def __hash__(self):
        # ä½¿çŠ¶æ€å¯å“ˆå¸Œï¼Œä»¥æ£€æŸ¥è®¿é—®è¿‡çš„çŠ¶æ€
        return hash((self.current_number, self.steps_taken))
    
    def __eq__(self, other):
        return self.__hash__() == other.__hash__()

def get_possible_moves(state: NumberPathState) -> list[NumberPathState]:
    """ä»å½“å‰çŠ¶æ€ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æœ‰æ•ˆä¸‹ä¸€ä¸ªçŠ¶æ€ï¼ˆæ ¹æ®é…ç½®çš„ç§»åŠ¨é€‰é¡¹ï¼‰ã€‚"""
    moves = []
    
    # éå†æ‰€æœ‰é…ç½®çš„ç§»åŠ¨é€‰é¡¹
    for move_symbol, move_func in CONFIG["MOVE_OPTIONS"]:
        new_state = state.model_copy(deep=True)
        new_state.current_number = move_func(new_state.current_number)
        new_state.path.append(new_state.current_number)
        new_state.steps_taken += 1
        new_state.move_description = f"æ­¥éª¤ {new_state.steps_taken}: {new_state.path[-2]} {move_symbol} = {new_state.current_number}"
        
        if new_state.is_valid():
            moves.append(new_state)
        
    return moves

print("æ•°å­—æ±‚å’Œè·¯å¾„ç¯å¢ƒå®šä¹‰æˆåŠŸã€‚")

# --- ç¬¬2é˜¶æ®µï¼šä½¿ç”¨LangGraphå®ç°æ€ç»´æ ‘æ™ºèƒ½ä½“ ---
# ç°åœ¨æˆ‘ä»¬å°†æ„å»ºæ™ºèƒ½ä½“æœ¬èº«ã€‚æˆ‘ä»¬å›¾çš„çŠ¶æ€å°†è·Ÿè¸ªæ€ç»´æ ‘ä¸­æ‰€æœ‰æ´»åŠ¨è·¯å¾„ï¼ˆåˆ†æ”¯ï¼‰ã€‚
# èŠ‚ç‚¹å°†æ‰§è¡Œå…³é”®çš„æ€ç»´æ ‘æ“ä½œï¼š
# 
# 1.  **æ‰©å±•è·¯å¾„ï¼ˆæ€è·¯ç”Ÿæˆå™¨ï¼‰**ï¼šä¸€ä¸ªåŸºäºLLMçš„èŠ‚ç‚¹ï¼ŒæŸ¥çœ‹æ¯ä¸ªæ´»åŠ¨è·¯å¾„çš„æœ€åçŠ¶æ€ï¼Œ
#     å¹¶ä»æœ‰æ•ˆå¯èƒ½æ€§åˆ—è¡¨ä¸­æå‡ºä¸€ä¸ªæœ‰å‰é€”çš„ä¸‹ä¸€æ­¥ã€‚
# 2.  **ä¿®å‰ªè·¯å¾„ï¼ˆçŠ¶æ€è¯„ä¼°å™¨ï¼‰**ï¼šè¿™ä¸ªèŠ‚ç‚¹åœ¨ç”Ÿæˆåè¿›è¡Œæ¸…ç†ã€‚å®ƒå°†ç§»é™¤ä»»ä½•
#     è¿›å…¥æ— æ•ˆçŠ¶æ€æˆ–å¾ªç¯ï¼ˆé‡æ–°è®¿é—®ä¹‹å‰çŠ¶æ€ï¼‰çš„è·¯å¾„ã€‚
# 3.  **æ£€æŸ¥è§£å†³æ–¹æ¡ˆï¼ˆç›®æ ‡æ£€æŸ¥ï¼‰**ï¼šä¸€ä¸ªæ¡ä»¶èŠ‚ç‚¹ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ´»åŠ¨è·¯å¾„å·²è¾¾åˆ°ç›®æ ‡çŠ¶æ€ã€‚
#     å¦‚æœæ˜¯è¿™æ ·ï¼Œå®ƒå°†ç»ˆæ­¢å¾ªç¯ã€‚

# LLMé€‰æ‹©ç§»åŠ¨çš„Pydanticæ¨¡å‹
class MoveChoice(BaseModel):
    best_move_index: int = Field(description="ä»æä¾›çš„å¯èƒ½ç§»åŠ¨åˆ—è¡¨ä¸­æœ€ä½³ç§»åŠ¨çš„ç´¢å¼•ã€‚")
    reasoning: str = Field(description="ä¸ºä»€ä¹ˆè¿™æ˜¯æœ€æœ‰å‰é€”çš„ç§»åŠ¨çš„ç®€è¦æ¨ç†ã€‚")

# LangGraphçŠ¶æ€
class ToTState(TypedDict):
    problem_description: str
    # æ¯ä¸ªè·¯å¾„æ˜¯NumberPathStateå¯¹è±¡çš„åˆ—è¡¨
    active_paths: List[List[NumberPathState]]
    # æˆ‘ä»¬å°†åœ¨è¿™é‡Œå­˜å‚¨æœ€ç»ˆè§£å†³æ–¹æ¡ˆ
    solution: Optional[List[NumberPathState]]

# å›¾èŠ‚ç‚¹

def initialize_search(state: ToTState) -> Dict[str, Any]:
    """è®¾ç½®æœç´¢åˆå§‹çŠ¶æ€çš„èŠ‚ç‚¹ã€‚"""
    initial_number_state = NumberPathState()
    return {"active_paths": [[initial_number_state]]}


def expand_paths(state: ToTState) -> Dict[str, Any]:
    """'æ€è·¯ç”Ÿæˆå™¨'ã€‚ç”¨æœ‰å‰é€”çš„ä¸‹ä¸€æ­¥æ‰©å±•æ¯ä¸ªæ´»åŠ¨è·¯å¾„ã€‚"""
    console.print("--- æ‰©å±•è·¯å¾„ ---")
    new_paths = []
    choice_llm = llm.with_structured_output(MoveChoice)
    
    # ç”ŸæˆåŠ¨æ€ç³»ç»Ÿæç¤ºï¼Œä½¿ç”¨é…ç½®å‚æ•°
    move_symbols = ", ".join([symbol for symbol, _ in CONFIG["MOVE_OPTIONS"]])
    system_prompt = (
        "æ‚¨æ˜¯ä¸€ä½é€»è¾‘è°œé¢˜ä¸“å®¶ã€‚æ‚¨çš„ç›®æ ‡æ˜¯è§£å†³æ•°å­—æ±‚å’Œè·¯å¾„é—®é¢˜ï¼š"
        f"ä»{CONFIG['START_NUMBER']}å¼€å§‹ï¼Œæ¯æ¬¡åªèƒ½{move_symbols}ï¼Œ"
        f"åœ¨{CONFIG['MAX_STEPS']}æ­¥å†…åˆ°è¾¾{CONFIG['TARGET_NUMBER']}ã€‚"
        "åˆ†æå½“å‰è·¯å¾„å¹¶ä»æä¾›çš„é€‰é¡¹åˆ—è¡¨ä¸­é€‰æ‹©æœ€æœ‰å‰é€”çš„ä¸‹ä¸€æ­¥ã€‚"
    )
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "é—®é¢˜: {problem}\n\nå½“å‰è·¯å¾„å†å²:\n{path_history}\n\nä»æœ€ç»ˆçŠ¶æ€ï¼Œä»ä»¥ä¸‹åˆ—è¡¨ä¸­é€‰æ‹©æœ€ä½³ä¸‹ä¸€æ­¥:\n{possible_moves}")
    ])
    
    for path in state['active_paths']:
        last_state = path[-1]
        possible_next_states = get_possible_moves(last_state)
        
        if not possible_next_states:
            continue  # è¿™æ¡è·¯å¾„æ˜¯æ­»èƒ¡åŒ
            
        path_history_str = " -> ".join([s.move_description for s in path])
        possible_moves_str = "\n".join([f"{i}: {s.move_description}" for i, s in enumerate(possible_next_states)])
        
        # ä¸ºäº†ç®€å•å’Œå±•ç¤ºå¹¿åº¦ï¼Œæˆ‘ä»¬å¯ä»¥æ¢ç´¢å¤šä¸ªç§»åŠ¨ã€‚
        # æ›´é«˜çº§çš„æ€ç»´æ ‘å¯èƒ½ä¼šä½¿ç”¨LLMåªé€‰æ‹©å•ä¸ªæœ€ä½³ç§»åŠ¨ã€‚
        # åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è®©æ‰€æœ‰æœ‰æ•ˆç§»åŠ¨åˆ†æ”¯ä»¥å±•ç¤ºæ ‘ç»“æ„ã€‚
        for next_state in possible_next_states:
            new_paths.append(path + [next_state])

    console.print(f"[cyan]æ‰©å±•åˆ° {len(new_paths)} æ¡æ½œåœ¨è·¯å¾„ã€‚[/cyan]")
    return {"active_paths": new_paths}


def prune_paths(state: ToTState) -> Dict[str, Any]:
    """'çŠ¶æ€è¯„ä¼°å™¨'ã€‚ä¿®å‰ªæ— æ•ˆæˆ–åŒ…å«å¾ªç¯çš„è·¯å¾„ï¼Œå¹¶åŸºäºå¯å‘å¼è¯„ä¼°ä¼˜å…ˆé€‰æ‹©æœ‰å¸Œæœ›çš„è·¯å¾„ã€‚"""
    console.print("--- ä¿®å‰ªè·¯å¾„ ---")
    valid_paths = []
    
    # ç¬¬ä¸€æ­¥ï¼šç§»é™¤æ— æ•ˆè·¯å¾„å’Œå¾ªç¯
    for path in state['active_paths']:
        # æ£€æŸ¥å¾ªç¯ï¼šå¦‚æœæœ€åä¸€ä¸ªçŠ¶æ€ä¹‹å‰åœ¨è·¯å¾„ä¸­å‡ºç°è¿‡
        if path[-1] in path[:-1]:
            continue  # å‘ç°å¾ªç¯ï¼Œä¿®å‰ªè¿™æ¡è·¯å¾„
        
        # æ£€æŸ¥æœ‰æ•ˆæ€§
        if path[-1].is_valid():
            valid_paths.append(path)
    
    # ç¬¬äºŒæ­¥ï¼šåŸºäºå¯å‘å¼è¯„ä¼°å¯¹è·¯å¾„è¿›è¡Œæ’åº
    def heuristic(path):
        last_state = path[-1]
        distance_to_goal = abs(CONFIG["TARGET_NUMBER"] - last_state.current_number)  # è·ç¦»ç›®æ ‡çš„è·ç¦»
        steps_efficiency = CONFIG["MAX_STEPS"] - last_state.steps_taken  # å‰©ä½™å¯ç”¨æ­¥æ•°
        
        # ç»¼åˆå¾—åˆ†ï¼šè·ç¦»è¶Šè¿‘å¾—åˆ†è¶Šé«˜ï¼Œå‰©ä½™æ­¥æ•°è¶Šå¤šå¾—åˆ†è¶Šé«˜
        # æ·»åŠ ä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œé¿å…æ•°å­—è¿‡å¤§ï¼ˆè¶…è¿‡ç›®æ ‡å¤ªå¤šï¼‰
        overshoot_penalty = max(0, last_state.current_number - (CONFIG["TARGET_NUMBER"] * 2)) * 2
        return - (distance_to_goal - steps_efficiency + overshoot_penalty)
    
    # æŒ‰å¯å‘å¼å¾—åˆ†æ’åº
    valid_paths.sort(key=heuristic, reverse=True)
    
    # ç¬¬ä¸‰æ­¥ï¼šåªä¿ç•™å‰10æ¡æœ€æœ‰å¸Œæœ›çš„è·¯å¾„ï¼ˆé¿å…æœç´¢ç©ºé—´è¿‡å¤§ï¼‰
    pruned_paths = valid_paths[:10]
    
    console.print(f"[green]ä¿®å‰ªåå‰©ä¸‹ {len(pruned_paths)} æ¡æœ‰æ•ˆã€éå¾ªç¯ä¸”æœ‰å¸Œæœ›çš„è·¯å¾„ã€‚[/green]")
    return {"active_paths": pruned_paths}


# æ¡ä»¶èŠ‚ç‚¹
def check_for_solution(state: ToTState) -> str:
    """æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è·¯å¾„å·²è¾¾åˆ°ç›®æ ‡å¹¶è·¯ç”±æ‰§è¡Œã€‚"""
    for path in state['active_paths']:
        if path[-1].is_goal():
            console.print("[bold green]æ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼[/bold green]")
            return "solution_found"
    return "continue_search"

# æ·»åŠ ä¸€ä¸ªæ–°çš„èŠ‚ç‚¹æ¥å¤„ç†è§£å†³æ–¹æ¡ˆ
def process_solution(state: ToTState) -> Dict[str, Any]:
    """æ‰¾åˆ°å¹¶ä¿å­˜è§£å†³æ–¹æ¡ˆã€‚"""
    for path in state['active_paths']:
        if path[-1].is_goal():
            return {
                "active_paths": state['active_paths'],
                "problem_description": state['problem_description'],
                "solution": path
            }
    return state

# æ„å»ºå›¾
workflow = StateGraph(ToTState)

workflow.add_node("initialize", initialize_search)
workflow.add_node("expand", expand_paths)
workflow.add_node("prune", prune_paths)
workflow.add_node("process_solution", process_solution)  # æ·»åŠ æ–°çš„å¤„ç†è§£å†³æ–¹æ¡ˆèŠ‚ç‚¹

workflow.set_entry_point("initialize")
workflow.add_edge("initialize", "expand")
workflow.add_edge("expand", "prune")

workflow.add_conditional_edges(
    "prune",
    check_for_solution,
    {
        "solution_found": "process_solution",  # å…ˆå¤„ç†è§£å†³æ–¹æ¡ˆ
        "continue_search": "expand"
    }
)

workflow.add_edge("process_solution", END)  # ä»å¤„ç†èŠ‚ç‚¹åˆ°ç»“æŸ

tot_agent = workflow.compile()
print("æ€ç»´æ ‘æ™ºèƒ½ä½“å›¾ç¼–è¯‘æˆåŠŸã€‚")

# --- ç¬¬3é˜¶æ®µï¼šæ¼”ç¤ºä¸åˆ†æ ---
# ç°åœ¨ï¼Œè®©æˆ‘ä»¬åœ¨è°œé¢˜ä¸Šè¿è¡Œæˆ‘ä»¬çš„æ€ç»´æ ‘æ™ºèƒ½ä½“ã€‚æˆ‘ä»¬å°†æ¯”è¾ƒå®ƒçš„ç³»ç»Ÿæ–¹æ³•ä¸ç®€å•çš„å•æ¬¡æ€ç»´é“¾è¯·æ±‚ï¼Œ
# ä»¥çªå‡ºç¨³å¥æ€§çš„å·®å¼‚ã€‚

problem_description = """
æ•°å­—æ±‚å’Œè·¯å¾„é—®é¢˜ï¼ˆå‡çº§ç‰ˆï¼‰ï¼š
ä»æ•°å­—1å¼€å§‹ï¼Œæ¯æ¬¡å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œä¹‹ä¸€ï¼š+1ã€Ã—3ã€-2
åœ¨8æ­¥å†…åˆ°è¾¾æ•°å­—29ã€‚
è¯·æ‰¾å‡ºä¸€æ¡æœ‰æ•ˆè·¯å¾„ã€‚
"""

# è¿è¡Œæ€ç»´æ ‘æ™ºèƒ½ä½“
console.print("\n--- ğŸš€ è¿è¡Œæ€ç»´æ ‘æ™ºèƒ½ä½“è§£å†³è°œé¢˜ ---")
result = tot_agent.invoke({
    "problem_description": problem_description,
    "active_paths": [],
    "solution": None
})

# å±•ç¤ºè§£å†³æ–¹æ¡ˆ
if result.get("solution"):
    console.print("\n--- ğŸ“‹ è§£å†³æ–¹æ¡ˆè·¯å¾„ ---")
    solution_path = result["solution"]
    
    # åˆ›å»ºå¯è§†åŒ–è·¯å¾„æ ‘
    path_tree = Tree("[bold blue]ğŸ“ˆ è§£å†³æ–¹æ¡ˆè·¯å¾„[/bold blue]")
    for i, state in enumerate(solution_path):
        node_label = f"[{i+1}] {state.move_description}"
        node = path_tree.add(node_label)
        node.add(f"å½“å‰æ•°å­—: {state.current_number} | å·²èµ°æ­¥æ•°: {state.steps_taken}")
    
    console.print(path_tree)
    
    # æ˜¾ç¤ºè§£å†³æ–¹æ¡ˆç»Ÿè®¡
    console.print("\n--- ğŸ“Š è§£å†³æ–¹æ¡ˆç»Ÿè®¡ ---")
    console.print(f"[green]âœ… æ€»æ­¥æ•°:[/green] {solution_path[-1].steps_taken}")
    console.print(f"[green]âœ… è·¯å¾„åºåˆ—:[/green] {' â†’ '.join(map(str, solution_path[-1].path))}")
    console.print(f"[green]âœ… è§£å†³æ•ˆç‡:[/green] {(1 - (solution_path[-1].steps_taken / 5)) * 100:.1f}% (5æ­¥é™åˆ¶)")
    console.print()
else:
    console.print("[red]âŒ æœªæ‰¾åˆ°è§£å†³æ–¹æ¡ˆã€‚[/red]")

# ä¸ç®€å•æ€ç»´é“¾è¿›è¡Œæ¯”è¾ƒ
console.print("\n--- ğŸ§  ä¸ç®€å•æ€ç»´é“¾æ¯”è¾ƒ ---")
simple_prompt = ChatPromptTemplate.from_messages([
    ("system", "æ‚¨æ˜¯ä¸€ä½é€»è¾‘è°œé¢˜ä¸“å®¶ã€‚è¯·è§£å†³ä»¥ä¸‹æ•°å­—æ±‚å’Œè·¯å¾„é—®é¢˜ã€‚"),
    ("human", "{problem}")
])

formatted_prompt = simple_prompt.format_messages(problem=problem_description)
formatted_prompt_str = formatted_prompt[0].content + "\n\n" + formatted_prompt[1].content

console.print("[yellow]è¿è¡Œç®€å•æ€ç»´é“¾è¯·æ±‚...[/yellow]")
chain_of_thought_response = llm.invoke(formatted_prompt_str)

console.print("\n--- ğŸ’¬ æ€ç»´é“¾å“åº” ---")
console.print(chain_of_thought_response)

# ğŸ” åˆ†ææ¯”è¾ƒ
console.print("\n--- ğŸ“Š åˆ†ææ¯”è¾ƒ ---")
console.print("[green]æ€ç»´æ ‘æ–¹æ³•çš„ä¼˜åŠ¿ï¼š[/green]")
console.print("- ç³»ç»Ÿåœ°æ¢ç´¢é—®é¢˜ç©ºé—´ï¼Œå‡å°‘é—æ¼å…³é”®æ­¥éª¤çš„é£é™©")
console.print("- èƒ½å¤Ÿå›æº¯å¹¶ä»æ­»èƒ¡åŒä¸­æ¢å¤")
console.print("- æ˜ç¡®éªŒè¯æ¯ä¸ªçŠ¶æ€çš„æœ‰æ•ˆæ€§")
console.print("- å¯è§†åŒ–æ€ç»´è¿‡ç¨‹ï¼Œä¾¿äºè°ƒè¯•å’Œç†è§£")

console.print("\n[red]æ€ç»´é“¾æ–¹æ³•çš„å±€é™æ€§ï¼š[/red]")
console.print("- çº¿æ€§æ€è€ƒå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜è§£")
console.print("- æ²¡æœ‰æ˜ç¡®çš„æœºåˆ¶æ¥éªŒè¯ä¸­é—´æ­¥éª¤")
console.print("- éš¾ä»¥å›æº¯å’Œä¿®æ­£é”™è¯¯å†³ç­–")

# ğŸ“ ç»“è®º
# åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬æˆåŠŸæ„å»ºäº†ä¸€ä¸ªä½¿ç”¨æ€ç»´æ ‘æ¶æ„çš„é—®é¢˜è§£å†³æ™ºèƒ½ä½“ã€‚
# æ¼”ç¤ºæ¸…æ¥šåœ°å±•ç¤ºäº†è¿™ç§æ–¹æ³•çš„å¼ºå¤§åŠŸèƒ½ï¼š
# 
# - **ç®€å•æ€ç»´é“¾çš„å±€é™æ€§**ï¼šå•ä¸€çš„çº¿æ€§æ€è€ƒè¿‡ç¨‹å¾ˆå®¹æ˜“å¿½ç•¥å…³é”®æ­¥éª¤æˆ–é™·å…¥æ— æ•ˆçŠ¶æ€ã€‚
# - **æ€ç»´æ ‘çš„ç¨³å¥æ€§**ï¼šé€šè¿‡ç³»ç»Ÿåœ°æ¢ç´¢å¤šä¸ªè·¯å¾„å¹¶éªŒè¯æ¯ä¸ªçŠ¶æ€ï¼Œæ€ç»´æ ‘æ™ºèƒ½ä½“èƒ½å¤Ÿæ‰¾åˆ°æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚
# - **å¯æ‰©å±•æ€§**ï¼šè¿™ç§æ¶æ„å¯ä»¥åº”ç”¨äºæ›´å¤æ‚çš„é—®é¢˜ï¼Œå¦‚æ•°å­¦è¯æ˜ã€ä»£ç ç”Ÿæˆå’Œæˆ˜ç•¥è§„åˆ’ã€‚
# 
# è™½ç„¶æ€ç»´æ ‘æ–¹æ³•éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå’ŒLLMè°ƒç”¨ï¼Œä½†å¯¹äºéœ€è¦å¯é æ€§å’Œç³»ç»Ÿæ€§çš„å¤æ‚é—®é¢˜ï¼Œè¿™ç§é¢å¤–çš„æˆæœ¬æ˜¯å€¼å¾—çš„ã€‚

console.print("\n--- ğŸ‰ æ€ç»´æ ‘è§„åˆ’æ¼”ç¤ºå®Œæˆï¼ ---")
