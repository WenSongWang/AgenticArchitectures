# -*- coding: utf-8 -*-
"""
ReActï¼ˆReason + Actï¼‰æ¶æ„çš„å¯è¿è¡Œç¤ºä¾‹
 
å­¦ä¹ ç›®æ ‡ï¼ˆçœ‹å®Œä½ èƒ½åšåˆ°ä»€ä¹ˆï¼‰ï¼š
- äº†è§£â€œå…ˆæ€è€ƒï¼Œå†è¡ŒåŠ¨ï¼Œå†è§‚å¯Ÿâ€çš„ ReAct äº¤äº’æ¨¡å¼
- ä½¿ç”¨ LangGraph ç¼–æ’â€œå¾ªç¯å¼å·¥ä½œæµâ€ï¼Œåœ¨è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶å‰æŒç»­è¿­ä»£
- ç”¨ Pydantic v2 è®© LLM ä»¥ç»“æ„åŒ– JSON è¿”å›â€œæƒ³æ³•/è¡ŒåŠ¨/æ˜¯å¦ç»“æŸâ€
- åœ¨çº¯æœ¬åœ°å·¥å…·ä¸‹å­¦ä¹  ReAct çš„å®ç°æ€è·¯ï¼Œä¿è¯å¯è¿è¡Œã€æ˜“ç†è§£
 
æ ¸å¿ƒæ¦‚å¿µé€Ÿè§ˆï¼š
- ReAct å¾ªç¯ï¼šReasonerï¼ˆæ€è€ƒï¼‰â†’ Actorï¼ˆè¡ŒåŠ¨ï¼‰â†’ è§‚å¯Ÿï¼ˆObservationï¼‰â†’ ä¸‹ä¸€è½® Reasoner
- ç»“æ„åŒ–è¾“å‡ºï¼šçº¦æŸ LLM ä»…è¾“å‡ºæˆ‘ä»¬å®šä¹‰çš„æ•°æ®ç»“æ„ï¼Œæé«˜è§£æç¨³å®šæ€§
- æ¡ä»¶è¾¹ä¸ç»ˆæ­¢ï¼šå½“ `is_final=True` æˆ–è¾¾åˆ°è¿­ä»£ä¸Šé™æ—¶ç»“æŸå¾ªç¯
 
è¿è¡Œå‰å‡†å¤‡ï¼š
- é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` å¹¶é…ç½®ï¼š
  - `MODELSCOPE_API_KEY`ï¼ˆå¿…éœ€ï¼ŒModelScope æ¨ç†ä»¤ç‰Œï¼‰
  - å¯é€‰ï¼š`MODELSCOPE_BASE_URL`ï¼ˆé»˜è®¤ `https://api-inference.modelscope.cn/v1`ï¼‰ã€`MODELSCOPE_MODEL_ID`ï¼ˆé»˜è®¤ `deepseek-ai/DeepSeek-V3.2`ï¼‰
  - å¯é€‰ï¼š`LANGCHAIN_API_KEY`ï¼ˆç”¨äº LangSmith è¿½è¸ªï¼‰
 
å¦‚ä½•è¿è¡Œï¼š
- é»˜è®¤ç¤ºä¾‹ï¼š`python 03_react.py`
- æŒ‡å®šé—®é¢˜ï¼š`python 03_react.py --question "è¯·è®¡ç®—è¡¨è¾¾å¼ 12*(3+4) å¹¶ç»™å‡ºç»“æœçš„æ–‡å­—è¯´æ˜"`
- å¼€å¯ä»¤ç‰Œæµä¸æ•™å­¦æ—¥å¿—ï¼š`python 03_react.py --stream --debug`
 
é˜…è¯»å»ºè®®ï¼š
- å…ˆçœ‹â€œæ•°æ®æ¨¡å‹ä¸çŠ¶æ€â€ä¸â€œModelScope é€‚é…å™¨â€ï¼Œå†çœ‹â€œReasoner/Actor èŠ‚ç‚¹â€ä¸â€œå·¥ä½œæµç¼–æ’ä¸å¾ªç¯â€
- å·¥å…·å‡ä¸ºæœ¬åœ°å®ç°ï¼ˆå®‰å…¨ï¼‰ï¼Œæœ‰åˆ©äºä¸“æ³¨å­¦ä¹  ReAct çš„äº¤äº’é€»è¾‘
"""
import os
import json
import argparse
from typing import List, TypedDict, Optional, Dict, Any
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from rich.console import Console
from rich.syntax import Syntax
from openai import OpenAI
import logging
from rich.logging import RichHandler

console = Console()
DEBUG: bool = True
STREAM_TOKENS: bool = False
MAX_ITERATIONS: int = 6

logger = logging.getLogger("react")
handler = RichHandler(console=console, rich_tracebacks=True, markup=True)
handler.setFormatter(logging.Formatter("%(message)s"))
logger.handlers = [handler]
logger.propagate = False
logger.setLevel(logging.DEBUG if DEBUG else logging.INFO)

class ModelScopeChat:
    """
    ModelScope çš„ OpenAI å…¼å®¹æ¥å£é€‚é…å™¨ï¼š
    - æä¾› invoke(prompt, stream_tokens) åŸºæœ¬è°ƒç”¨
    - æä¾› with_structured_output(PydanticModel) çš„ç»“æ„åŒ–è¾“å‡ºåŒ…è£…
 
åˆå­¦è€…ç†è§£è¦ç‚¹ï¼š
- æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿæ¶ˆæ¯æ˜ç¡®å­—æ®µä¸ç±»å‹ï¼Œè®©æ¨¡å‹â€œåªè¾“å‡º JSON å¯¹è±¡â€ï¼Œå°½é‡é¿å…è‡ªç”±æ–‡æœ¬
- è¿”å›åä½¿ç”¨ Pydantic v2 éªŒè¯ä¸å­—æ®µåˆ«åå…œåº•ï¼Œæé«˜å¥å£®æ€§
    """
    def __init__(self, base_url: str, api_key: str, model: str, temperature: float = 0.2, extra_body: Optional[dict] = None):
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model = model
        self.base_url = base_url
        self.temperature = temperature
        self.extra_body = extra_body or {}
    def invoke(self, prompt: str, stream_tokens: bool = False) -> str:
        # éç»“æ„åŒ–è°ƒç”¨ï¼šå¯é€‰æ‹©ä»¤ç‰Œæµï¼ˆå®æ—¶æ‰“å°ï¼‰ï¼Œé€‚åˆè°ƒè¯•æ€è€ƒè¿‡ç¨‹
        if stream_tokens:
            resp_iter = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                stream=True,
                extra_body=self.extra_body,
            )
            parts = []
            import sys as _sys
            for ch in resp_iter:
                delta = getattr(ch.choices[0], "delta", None)
                token = getattr(delta, "content", "") if delta else ""
                if token:
                    parts.append(token)
                    _sys.stdout.write(token)
                    _sys.stdout.flush()
            return "".join(parts)
        else:
            # éæµå¼ï¼šä¸€æ¬¡æ€§è¿”å›æ–‡æœ¬
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                stream=False,
                extra_body=self.extra_body,
            )
            return resp.choices[0].message.content or ""
    def with_structured_output(self, pyd_model: type[BaseModel]):
        class _StructuredWrapper:
            def __init__(self, outer: "ModelScopeChat"):
                self.outer = outer
            def invoke(self, prompt: str) -> BaseModel:
                # ç”Ÿæˆâ€œå­—æ®µ/ç±»å‹çº¦æŸâ€çš„ç³»ç»Ÿæ¶ˆæ¯ï¼Œæå‡ JSON ç»“æ„åŒ–è¾“å‡ºç¨³å®šæ€§
                schema = pyd_model.model_json_schema()
                props = schema.get("properties", {})
                required = schema.get("required", [])
                schema_text_lines = []
                for k, v in props.items():
                    t = v.get("type", "string")
                    schema_text_lines.append(f"- {k}: {t}")
                schema_text = "\n".join(schema_text_lines) or "- fields"
                required_text = ", ".join(required) if required else "all"
                system_msg = (
                    "åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸¥æ ¼åŒ¹é…ä»¥ä¸‹å­—æ®µä¸ç±»å‹ï¼š\n"
                    f"{schema_text}\n"
                    f"å¿…é¡»åŒ…å«å­—æ®µï¼š{required_text}\n"
                    "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæˆ–å¤šä½™æ–‡æœ¬ã€‚"
                )
                messages = [
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt},
                ]
                if STREAM_TOKENS:
                    # ç»“æ„åŒ–ä»¤ç‰Œæµï¼šè¾¹æ¥æ”¶è¾¹æ‰“å°ï¼Œä¾¿äºè§‚å¯Ÿæ¨¡å‹å¦‚ä½•å¡«å…… JSON
                    content_iter = self.outer.client.chat.completions.create(
                        model=self.outer.model,
                        messages=messages,
                        temperature=self.outer.temperature,
                        stream=True,
                        extra_body=self.outer.extra_body,
                    )
                    import sys as _sys
                    _sys.stdout.write("\nğŸ“¡ æ­£åœ¨æ¥æ”¶ç»“æ„åŒ– JSON...\n")
                    _sys.stdout.flush()
                    parts = []
                    for chunk in content_iter:
                        delta = getattr(chunk.choices[0], "delta", None)
                        token = getattr(delta, "content", "") if delta else ""
                        if token:
                            parts.append(token)
                            _sys.stdout.write(token)
                            _sys.stdout.flush()
                    content = "".join(parts)
                else:
                    # ä¸€æ¬¡æ€§è¿”å› JSON å­—ç¬¦ä¸²
                    resp = self.outer.client.chat.completions.create(
                        model=self.outer.model,
                        messages=messages,
                        temperature=self.outer.temperature,
                        stream=False,
                        extra_body=self.outer.extra_body,
                    )
                    content = resp.choices[0].message.content or ""
                import json as _json, re
                from pydantic import ValidationError
                def _extract_json(s: str) -> str:
                    # å®½æ¾æå–ï¼šä»æ–‡æœ¬ä¸­æˆªå–æœ€å¤–å±‚ {...} æˆ– [...]
                    m = re.search(r'(\{[\s\S]*\}|\[[\s\S]*\])', s)
                    return m.group(1) if m else "{}"
                raw = content.strip()
                try:
                    # é¦–é€‰ï¼šç›´æ¥è§£æå®Œæ•´ JSON
                    data = _json.loads(raw)
                except Exception:
                    # é€€è·¯ï¼šä½¿ç”¨å®½æ¾æå–åçš„ç‰‡æ®µå†è§£æ
                    data = _json.loads(_extract_json(raw))
                try:
                    return pyd_model.model_validate(data)
                except ValidationError:
                    # å­—æ®µå…œåº•æ˜ å°„
                    # å½“æ¨¡å‹ä½¿ç”¨äº†å¸¸è§åŒä¹‰å­—æ®µåæ—¶ï¼Œè¿›è¡Œç¨³å®šçš„åˆ«åå›å¡«ï¼Œç¡®ä¿ Pydantic æ ¡éªŒé€šè¿‡
                    # å°†å¸¸è§åˆ«åâ€œreasonâ†’thoughtâ€ã€â€œtool_nameâ†’actionâ€ã€â€œargumentsâ†’action_inputâ€ã€â€œanswerâ†’final_answerâ€
                    if "thought" not in data and "reason" in data:
                        data["thought"] = data.pop("reason")
                    if "action" not in data and "tool_name" in data:
                        data["action"] = data.pop("tool_name")
                    if "action_input" not in data and "arguments" in data:
                        data["action_input"] = data.pop("arguments")
                    if "final_answer" not in data and "answer" in data:
                        data["final_answer"] = data.pop("answer")
                    ai = data.get("action_input", None)
                    act = data.get("action", None)
                    if not isinstance(ai, dict):
                        if isinstance(ai, str):
                            try:
                                parsed = _json.loads(ai)
                                if isinstance(parsed, dict):
                                    data["action_input"] = parsed
                                else:
                                    raise Exception()
                            except Exception:
                                if act == "calc":
                                    data["action_input"] = {"expression": ai}
                                elif act == "normalize_text":
                                    data["action_input"] = {"text": ai}
                                elif act == "current_time":
                                    data["action_input"] = {"format": ai}
                                else:
                                    data["action_input"] = {"input": ai}
                        else:
                            data["action_input"] = {"input": ai} if ai is not None else {}
                    return pyd_model.model_validate(data)
        return _StructuredWrapper(self)

def init_llm() -> ModelScopeChat:
    """
    åˆå§‹åŒ– ModelScope LLMï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰ã€‚
    - MODELSCOPE_BASE_URLï¼ˆé»˜è®¤ï¼šhttps://api-inference.modelscope.cn/v1ï¼‰
    - MODELSCOPE_API_KEY
    - MODELSCOPE_MODEL_IDï¼ˆé»˜è®¤ï¼šdeepseek-ai/DeepSeek-V3.2ï¼‰
    - å¼ºåˆ¶ JSON è¾“å‡ºä»¥é…åˆç»“æ„åŒ–è§£æ
    """
    base_url = os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
    api_key = os.environ.get("MODELSCOPE_API_KEY", "")
    model_id = os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
    extra = {
        "enable_thinking": True,
        "trust_request_chat_template": True,
        "response_format": {"type": "json_object"},
    }
    return ModelScopeChat(base_url=base_url, api_key=api_key, model=model_id, temperature=0.2, extra_body=extra)

# =========================
# å·¥å…·é›†åˆï¼ˆæœ¬åœ°ã€å®‰å…¨ï¼‰
# =========================
def _safe_eval(expr: str) -> float:
    """
    å®‰å…¨ç®—æœ¯è¡¨è¾¾å¼æ±‚å€¼ï¼šä»…æ”¯æŒ + - * / ** å’Œä¸€å…ƒè´Ÿå·
    é¿å…ä»»æ„ä»£ç æ‰§è¡Œé£é™©
    """
    import ast, operator as op
    allowed = {
        ast.Add: op.add,
        ast.Sub: op.sub,
        ast.Mult: op.mul,
        ast.Div: op.truediv,
        ast.Pow: op.pow,
        ast.USub: op.neg,
    }
    def _eval(node):
        # é€’å½’è®¡ç®—ï¼šä»…å…è®¸å®šä¹‰å¥½çš„ AST èŠ‚ç‚¹ä¸è¿ç®—ç¬¦
        if isinstance(node, ast.Num):
            return node.n
        if isinstance(node, ast.BinOp) and type(node.op) in allowed:
            return allowed[type(node.op)](_eval(node.left), _eval(node.right))
        if isinstance(node, ast.UnaryOp) and type(node.op) in allowed:
            return allowed[type(node.op)](_eval(node.operand))
        raise ValueError("unsupported expression")
    tree = ast.parse(expr, mode="eval")
    return float(_eval(tree.body))

def tool_calc(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šè®¡ç®—ç®—æœ¯è¡¨è¾¾å¼çš„å€¼"""
    expr = str(arguments.get("expression", ""))
    try:
        # ä½¿ç”¨å—æ§çš„ _safe_eval æ‰§è¡Œè¡¨è¾¾å¼è®¡ç®—
        value = _safe_eval(expr)
        return {"expression": expr, "value": value}
    except Exception as e:
        return {"expression": expr, "error": str(e)}

def tool_normalize_text(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šæ ‡å‡†åŒ–æ–‡æœ¬ï¼ˆå°å†™ã€å»æ ‡ç‚¹ã€å‹ç¼©ç©ºç™½ï¼‰"""
    import re
    text = str(arguments.get("text", ""))
    # å°å†™åŒ– â†’ å»æ ‡ç‚¹ï¼ˆæ›¿æ¢ä¸ºç©ºæ ¼ï¼‰â†’ å‹ç¼©è¿ç»­ç©ºç™½
    lower = text.lower()
    no_punct = re.sub(r"[^\w\s]", " ", lower)
    normalized = re.sub(r"\s+", " ", no_punct).strip()
    return {"normalized": normalized, "orig": text}

def tool_current_time(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šè¿”å›å½“å‰æœ¬åœ°æ—¶é—´å­—ç¬¦ä¸²"""
    from datetime import datetime
    fmt = str(arguments.get("format", "%Y-%m-%d %H:%M:%S"))
    return {"now": datetime.now().strftime(fmt), "format": fmt}

TOOLS_REGISTRY = {
    "calc": tool_calc,
    "normalize_text": tool_normalize_text,
    "current_time": tool_current_time,
}

# =========================
# æ•°æ®æ¨¡å‹ä¸çŠ¶æ€
# =========================
class ReActStep(BaseModel):
    """ç»“æ„åŒ–çš„ ReAct æ­¥éª¤è¾“å‡ºï¼šæƒ³æ³•/è¡ŒåŠ¨/è¾“å…¥/æ˜¯å¦ç»“æŸ/æœ€ç»ˆç­”æ¡ˆ"""
    thought: str = Field(description="æ€è€ƒè¿‡ç¨‹çš„ç®€è¦è¯´æ˜")
    action: Optional[str] = Field(default=None, description="é€‰æ‹©çš„å·¥å…·åï¼ˆå¯é€‰ï¼‰")
    action_input: Optional[Dict[str, Any]] = Field(default=None, description="ä¼ ç»™å·¥å…·çš„å‚æ•°å¯¹è±¡ï¼ˆå¯é€‰ï¼‰")
    is_final: bool = Field(description="æ˜¯å¦ä¸ºæœ€ç»ˆå›ç­”ï¼ˆTrue åˆ™ä¸å†è¡ŒåŠ¨ï¼‰")
    final_answer: Optional[str] = Field(default=None, description="æœ€ç»ˆå›ç­”ï¼ˆå½“ is_final=True æ—¶åº”æä¾›ï¼‰")

class ReActState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€ï¼šåœ¨èŠ‚ç‚¹é—´ä¼ é€’çš„å…±äº«æ•°æ®"""
    question: str
    steps: List[dict]              # æ¯è½®çš„ç»“æ„åŒ–æ­¥éª¤ä¸è§‚å¯Ÿ
    final_answer: Optional[str]    # æœ€ç»ˆå›ç­”ï¼ˆè‹¥å·²ç»“æŸï¼‰

# =========================
# Reasoner ä¸ Actor èŠ‚ç‚¹
# =========================
def make_reasoner_node(llm: "ModelScopeChat"):
    """
    Reasonerï¼šç»™æ¨¡å‹ä¸Šä¸‹æ–‡ï¼ˆé—®é¢˜ + è¿‡å¾€è§‚å¯Ÿï¼‰ï¼Œè®©å…¶â€œå…ˆæ€è€ƒâ€ï¼Œ
    å†å†³å®šæ˜¯å¦é€‰æ‹©ä¸€ä¸ªå·¥å…·æ‰§è¡Œï¼›æˆ–è€…ç›´æ¥è¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚
    """
    reasoner_llm = llm.with_structured_output(ReActStep)
    def _node(state: ReActState) -> dict:
        question = state["question"]
        history = state.get("steps", [])
        # æä¾›â€œå¯ç”¨å·¥å…·ç›®å½•â€ï¼Œå¼•å¯¼æ¨¡å‹é€‰æ‹©åˆæ³•å·¥å…·ä¸å‚æ•°ï¼ˆåç§°éœ€ä¸ TOOLS_REGISTRY å¯¹é½ï¼‰
        tool_catalog = json.dumps(
            {
                "available_tools": [
                    {"name": "calc", "args": {"expression": "å­—ç¬¦ä¸²ç®—æœ¯è¡¨è¾¾å¼ï¼Œå¦‚ '12*(3+4)'" }},
                    {"name": "normalize_text", "args": {"text": "è¦æ ‡å‡†åŒ–çš„æ–‡æœ¬"}},
                    {"name": "current_time", "args": {"format": "å¯é€‰æ—¶é—´æ ¼å¼"}},
                ]
            },
            ensure_ascii=False,
            indent=2,
        )
        # å°†å†å²æ­¥éª¤ï¼ˆå«ä¸Šä¸€è½®çš„è§‚å¯Ÿï¼‰ä¼ å…¥ï¼Œä¾›â€œæ€è€ƒâ€å‚è€ƒ
        history_text = json.dumps(history, ensure_ascii=False, indent=2)
        # æ˜ç¡®è¦æ±‚è¾“å‡ºä¸¥æ ¼ JSONï¼Œå­—æ®µä¸ç±»å‹å›ºå®šï¼Œå‡å°‘è§£ææ­§ä¹‰
        prompt = (
            "ä½ æ˜¯ä¸€ä¸ª ReAct æ™ºèƒ½ä½“ï¼šå…ˆæ€è€ƒï¼ˆthoughtï¼‰ï¼Œå†é€‰æ‹©è¡ŒåŠ¨ï¼ˆaction/action_inputï¼‰ï¼Œæˆ–ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚\n"
            "è¯·ä¸¥æ ¼è¾“å‡º JSONï¼Œå­—æ®µï¼šthought, action(å¯é€‰), action_input(å¯é€‰), is_final(å¿…å¡«), final_answer(å¯é€‰)ã€‚\n"
            "è‹¥é€‰æ‹©è¡ŒåŠ¨ï¼Œåªèƒ½ä»ç»™å®šå·¥å…·ä¸­é€‰ä¸€ä¸ªï¼›è‹¥ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œè®¾ç½® is_final=true å¹¶å¡«å†™ final_answerã€‚\n\n"
            f"å·¥å…·ç›®å½•ï¼š\n{tool_catalog}\n\n"
            f"é—®é¢˜ï¼š\n{question}\n\n"
            f"è¿‡å¾€æ­¥éª¤ä¸è§‚å¯Ÿï¼ˆä¾›å‚è€ƒï¼‰ï¼š\n{history_text}\n"
        )
        # è¿”å›ç»“æ„åŒ–çš„â€œæ€è€ƒ/å†³ç­–â€ï¼Œå¹¶æŠŠè¯¥æ­¥åŠ å…¥çŠ¶æ€åºåˆ—
        step = reasoner_llm.invoke(prompt)
        return {"steps": state["steps"] + [step.model_dump()]}
    return _node

def make_actor_node():
    """
    Actorï¼šå¦‚æœ‰è¡ŒåŠ¨ï¼ˆactionï¼‰ï¼Œæ‰§è¡Œå¯¹åº”çš„æœ¬åœ°å·¥å…·ï¼Œå¹¶è®°å½• observationï¼›
    å¦‚ is_final=Trueï¼Œåˆ™ä¸æ‰§è¡Œä»»ä½•å·¥å…·ï¼Œä»…ä¿ç•™æ¨¡å‹æœ€ç»ˆå›ç­”ã€‚
    """
    def _node(state: ReActState) -> dict:
        steps = list(state.get("steps", []))
        last = steps[-1] if steps else {}
        action = last.get("action")
        action_input = last.get("action_input") or {}
        is_final = bool(last.get("is_final"))
        final_answer = last.get("final_answer") or ""
        if is_final:
            # ç›´æ¥è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸å†è¡ŒåŠ¨
            return {"final_answer": final_answer}
        if not action:
            # æ²¡æœ‰æŒ‡å®šå·¥å…·ï¼Œåˆ™è®°å½•ç©ºè§‚å¯Ÿ
            last["observation"] = {"info": "no_action"}
            steps[-1] = last
            return {"steps": steps}
        func = TOOLS_REGISTRY.get(action)
        if func is None:
            # éæ³•å·¥å…·åï¼šè®°å½•é”™è¯¯è§‚å¯Ÿï¼Œä¾¿äºä¸‹ä¸€è½®æ€è€ƒè°ƒæ•´
            last["observation"] = {"error": f"unknown tool '{action}'"}
            steps[-1] = last
            return {"steps": steps}
        try:
            # æ‰§è¡Œæœ¬åœ°å·¥å…·ï¼Œäº§å‡ºç»“æ„åŒ–è§‚å¯Ÿ
            output = func(action_input)
            last["observation"] = output
        except Exception as e:
            last["observation"] = {"error": str(e)}
        steps[-1] = last
        return {"steps": steps}
    return _node

# =========================
# æ¡ä»¶åˆ¤æ–­ï¼šç»§ç»­å¾ªç¯è¿˜æ˜¯ç»“æŸ
# =========================
def should_continue(state: ReActState) -> str:
    """
    æ¡ä»¶è¾¹æ˜ å°„å‡½æ•°ï¼š
    - è‹¥æœ€åä¸€æ­¥ is_final=True â†’ ç»“æŸ
    - è‹¥è¿­ä»£æ¬¡æ•°è¾¾åˆ°ä¸Šé™ â†’ ç»“æŸ
    - å¦åˆ™ç»§ç»­ Reasoner
    """
    steps = state.get("steps", [])
    if steps:
        last = steps[-1]
        if bool(last.get("is_final")):
            return "end"
    if len(steps) >= MAX_ITERATIONS:
        # é˜²æ­¢æ— é™å¾ªç¯ï¼šè¾¾åˆ°ä¸Šé™åå¼ºåˆ¶ç»“æŸ
        return "end"
    return "reason"

# =========================
# æ„å»ºä¸è¿è¡Œå·¥ä½œæµ
# =========================
def build_app(llm: "ModelScopeChat"):
    """
    æ„å»ºå¾ªç¯å·¥ä½œæµï¼šreason â†’ act â†’ [ç»§ç»­/ç»“æŸ]
    - ä½¿ç”¨æ¡ä»¶è¾¹åœ¨ act åå†³å®šä¸‹ä¸ªèŠ‚ç‚¹ï¼ˆç»§ç»­ reason æˆ– ENDï¼‰
    è®¾è®¡æ„å›¾ï¼š
    - â€œæ€è€ƒâ€èŠ‚ç‚¹åªè´Ÿè´£ç»“æ„åŒ–å†³ç­–ï¼ˆthought / action / is_final / final_answerï¼‰
    - â€œè¡ŒåŠ¨â€èŠ‚ç‚¹åªè´Ÿè´£åŸºäºå†³ç­–æ‰§è¡Œæœ¬åœ°å·¥å…·å¹¶å†™å…¥ observation
    - æ¡ä»¶è¾¹æ˜¯å…³é”®ï¼šæ ¹æ®æœ€æ–°æ­¥éª¤æ˜¯å¦ is_final æˆ–æ˜¯å¦è¾¾åˆ°ä¸Šé™æ¥å†³å®šæ˜¯å¦ç»§ç»­
    ä¼ªä»£ç ï¼ˆç®€åŒ–è¡¨è¾¾ï¼‰ï¼š
      state = {question, steps: [], final_answer: None}
      loop:
        step = reason(state)              # äº§å‡ºç»“æ„åŒ– {thought, action, action_input, is_final, final_answer}
        state = act(state + step)         # æ‰§è¡Œå·¥å…·å¹¶å†™å…¥ observationï¼›æˆ–ç›´æ¥å†™å…¥ final_answer
        if is_final(step) or reach_limit: # ç»ˆæ­¢æ¡ä»¶
            break
        else:
            continue                      # å›åˆ° reason
    """
    graph = StateGraph(ReActState)
    # æ·»åŠ ä¸¤ä¸ªæ ¸å¿ƒèŠ‚ç‚¹ï¼šæ€è€ƒï¼ˆreasonï¼‰ä¸è¡ŒåŠ¨ï¼ˆactï¼‰
    graph.add_node("reason", make_reasoner_node(llm))
    graph.add_node("act", make_actor_node())
    # è®¾ç½®å…¥å£ä¸ºâ€œæ€è€ƒâ€ï¼Œç„¶åå›ºå®šè¾¹ reasonâ†’act
    graph.set_entry_point("reason")
    graph.add_edge("reason", "act")
    # æ¡ä»¶è¾¹ï¼šact ä¹‹åæ ¹æ®çŠ¶æ€å†³å®šç»§ç»­ reason æˆ– END
    def _decide_next(state: ReActState):
        # æ ¹æ® should_continue çš„è¿”å›å€¼ï¼ˆ'reason' æˆ– 'end'ï¼‰è¿›è¡Œè·¯ç”±
        # æ³¨æ„ï¼šadd_conditional_edges çš„ç¬¬ä¸‰ä¸ªå‚æ•°æä¾›äº†æ ‡ç­¾åˆ°èŠ‚ç‚¹/END çš„æ˜ å°„
        # - 'reason' â†’ èŠ‚ç‚¹ 'reason'ï¼ˆç»§ç»­å¾ªç¯ï¼‰
        # - 'end'    â†’ ENDï¼ˆç»ˆæ­¢ï¼‰
        return should_continue(state)
    graph.add_conditional_edges("act", _decide_next, {"reason": "reason", "end": END})
    return graph.compile()

def run_workflow(app, question: str) -> ReActState:
    """
    æ‰§è¡Œå·¥ä½œæµå¹¶è¿”å›æœ€ç»ˆçŠ¶æ€ï¼›æ‰“å°çŠ¶æ€å­—æ®µå˜åŒ–ä¾¿äºå­¦ä¹ ç†è§£
    - åˆå§‹çŠ¶æ€åŒ…å«ï¼šquestion ä¸ç©º steps
    - æµå¼æ‰§è¡Œï¼šreason â†’ act â†’ reason â†’ ... â†’ END
    å…³é”®ç‚¹ï¼š
    - ä½¿ç”¨ app.stream è·å–æ¯ä¸€æ­¥çš„çŠ¶æ€å¢é‡ï¼Œå¯ç›´è§‚çœ‹åˆ°çŠ¶æ€é”®çš„å˜åŒ–
    - DEBUG æ¨¡å¼ä¸‹ï¼Œæ‰“å°æœ€è¿‘ä¸€æ­¥çš„ç»“æ„åŒ–å†…å®¹ï¼ˆå« thought / action / observationï¼‰
    - å·¥ä½œæµç»“æŸåæ‰“å°â€œä¸€æ¬¡ä¼šè¯å›æ”¾â€å’Œâ€œæœ€ç»ˆå›ç­”â€ï¼Œä¾¿äºæ•™å­¦å±•ç¤º
    ä¼ªä»£ç ï¼ˆçŠ¶æ€æ‰“å°é€»è¾‘ï¼‰ï¼š
      initial = {question, steps: [], final_answer: None}
      for update in app.stream(initial):
          print(keys(update))              # å¯è§çš„é”®
          last = tail(update.steps)        # æœ€è¿‘ä¸€æ­¥
          print(JSON(last))                # æ€è€ƒ/è¡ŒåŠ¨/è§‚å¯Ÿé¢„è§ˆ
      print("å®Œæˆ")
    """
    initial = {"question": question, "steps": [], "final_answer": None}
    final_state: Optional[ReActState] = None
    # ä»¥æµå¼æ–¹å¼é€æ­¥è·å–çŠ¶æ€å¢é‡ï¼Œæ¯æ¬¡åŒ…å«å½“å‰å¯è§çš„é”®å€¼
    for update in app.stream(initial, stream_mode="values"):
        final_state = update
        if DEBUG:
            console.print(f"[bold]çŠ¶æ€æ›´æ–°ï¼š[/bold]{list(update.keys())}")
            # å±•ç¤ºæœ€è¿‘ä¸€æ­¥ï¼ˆthought/action/observationï¼‰
            last = (update.get("steps") or [])[-1] if (update.get("steps") or []) else {}
            if last:
                preview = json.dumps(last, ensure_ascii=False, indent=2)
                console.print(preview)
    console.print("[bold green]âœ… ReAct å·¥ä½œæµå®Œæˆ[/bold green]")
    return final_state or initial

# =========================
# è¾“å‡ºè¾…åŠ©
# =========================
def print_session_summary(state: ReActState) -> None:
    """æ‰“å°æ‰€æœ‰æ­¥éª¤ä¸æœ€ç»ˆå›ç­”ï¼Œä¾¿äºç›´è§‚æ•™å­¦å±•ç¤º"""
    console.print("--- ### ReAct æ­¥éª¤å›æ”¾ ---")
    steps = state.get("steps", [])
    if steps:
        console.print(json.dumps(steps, ensure_ascii=False, indent=2))
    console.print("--- ### æœ€ç»ˆå›ç­” ---")
    ans = state.get("final_answer") or ""
    if ans:
        console.print(ans)

# =========================
# CLI ä¸å…¥å£
# =========================
def parse_args() -> argparse.Namespace:
    """å‘½ä»¤è¡Œå‚æ•°è§£æï¼šé—®é¢˜ã€æ•™å­¦æ—¥å¿—ã€ä»¤ç‰Œæµã€è¿­ä»£ä¸Šé™"""
    parser = argparse.ArgumentParser(description="ReActï¼ˆReason + Actï¼‰æ¶æ„ï¼šå¯è¿è¡Œå­¦ä¹ è„šæœ¬")
    parser.add_argument(
        "--question",
        type=str,
        default="è¯·è®¡ç®—è¡¨è¾¾å¼ 12*(3+4)ï¼Œå¹¶ç”¨ä¸€å¥è¯è¯´æ˜ç»“æœã€‚",
        help="ç”¨æˆ·é—®é¢˜ï¼ˆReAct å°†æŒ‰éœ€é€‰æ‹©å·¥å…·æˆ–ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼‰",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¼€å¯è¯¦ç»†æ•™å­¦æ—¥å¿—",
    )
    parser.add_argument(
        "--stream",
        action="store_true",
        help="å®æ—¶æ‰“å°æ¨¡å‹ä»¤ç‰Œï¼ˆç»“æ„åŒ– JSON ä¹Ÿå°†è¾¹æ¥æ”¶è¾¹å±•ç¤ºï¼‰",
    )
    parser.add_argument(
        "--max-iters",
        type=int,
        default=6,
        help="ReAct å¾ªç¯çš„æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆè¾¾åˆ°ä¸Šé™å°†å¼ºåˆ¶ç»“æŸï¼‰",
    )
    return parser.parse_args()

def main():
    load_dotenv()
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_PROJECT"] = "Agentic Architecture - ReAct (ModelScope)"
    if not os.environ.get("MODELSCOPE_API_KEY"):
        console.print("[bold red]MODELSCOPE_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨é¡¹ç›®æ ¹ç›®å½•é…ç½® .env[/bold red]")
    if not os.environ.get("LANGCHAIN_API_KEY"):
        console.print("[bold yellow]æç¤ºï¼šæœªè®¾ç½® LANGCHAIN_API_KEYï¼ŒLangSmith è¿½è¸ªå°†ä¸å¯ç”¨[/bold yellow]")
    args = parse_args()
    global DEBUG, STREAM_TOKENS, MAX_ITERATIONS
    DEBUG = bool(args.debug)
    STREAM_TOKENS = bool(args.stream)
    try:
        MAX_ITERATIONS = int(args.max_iters)
    except Exception:
        MAX_ITERATIONS = 6
    llm = init_llm()
    if DEBUG:
        console.print("[bold cyan]æ¨ç†æœåŠ¡é…ç½®[/bold cyan]:")
        console.print(f"base_url={llm.base_url}")
        console.print(f"model_id={llm.model}")
    app = build_app(llm)
    final_state = run_workflow(app, args.question)
    # Reasoner å¯èƒ½åœ¨æœ€åä¸€æ­¥ç›´æ¥å†™å…¥ final_answerï¼›è‹¥ç©ºåˆ™å°è¯•ä»æœ€åä¸€æ­¥è¯»å‡º
    if not final_state.get("final_answer"):
        steps = final_state.get("steps") or []
        if steps and steps[-1].get("final_answer"):
            final_state["final_answer"] = steps[-1]["final_answer"]
    print_session_summary(final_state)

if __name__ == "__main__":
    main()
