# ğŸ“˜ Agentic Architectures 4: Planning

# æœ¬è„šæœ¬æ¢ç´¢è§„åˆ’æ¶æ„ï¼Œå¹¶å°†å…¶ä¸ReActæ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
# è§„åˆ’æ™ºèƒ½ä½“åœ¨é‡‡å–ä»»ä½•è¡ŒåŠ¨ä¹‹å‰ï¼Œé¦–å…ˆå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œ
# ä¸ºå¤šæ­¥éª¤ä»»åŠ¡æä¾›ç»“æ„å’Œæ•ˆç‡ã€‚

# é˜¶æ®µ 0ï¼šåŸºç¡€ä¸è®¾ç½®
# å®‰è£…å¿…è¦çš„åº“ï¼š
# !pip install -q -U langchain-nebius langchain langgraph rich python-dotenv langchain-tavily

import os
import re
import json
from typing import List, Annotated, TypedDict, Optional
from dotenv import load_dotenv

# LangChain components
from langchain_core.messages import BaseMessage, ToolMessage, SystemMessage, HumanMessage, AIMessage
from pydantic import BaseModel, Field
from langchain_core.tools import tool
from langchain_tavily import TavilySearch
from openai import OpenAI, RateLimitError, APIError
import logging
from rich.logging import RichHandler

# LangGraph components
from langgraph.graph import StateGraph, END
from langgraph.graph.message import AnyMessage, add_messages
from langgraph.prebuilt import ToolNode, tools_condition

# For pretty printing
from rich.console import Console
from rich.markdown import Markdown

# --- API Key and Tracing Setup ---
load_dotenv()

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "Agentic Architecture - Planning (ModelScope)"

# Check that the keys are set
for key in ["MODELSCOPE_API_KEY", "LANGCHAIN_API_KEY", "TAVILY_API_KEY"]:
    if not os.environ.get(key):
        print(f"æœªæ‰¾åˆ°{key}ã€‚è¯·åˆ›å»º.envæ–‡ä»¶å¹¶è®¾ç½®è¯¥å˜é‡ã€‚")

print("ç¯å¢ƒå˜é‡å·²åŠ è½½ï¼Œè¿½è¸ªå·²è®¾ç½®å®Œæˆã€‚")

# Define ModelScopeChat class for compatibility
class ModelScopeChat:
    """
    ModelScope çš„ OpenAI å…¼å®¹æ¥å£é€‚é…å™¨
    - æä¾› invoke(prompt) åŸºæœ¬è°ƒç”¨
    - æä¾› with_structured_output(PydanticModel) çš„ç»“æ„åŒ–è¾“å‡ºåŒ…è£…
    """
    def __init__(self, base_url: str, api_key: str, model: str, fallback_model: Optional[str] = None, temperature: float = 0.2, extra_body: Optional[dict] = None):
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model = model
        self.fallback_model = fallback_model
        self.base_url = base_url
        self.temperature = temperature
        self.extra_body = extra_body or {}
        self.switched = False

    def invoke(self, prompt: str):
        try:
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                stream=False,
                extra_body=self.extra_body,
            )
            return resp.choices[0].message.content
        except (RateLimitError, APIError) as e:
            if not self.switched and self.fallback_model:
                console.print(f"[bold yellow]âš ï¸ ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥ï¼š{e}ï¼Œå°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹[/bold yellow]")
                self.model = self.fallback_model
                self.switched = True
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    stream=False,
                    extra_body=self.extra_body,
                )
                return resp.choices[0].message.content
            else:
                raise

    def with_structured_output(self, pyd_model: type[BaseModel]):
        class _StructuredWrapper:
            def __init__(self, outer: "ModelScopeChat"):
                self.outer = outer

            def invoke(self, prompt: str) -> BaseModel:
                schema = pyd_model.model_json_schema()
                props = schema.get("properties", {})
                required = schema.get("required", [])
                schema_text_lines = []
                for k, v in props.items():
                    t = v.get("type", "string")
                    schema_text_lines.append(f"- {k}: {t}")
                schema_text = "\n".join(schema_text_lines) or "- è¯·æŒ‰æ¨¡å‹å®šä¹‰ç”Ÿæˆå­—æ®µ"
                required_text = ", ".join(required) if required else "æ‰€æœ‰å­—æ®µ"
                system_msg = (
                    "ä½ æ˜¯ä¸€ä¸ªç»“æ„åŒ–è¾“å‡ºç”Ÿæˆå™¨ã€‚åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸¥æ ¼åŒ¹é…ä»¥ä¸‹å­—æ®µä¸ç±»å‹ï¼š\n"
                    f"{schema_text}\n"
                    f"å¿…é¡»åŒ…å«å­—æ®µï¼š{required_text}\n"
                    "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæˆ–å¤šä½™æ–‡æœ¬ï¼ˆä¾‹å¦‚ä»£ç å—æ ‡è®°ã€å‰åç¼€ï¼‰ã€‚"
                )
                messages = [
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt},
                ]
                try:
                    resp = self.outer.client.chat.completions.create(
                        model=self.outer.model,
                        messages=messages,
                        temperature=self.outer.temperature,
                        stream=False,
                        extra_body=self.outer.extra_body,
                    )
                    content = resp.choices[0].message.content or ""
                except (RateLimitError, APIError) as e:
                    if not self.outer.switched and self.outer.fallback_model:
                        console.print(f"[bold yellow]âš ï¸ ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥ï¼š{e}ï¼Œå°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹[/bold yellow]")
                        self.outer.model = self.outer.fallback_model
                        self.outer.switched = True
                        resp = self.outer.client.chat.completions.create(
                            model=self.outer.model,
                            messages=messages,
                            temperature=self.outer.temperature,
                            stream=False,
                            extra_body=self.outer.extra_body,
                        )
                        content = resp.choices[0].message.content or ""
                    else:
                        raise
                def _extract_json(s: str) -> str:
                    m = re.search(r'(\{[\s\S]*\}|\[[\s\S]*\])', s)
                    return m.group(1) if m else "{}"
                raw = content.strip()
                try:
                    data = json.loads(raw)
                except Exception:
                    data = json.loads(_extract_json(raw))
                # å…œåº•å­—æ®µæ˜ å°„
                try:
                    parsed = pyd_model.model_validate(data)
                    return parsed
                except Exception:
                    # DraftCode: å¸¸è§è¿”å› 'function' å­—æ®µï¼Œæ˜ å°„åˆ° 'code'
                    if "code" not in data and "function" in data:
                        data["code"] = data.pop("function")
                    if "explanation" not in data and "desc" in data:
                        data["explanation"] = data.pop("desc")
                    # RefinedCode: å°† 'code' æ˜ å°„ä¸º 'refined_code'
                    if "refined_code" not in data and "code" in data:
                        data["refined_code"] = data.pop("code")
                    # Critique: å¯èƒ½ç»™ 'summary' æ˜ å°„ä¸º 'critique_summary'
                    if "critique_summary" not in data and "summary" in data:
                        data["critique_summary"] = data.pop("summary")
                    parsed = pyd_model.model_validate(data)
                    return parsed

        return _StructuredWrapper(self)

def init_llm() -> ModelScopeChat:
    """
    åˆå§‹åŒ– ModelScope LLMï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰ã€‚
    - å¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼š
      MODELSCOPE_BASE_URLï¼ˆé»˜è®¤ï¼šhttps://api-inference.modelscope.cn/v1ï¼‰
      MODELSCOPE_API_KEY
      MODELSCOPE_MODEL_IDï¼ˆé»˜è®¤ï¼šdeepseek-ai/DeepSeek-V3.2ï¼‰
      MODELSCOPE_MODEL_ID_R1ï¼ˆå¤‡ç”¨æ¨¡å‹ï¼Œå¯é€‰ï¼‰
    - å½“ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥æ—¶ï¼Œä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹ï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
    """
    base_url = os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
    api_key = os.environ.get("MODELSCOPE_API_KEY", "")
    model_id = os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
    fallback_model_id = os.environ.get("MODELSCOPE_MODEL_ID_R1")
    # ä¸ºé¿å…â€œæœªä¿¡ä»»çš„ chat templateâ€é”™è¯¯ï¼Œå¢åŠ ä¿¡ä»»å‚æ•°ï¼›å¹¶è¯·æ±‚ JSON è¾“å‡ºæ ¼å¼
    extra = {
        "enable_thinking": True,
        "trust_request_chat_template": True,
        "response_format": {"type": "json_object"},
    }
    return ModelScopeChat(base_url=base_url, api_key=api_key, model=model_id, fallback_model=fallback_model_id, temperature=0.2, extra_body=extra)

# Phase 1: The Baseline - A Reactive Agent (ReAct)
# We'll rebuild the ReAct agent to compare against the planning agent

console = Console()

# 3. Define the state for our graphs
class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

# ç®€å•çš„æ¨¡æ‹Ÿç½‘ç»œæœç´¢å·¥å…·ç”¨äºæµ‹è¯•
@tool
def web_search(query: str) -> str:
    """æ¨¡æ‹Ÿç½‘ç»œæœç´¢å¹¶è¿”å›æ¨¡æ‹Ÿç»“æœã€‚"""
    console.print(f"--- å·¥å…·ï¼šæœç´¢ '{query}'...")
    
    # å¸¸è§æŸ¥è¯¢çš„æ¨¡æ‹Ÿç»“æœ
    mock_data = {
        "åŒ—äº¬äººå£": "åŒ—äº¬çš„äººå£çº¦ä¸º2154ä¸‡ï¼ˆ2023å¹´ï¼‰ã€‚",
        "ä¸Šæµ·äººå£": "ä¸Šæµ·çš„äººå£çº¦ä¸º2487ä¸‡ï¼ˆ2023å¹´ï¼‰ã€‚",
        "å¹¿å·äººå£": "å¹¿å·çš„äººå£çº¦ä¸º1873ä¸‡ï¼ˆ2023å¹´ï¼‰ã€‚",
        "æ·±åœ³äººå£": "æ·±åœ³çš„äººå£çº¦ä¸º1756ä¸‡ï¼ˆ2023å¹´ï¼‰ã€‚",
        "ä¸­å›½äººå£": "ä¸­å›½çš„äººå£çº¦ä¸º14.12äº¿ï¼ˆ2023å¹´ï¼‰ã€‚"
    }
    
    # è¿”å›æ¨¡æ‹Ÿæ•°æ®æˆ–é€šç”¨å“åº”
    for key, value in mock_data.items():
        if key in query:
            return value
    
    return f"æ¨¡æ‹Ÿæœç´¢ç»“æœï¼š{query}"

# 3. Define the LLM and bind it to our custom tool
llm = init_llm()

# Create a wrapper for ModelScopeChat to work with bind_tools
class ModelScopeChatWithTools:
    def __init__(self, llm_instance: ModelScopeChat, tools: list):
        self.llm = llm_instance
        self.tools = tools
    
    def invoke(self, messages: list):
        # Convert messages to a single prompt string
        prompt = ""
        for msg in messages:
            if isinstance(msg, SystemMessage):
                prompt += f"[SYSTEM] {msg.content}\n"
            elif isinstance(msg, HumanMessage):
                prompt += f"[USER] {msg.content}\n"
            elif isinstance(msg, AIMessage):
                prompt += f"[ASSISTANT] {msg.content}\n"
            else:
                # For other message types, try to extract content
                try:
                    prompt += f"[MESSAGE] {msg.content}\n"
                except:
                    continue
        
        # Generate response
        response = self.llm.invoke(prompt)
        
        # Create a response object that extends LangChain's AIMessage
        from langchain_core.messages import AIMessage
        
        # Parse tool calls if any
        tool_calls = []
        if "web_search" in response and "query" in response:
            try:
                tool_data = json.loads(response)
                if "tool_call" in tool_data:
                    tool_calls = [{
                        "name": tool_data["tool_call"]["name"],
                        "args": tool_data["tool_call"]["args"]
                    }]
            except:
                # If parsing fails, check for simple pattern
                import re
                match = re.search(r'web_search\(query=[\'"]([^\'"]+)[\'"]\)', response)
                if match:
                    tool_calls = [{
                        "name": "web_search",
                        "args": {"query": match.group(1)}
                    }]
        
        # Create and return an actual AIMessage object
        return AIMessage(content=response, tool_calls=tool_calls)

# Use the wrapper with tools
llm_with_tools = ModelScopeChatWithTools(llm, [web_search])

# 4. Agent node with a system prompt to force one tool call at a time
def react_agent_node(state: AgentState):
    console.print("--- ååº”å¼æ™ºèƒ½ä½“ï¼šæ€è€ƒä¸­... ---")
    
    messages_with_system_prompt = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„ç ”ç©¶åŠ©æ‰‹ã€‚æ¯æ¬¡å¿…é¡»ä¸”åªèƒ½è°ƒç”¨ä¸€ä¸ªå·¥å…·ã€‚ä¸è¦åœ¨ä¸€æ¬¡å¯¹è¯ä¸­è°ƒç”¨å¤šä¸ªå·¥å…·ã€‚æ”¶åˆ°å·¥å…·ç»“æœåï¼Œä½ å°†å†³å®šä¸‹ä¸€æ­¥æ“ä½œã€‚")
    ] + state["messages"]

    response = llm_with_tools.invoke(messages_with_system_prompt)
    
    return {"messages": [response]}

# 5. Use our custom tool in the ToolNode
tool_node = ToolNode([web_search])

# The ReAct graph with its characteristic loop
react_graph_builder = StateGraph(AgentState)
react_graph_builder.add_node("agent", react_agent_node)
react_graph_builder.add_node("tools", tool_node)
react_graph_builder.set_entry_point("agent")
react_graph_builder.add_conditional_edges("agent", tools_condition)
react_graph_builder.add_edge("tools", "agent")

react_agent_app = react_graph_builder.compile()
print("ååº”å¼(ReAct)æ™ºèƒ½ä½“ç¼–è¯‘æˆåŠŸã€‚")

# åœ¨ä»¥è§„åˆ’ä¸ºä¸­å¿ƒçš„æŸ¥è¯¢ä¸Šæµ‹è¯•ååº”å¼æ™ºèƒ½ä½“
print("\nåœ¨ä»¥è§„åˆ’ä¸ºä¸­å¿ƒçš„æŸ¥è¯¢ä¸Šæµ‹è¯•ååº”å¼æ™ºèƒ½ä½“ï¼š")
print("'æŸ¥æ‰¾åŒ—äº¬ã€ä¸Šæµ·å’Œå¹¿å·çš„äººå£ã€‚ ")
print("ç„¶åè®¡ç®—å®ƒä»¬çš„æ€»äººå£ã€‚ ")
print("æœ€åï¼Œå°†æ€»äººå£ä¸ä¸­å›½äººå£è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯´æ˜å“ªä¸ªæ›´å¤§ã€‚'")

react_result = react_agent_app.invoke({
    "messages": [
        ("human", "æŸ¥æ‰¾åŒ—äº¬ã€ä¸Šæµ·å’Œå¹¿å·çš„äººå£ã€‚ "
                 "ç„¶åè®¡ç®—å®ƒä»¬çš„æ€»äººå£ã€‚ "
                 "æœ€åï¼Œå°†æ€»äººå£ä¸ä¸­å›½äººå£è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯´æ˜å“ªä¸ªæ›´å¤§ã€‚")
    ]
})

# é˜¶æ®µ2ï¼šè§„åˆ’æ™ºèƒ½ä½“
# ç°åœ¨æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªè§„åˆ’æ™ºèƒ½ä½“ï¼Œå®ƒä¼šåœ¨æ‰§è¡Œå‰åˆ›å»ºä¸€ä¸ªé€æ­¥è®¡åˆ’

# å®šä¹‰è®¡åˆ’çš„æ¨¡å¼
plan_schema_prompt = """
æˆ‘éœ€è¦ä½ åˆ›å»ºä¸€ä¸ªè§£å†³æ­¤é—®é¢˜çš„è¯¦ç»†è®¡åˆ’ã€‚ä½ çš„è®¡åˆ’åº”è¯¥æ˜¯ä¸€ä¸ªç¼–å·çš„å…·ä½“æ­¥éª¤åˆ—è¡¨ã€‚
æ¯ä¸ªæ­¥éª¤åº”è¯¥æ˜¯ä¸€ä¸ªç®€å•çš„ä»»åŠ¡ï¼Œå¯ä»¥é€šè¿‡å•æ¬¡å·¥å…·è°ƒç”¨æˆ–ç®€å•è®¡ç®—å®Œæˆã€‚

ç¤ºä¾‹ï¼š
1. æœç´¢åŒ—äº¬çš„äººå£ã€‚
2. æœç´¢ä¸Šæµ·çš„äººå£ã€‚
3. æœç´¢å¹¿å·çš„äººå£ã€‚
4. å°†è¿™ä¸‰ä¸ªäººå£ç›¸åŠ ï¼Œå¾—åˆ°æ€»äººå£ã€‚
5. æœç´¢ä¸­å›½çš„äººå£ã€‚
6. å°†æ€»äººå£ä¸ä¸­å›½äººå£è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯´æ˜å“ªä¸ªæ›´å¤§ã€‚

è¯·ä»¥ç¼–å·åˆ—è¡¨çš„å½¢å¼æä¾›ä½ çš„è®¡åˆ’ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚
"""

# Planner node - creates a detailed plan before execution
def planner_node(state: AgentState):
    console.print("--- è§„åˆ’å™¨ï¼šåˆ›å»ºè®¡åˆ’ä¸­... ---")
    
    # Add the plan schema prompt to the messages
    plan_prompt = [SystemMessage(content=plan_schema_prompt)] + state["messages"]
    
    # Convert messages to prompt string
    prompt_str = ""
    for msg in plan_prompt:
        if isinstance(msg, SystemMessage):
            prompt_str += f"[SYSTEM] {msg.content}\n"
        elif isinstance(msg, HumanMessage):
            prompt_str += f"[USER] {msg.content}\n"
        elif isinstance(msg, AIMessage):
            prompt_str += f"[ASSISTANT] {msg.content}\n"
        else:
            try:
                prompt_str += f"[MESSAGE] {msg.content}\n"
            except:
                continue
    
    # Generate the plan using the LLM
    plan_response = llm.invoke(prompt_str)
    
    return {"messages": [AIMessage(content=plan_response)]}

# Plan execution node - executes the plan step by step
def executor_node(state: AgentState):
    console.print("--- æ‰§è¡Œå™¨ï¼šæ‰§è¡Œè®¡åˆ’ä¸­... ---")
    
    # è·å–æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆåº”è¯¥æ˜¯è®¡åˆ’ï¼‰
    plan = state["messages"][-1].content
    
    # ä»è®¡åˆ’ä¸­æå–æ­¥éª¤
    steps = re.findall(r"\d+\.\s*(.+)", plan)
    
    # æ‰§è¡Œæ¯ä¸ªæ­¥éª¤
    execution_results = []
    for step in steps:
        console.print(f"\n--- æ‰§è¡Œæ­¥éª¤ï¼š{step} ---")
        
        # Create a message for this step
        step_message = [("human", step)]
        
        # Execute the step using the reactive agent
        step_result = react_agent_app.invoke({"messages": step_message})
        
        # Add the result to our execution results
        execution_results.append(step_result["messages"][-1].content)
    
    # Return the execution results
    return {"messages": [("assistant", f"æ‰§è¡Œå®Œæˆã€‚ç»“æœï¼š\n{chr(10).join(execution_results)}")]}

# Synthesis node - combines the results into a final answer
def synthesizer_node(state: AgentState):
    console.print("--- åˆæˆå™¨ï¼šåˆ›å»ºæœ€ç»ˆç­”æ¡ˆä¸­... ---")
    
    # è·å–æ‰€æœ‰æ¶ˆæ¯
    messages = state["messages"]
    
    # åˆ›å»ºåˆæˆæç¤º
    synthesis_prompt_list = [SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸Šè¿°æ‰§è¡Œç»“æœæä¾›æ¸…æ™°ç®€æ´çš„æœ€ç»ˆç­”æ¡ˆã€‚")] + messages
    
    # Convert messages to prompt string
    synthesis_prompt_str = ""
    for msg in synthesis_prompt_list:
        if isinstance(msg, SystemMessage):
            synthesis_prompt_str += f"[SYSTEM] {msg.content}\n"
        elif isinstance(msg, HumanMessage):
            synthesis_prompt_str += f"[USER] {msg.content}\n"
        elif isinstance(msg, AIMessage):
            synthesis_prompt_str += f"[ASSISTANT] {msg.content}\n"
        else:
            try:
                synthesis_prompt_str += f"[MESSAGE] {msg.content}\n"
            except:
                continue
    
    # Generate the final answer
    final_answer = llm.invoke(synthesis_prompt_str)
    
    return {"messages": [AIMessage(content=final_answer)]}

# Create the planning agent graph
planning_graph_builder = StateGraph(AgentState)
planning_graph_builder.add_node("planner", planner_node)
planning_graph_builder.add_node("executor", executor_node)
planning_graph_builder.add_node("synthesizer", synthesizer_node)

# Define the flow
planning_graph_builder.set_entry_point("planner")
planning_graph_builder.add_edge("planner", "executor")
planning_graph_builder.add_edge("executor", "synthesizer")
planning_graph_builder.add_edge("synthesizer", END)

# ç¼–è¯‘å›¾
planning_agent_app = planning_graph_builder.compile()
print("è§„åˆ’æ™ºèƒ½ä½“ç¼–è¯‘æˆåŠŸã€‚")

if __name__ == "__main__":
    # æµ‹è¯•è§„åˆ’æ™ºèƒ½ä½“
    print("\nåœ¨ç›¸åŒæŸ¥è¯¢ä¸Šæµ‹è¯•è§„åˆ’æ™ºèƒ½ä½“ï¼š")
    print("'æŸ¥æ‰¾åŒ—äº¬ã€ä¸Šæµ·å’Œå¹¿å·çš„äººå£ã€‚ ")
    print("ç„¶åè®¡ç®—å®ƒä»¬çš„æ€»äººå£ã€‚ ")
    print("æœ€åï¼Œå°†æ€»äººå£ä¸ä¸­å›½äººå£è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯´æ˜å“ªä¸ªæ›´å¤§ã€‚'")
    
    planning_result = planning_agent_app.invoke({
        "messages": [
            ("human", "æŸ¥æ‰¾åŒ—äº¬ã€ä¸Šæµ·å’Œå¹¿å·çš„äººå£ã€‚ "
                     "ç„¶åè®¡ç®—å®ƒä»¬çš„æ€»äººå£ã€‚ "
                     "æœ€åï¼Œå°†æ€»äººå£ä¸ä¸­å›½äººå£è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯´æ˜å“ªä¸ªæ›´å¤§ã€‚")
        ]
    })
    
    # æ¯”è¾ƒç»“æœ
    print("\n=== æ¯”è¾ƒç»“æœ ===")
    print("ååº”å¼(ReAct)æ™ºèƒ½ä½“ç»“æœï¼š")
    print(react_result["messages"][-1].content)
    print("\nè§„åˆ’æ™ºèƒ½ä½“ç»“æœï¼š")
    print(planning_result["messages"][-1].content)