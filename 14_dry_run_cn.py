# -*- coding: utf-8 -*-
"""
å¯è§‚æµ‹ä¸è¯•è·‘å¤–å£³ï¼ˆObservability + Dry-Run Harnessï¼‰æ¶æ„çš„å¯è¿è¡Œç¤ºä¾‹

å­¦ä¹ ç›®æ ‡ï¼ˆçœ‹å®Œä½ èƒ½åšåˆ°ä»€ä¹ˆï¼‰ï¼š
- ç†è§£ã€Œå…ˆè¯•è·‘ã€å†äººå·¥å®¡æ ¸ã€å†çœŸå®æ‰§è¡Œã€çš„å®‰å…¨å‘å¸ƒæµç¨‹
- æŒæ¡å¸¦ dry_run æ ‡å¿—çš„å·¥å…·ä¸ LangGraph æ¡ä»¶è¾¹ï¼ˆå®¡æ ¸é€šè¿‡/æ‹’ç»ï¼‰
- å­¦ä¼šç”¨ ModelScopeChat åšç»“æ„åŒ–è¾“å‡ºï¼ˆæ‹Ÿå‘å¸ƒå†…å®¹ï¼‰ä¸äººæœºå®¡æ ¸äº¤äº’
- èƒ½è¿è¡Œä¼ä¸šç¤¾äº¤åª’ä½“æ‹Ÿå‘å¸–æ¼”ç¤ºï¼šç”Ÿæˆ â†’ è¯•è·‘é¢„è§ˆ â†’ è¾“å…¥ approve/reject â†’ æ‰§è¡Œæˆ–å–æ¶ˆ

æ ¸å¿ƒæ¦‚å¿µé€Ÿè§ˆï¼š
- è¯•è·‘ï¼ˆDry Runï¼‰ï¼šå·¥å…·åœ¨ dry_run=True ä¸‹åªè¾“å‡ºã€Œå°†è¦æ‰§è¡Œçš„åŠ¨ä½œã€ä¸æ—¥å¿—ï¼Œä¸äº§ç”ŸçœŸå®å‰¯ä½œç”¨
- äººæœºå®¡æ ¸ï¼šå°†è¯•è·‘ç»“æœå±•ç¤ºç»™æ“ä½œå‘˜ï¼Œåªæœ‰è¾“å…¥ approve æ‰æ‰§è¡ŒçœŸå®åŠ¨ä½œ
- é€‚ç”¨åœºæ™¯ï¼šå‘å¸–ã€å‘é‚®ä»¶ã€æ”¹æ•°æ®åº“ç­‰ä¸å¯é€†æ“ä½œçš„å‰ç½®æ ¡éªŒ

è¿è¡Œå‰å‡†å¤‡ï¼š
- é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` å¹¶é…ç½®ï¼š
  - `MODELSCOPE_API_KEY`ï¼ˆå¿…éœ€ï¼‰
  - `MODELSCOPE_BASE_URL`ã€`MODELSCOPE_MODEL_ID`ï¼ˆå¯é€‰ï¼Œæœ‰é»˜è®¤ï¼‰
  - ä»…ä½¿ç”¨ä¸»æ¨¡å‹ï¼ˆMODELSCOPE_MODEL_IDï¼Œé»˜è®¤ DeepSeek-V3.2ï¼‰ï¼Œä¸è€ƒè™‘å¤‡ç”¨æ¨¡å‹

å¦‚ä½•è¿è¡Œï¼š
- ç›´æ¥è¿è¡Œé»˜è®¤ç¤ºä¾‹ï¼š`python 14_dry_run_cn.py`
- è‡ªå®šä¹‰è¯·æ±‚ï¼š`python 14_dry_run_cn.py --request "ä¸ºæˆ‘ä»¬çš„æ–° AI äº§å“å†™ä¸€æ¡å‘å¸ƒå…¬å‘Š"`

é˜…è¯»å»ºè®®ï¼š
- å…ˆçœ‹ã€ŒçŠ¶æ€ä¸ç»“æ„åŒ–æ¨¡å‹ã€ã€Œè¯•è·‘å·¥å…·ã€ï¼Œå†çœ‹ã€Œæè®®/è¯•è·‘/å®¡æ ¸/æ‰§è¡Œã€èŠ‚ç‚¹ä¸æ¡ä»¶è¾¹ã€‚
"""

import os
import argparse
import datetime
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict
from rich.console import Console
from rich.panel import Panel
from openai import OpenAI
from openai import RateLimitError, APIError

# =========================
# 1) æ•°æ®ç»“æ„ä¸æ¨¡å‹å®šä¹‰ï¼ˆPydantic v2ï¼‰
# =========================

class SocialMediaPost(BaseModel):
    """æ‹Ÿå‘å¸ƒçš„ç¤¾äº¤åª’ä½“å¸–å­ï¼ˆç»“æ„åŒ–ï¼‰ã€‚"""
    content: str = Field(description="å¸–å­æ­£æ–‡")
    hashtags: List[str] = Field(description="æ ‡ç­¾åˆ—è¡¨ï¼Œä¸å« #")


# æ™ºèƒ½ä½“çŠ¶æ€ï¼ŒåŒ…å«ç”¨æˆ·è¯·æ±‚ã€æ‹Ÿå‘å¸ƒå¸–å­ã€è¯•è·‘æ—¥å¿—ã€å®¡æ ¸å†³å®šã€æœ€ç»ˆçŠ¶æ€
class AgentState(TypedDict):
    user_request: str  # ç”¨æˆ·è¯·æ±‚
    proposed_post: Optional[SocialMediaPost]  # æ‹Ÿå‘å¸ƒå¸–å­
    dry_run_log: Optional[str]  # è¯•è·‘æ—¥å¿—
    review_decision: Optional[str]  # å®¡æ ¸å†³å®š
    final_status: str  # æœ€ç»ˆçŠ¶æ€


def initial_state(request: str) -> Dict[str, Any]:
    """æŒ‰ AgentState çš„ schema æ„é€ åˆå§‹çŠ¶æ€ï¼šä»… user_request æœ‰å€¼ï¼Œå…¶ä½™ä¸ºå ä½ã€‚"""
    return {
        "user_request": request,
        "proposed_post": None,
        "dry_run_log": None,
        "review_decision": None,
        "final_status": "",
    }


# =========================
# 2) è¯•è·‘å·¥å…·ä¸ LLM
# =========================

console = Console()
DEBUG: bool = False


class SocialMediaAPI:
    """æ”¯æŒ dry_run çš„æ¨¡æ‹Ÿå‘å¸– APIï¼šdry_run=True åªæ‰“æ—¥å¿—ä¸çœŸå®å‘å¸–ã€‚"""
    def publish_post(self, post: SocialMediaPost, dry_run: bool = True) -> Dict[str, Any]:
        ts = datetime.datetime.now().isoformat()
        hashtags_str = " ".join(f"#{h}" for h in post.hashtags)
        full_text = f"{post.content}\n\n{hashtags_str}"
        if dry_run:
            log = f"[è¯•è·‘] {ts} å°†å‘å¸ƒä»¥ä¸‹å†…å®¹ï¼š\n--- é¢„è§ˆ ---\n{full_text}\n--- ç»“æŸ ---"
            console.print(Panel(log, title="[yellow]è¯•è·‘æ—¥å¿—[/yellow]", border_style="yellow"))
            return {"status": "DRY_RUN_SUCCESS", "log": log, "proposed_post": full_text}
        log = f"[æ­£å¼] {ts} å·²å‘å¸ƒã€‚"
        console.print(Panel(log, title="[green]æ­£å¼æ‰§è¡Œæ—¥å¿—[/green]", border_style="green"))
        return {"status": "LIVE_SUCCESS", "log": log, "post_id": f"post_{hash(full_text) & 0x7FFFFFFF}"}


social_media_tool = SocialMediaAPI()


class ModelScopeChat:
    """
    ModelScope çš„ OpenAI å…¼å®¹æ¥å£ï¼šinvokeã€with_structured_outputã€‚
    ä»…ä½¿ç”¨ä¸»æ¨¡å‹ï¼ˆMODELSCOPE_MODEL_IDï¼Œé»˜è®¤ deepseek-ai/DeepSeek-V3.2ï¼‰ï¼Œä¸è€ƒè™‘å¤‡ç”¨æ¨¡å‹ï¼›æ—  API å¯†é’¥æ—¶è¿”å›æ¨¡æ‹Ÿå“åº”ã€‚
    """
    def __init__(self, base_url: str = None, api_key: str = None, model: str = None, temperature: float = 0.5, extra_body: Optional[dict] = None):
        self.base_url = base_url or os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
        self.api_key = api_key or os.environ.get("MODELSCOPE_API_KEY")
        self.model = model or os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
        self.temperature = temperature
        self.extra_body = extra_body or {"enable_thinking": True, "trust_request_chat_template": True, "response_format": {"type": "json_object"}}
        if not self.api_key:
            console.print("[bold yellow]âš ï¸ æœªè®¾ç½® MODELSCOPE_API_KEYï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”[/bold yellow]")
            self.client = None
        else:
            self.client = OpenAI(base_url=self.base_url, api_key=self.api_key)

    def invoke(self, prompt: str) -> str:
        if not self.client:
            return "ï¼ˆæœªé…ç½® APIï¼Œæ¨¡æ‹Ÿæ­£æ–‡ä¸ #AI #å‘å¸ƒï¼‰"
        extra = dict(self.extra_body) if self.extra_body else {}
        try:
            r = self.client.chat.completions.create(
                model=self.model, messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature, stream=False, extra_body=extra,
            )
            return (r.choices[0].message.content or "").strip()
        except (RateLimitError, APIError) as e:
            if "balance" in str(e).lower() or "403" in str(e) or "insufficient" in str(e).lower():
                raise RuntimeError(
                    "ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥ï¼šè´¦æˆ·ä½™é¢ä¸è¶³(403)ï¼Œè¯·å……å€¼æˆ–æ£€æŸ¥ MODELSCOPE_API_KEY / MODELSCOPE_MODEL_ID é…ç½®ã€‚"
                ) from e
            raise

    def with_structured_output(self, pyd_model: type[BaseModel]):
        import json, re
        class _Wrap:
            def __init__(self, outer): self.outer = outer
            def invoke(self, prompt: str) -> BaseModel:
                schema = pyd_model.model_json_schema()
                props = schema.get("properties", {})
                req = schema.get("required", [])
                schema_txt = "\n".join(f"- {k}: {v.get('type','string')}" for k, v in props.items()) or "- æŒ‰æ¨¡å‹å­—æ®µ"
                req_txt = ", ".join(req) if req else "æ‰€æœ‰å­—æ®µ"
                system = f"åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸¥æ ¼åŒ¹é…ï¼š\n{schema_txt}\nå¿…é¡»åŒ…å«ï¼š{req_txt}\nä¸è¦è§£é‡Šæˆ–ä»£ç å—ã€‚"
                messages = [{"role": "system", "content": system}, {"role": "user", "content": prompt}]
                if not self.outer.client:
                    return pyd_model(content="ï¼ˆæ¨¡æ‹Ÿæ­£æ–‡ï¼‰", hashtags=["AI", "å‘å¸ƒ"])
                try:
                    r = self.outer.client.chat.completions.create(
                        model=self.outer.model, messages=messages,
                        temperature=self.outer.temperature, stream=False, extra_body=self.outer.extra_body,
                    )
                except (RateLimitError, APIError) as e:
                    if "balance" in str(e).lower() or "403" in str(e) or "insufficient" in str(e).lower():
                        raise RuntimeError(
                            "ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥ï¼šè´¦æˆ·ä½™é¢ä¸è¶³(403)ï¼Œè¯·å……å€¼æˆ–æ£€æŸ¥ MODELSCOPE_API_KEY / MODELSCOPE_MODEL_ID é…ç½®ã€‚"
                        ) from e
                    raise
                raw = (r.choices[0].message.content or "").strip()
                try:
                    data = json.loads(raw)
                except Exception:
                    m = re.search(r"\{[\s\S]*\}", raw)
                    data = json.loads(m.group(0)) if m else {}
                return pyd_model.model_validate(data)
        return _Wrap(self)


def init_llm() -> ModelScopeChat:
    base_url = os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
    api_key = os.environ.get("MODELSCOPE_API_KEY", "")
    model_id = os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
    extra = {"enable_thinking": True, "trust_request_chat_template": True, "response_format": {"type": "json_object"}}
    return ModelScopeChat(base_url=base_url, api_key=api_key, model=model_id, temperature=0.5, extra_body=extra)


# =========================
# 3) å›¾èŠ‚ç‚¹ï¼šæè®®ã€è¯•è·‘å®¡æ ¸ã€æ‰§è¡Œã€æ‹’ç»
# =========================

def propose_post_node(llm: ModelScopeChat):
    def node(state: AgentState) -> Dict[str, Any]:
        console.print("--- ğŸ“ æ‹Ÿç¨¿ä¸­ ---")
        prompt = (
            "ä½ æ˜¯ä¸€å®¶ AI å…¬å¸çš„ç¤¾äº¤åª’ä½“è¿è¥ã€‚æ ¹æ®ç”¨æˆ·è¯·æ±‚ï¼Œå†™ä¸€æ¡å¸å¼•äººçš„å¸–å­å¹¶ç»™å‡ºè‹¥å¹²æ ‡ç­¾ï¼ˆä»…æ ‡ç­¾åï¼Œä¸è¦ #ï¼‰ã€‚\n\n"
            f"è¯·æ±‚ï¼š{state['user_request']}"
        )
        structured = llm.with_structured_output(SocialMediaPost)
        post = structured.invoke(prompt)
        return {"proposed_post": post}
    return node


def dry_run_review_node(state: AgentState) -> Dict[str, Any]:
    console.print("--- ğŸ§ è¯•è·‘å¹¶ç­‰å¾…äººå·¥å®¡æ ¸ ---")
    result = social_media_tool.publish_post(state["proposed_post"], dry_run=True)
    console.print(Panel(
        result["proposed_post"],
        title="[bold yellow]è¯·å®¡æ ¸ï¼šè¾“å…¥ approve å‘å¸ƒï¼Œreject å–æ¶ˆ[/bold yellow]",
        border_style="yellow",
    ))
    decision = ""
    # å”¯ä¸€éœ€è¦äººå·¥è¾“å…¥å¤„ï¼šå¾ªç¯ç›´åˆ°è¾“å…¥ approve/rejectï¼ˆå«å¤§å°å†™å˜ä½“ï¼Œè§ .lower()ï¼‰
    while decision.lower() not in ("approve", "reject"):
        # è¾“å…¥ approve æˆ– rejectï¼Œå¦‚æœè¾“å…¥çš„æ˜¯ approve æˆ– reject çš„å˜ä½“ï¼Œæ¯”å¦‚ APPROVE æˆ– REJECTï¼Œé‚£ä¹ˆä¼šè®¤ä¸ºè¾“å…¥ä¸åˆæ³•ï¼Œéœ€è¦ç»§ç»­ç­‰å¾…ç”¨æˆ·è¾“å…¥
        #console.inputæ˜¯è¯»å–ç”¨æˆ·è¾“å…¥ï¼Œå’Œpython çš„inputä¸€æ ·ï¼Œä½†æ˜¯console.inputä¼šæ˜¾ç¤ºä¸€ä¸ªæç¤ºç¬¦ï¼Œè€Œinputä¸ä¼šæ˜¾ç¤ºæç¤ºç¬¦,ç”¨æˆ·è¾“å…¥çš„å†…å®¹ä¼šèµ‹å€¼ç»™decision
        decision = console.input("è¾“å…¥ approve æˆ– rejectï¼š").strip() or ""
    return {"dry_run_log": result["log"], "review_decision": decision.lower()}


def execute_live_post_node(state: AgentState) -> Dict[str, Any]:
    console.print("--- âœ… å·²æ‰¹å‡†ï¼Œæ­£å¼æ‰§è¡Œ ---")
    result = social_media_tool.publish_post(state["proposed_post"], dry_run=False)
    return {"final_status": f"å‘å¸ƒæˆåŠŸï¼ŒID: {result.get('post_id', '')}"}


def post_rejected_node(state: AgentState) -> Dict[str, Any]:
    console.print("--- âŒ å·²æ‹’ç»ï¼Œä¸æ‰§è¡Œ ---")
    return {"final_status": "å·²æ‹’ç»ï¼Œæœªæ‰§è¡Œå‘å¸ƒã€‚"}


def route_after_review(state: AgentState) -> str:
    return "execute_live" if state.get("review_decision") == "approve" else "reject"


# =========================
# 4) å·¥ä½œæµæ„å»ºä¸è¿è¡Œ
# =========================

def build_app(llm: ModelScopeChat):
    workflow = StateGraph(AgentState)
    workflow.add_node("propose_post", propose_post_node(llm))
    workflow.add_node("dry_run_review", dry_run_review_node)
    workflow.add_node("execute_live", execute_live_post_node)
    workflow.add_node("reject", post_rejected_node)
    workflow.set_entry_point("propose_post")
    workflow.add_edge("propose_post", "dry_run_review")
    workflow.add_conditional_edges("dry_run_review", route_after_review, {"execute_live": "execute_live", "reject": "reject"})
    workflow.add_edge("execute_live", END)
    workflow.add_edge("reject", END)
    return workflow.compile()


def run_workflow(app, request: str) -> Dict[str, Any]:
    """ä¼ å…¥åˆå§‹çŠ¶æ€ï¼ˆç”± initial_state æä¾›ï¼‰ï¼Œè¿è¡Œå›¾å¹¶è¿”å›æœ€ç»ˆçŠ¶æ€ã€‚"""
    return app.invoke(initial_state(request))


# =========================
# 5) CLI ä¸å…¥å£
# =========================

def parse_args():
    p = argparse.ArgumentParser(description="å¯è§‚æµ‹ä¸è¯•è·‘å¤–å£³ï¼šæ‹Ÿå‘å¸– â†’ è¯•è·‘ â†’ å®¡æ ¸ â†’ æ‰§è¡Œ/å–æ¶ˆ")
    p.add_argument("--request", type=str, default="ä¸ºæˆ‘ä»¬çš„æ–° AI æ¨¡å‹ã€Œæ˜Ÿäº‘ã€å†™ä¸€æ¡æ­£é¢å‘å¸ƒå…¬å‘Šã€‚", help="å‘å¸–è¯·æ±‚")
    p.add_argument("--debug", action="store_true", help="è°ƒè¯•è¾“å‡º")
    return p.parse_args()


def main():
    global DEBUG
    load_dotenv()
    args = parse_args()
    DEBUG = getattr(args, "debug", False)
    if not os.environ.get("MODELSCOPE_API_KEY"):
        console.print("[bold yellow]æœªè®¾ç½® MODELSCOPE_API_KEYï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”[/bold yellow]")
    llm = init_llm()
    app = build_app(llm)
    console.print(f"--- è¯·æ±‚ï¼š{args.request} ---")
    result = run_workflow(app, args.request)
    console.print(f"\n[bold]æœ€ç»ˆçŠ¶æ€ï¼š[/bold] {result.get('final_status', '')}")


if __name__ == "__main__":
    main()
