# -*- coding: utf-8 -*-
"""
å¹¶è¡Œæ¢ç´¢ + é›†æˆå†³ç­–ï¼ˆParallel Exploration + Ensemble Decisionï¼‰æ¶æ„çš„å¯è¿è¡Œç¤ºä¾‹

å­¦ä¹ ç›®æ ‡ï¼ˆçœ‹å®Œä½ èƒ½åšåˆ°ä»€ä¹ˆï¼‰ï¼š
- ç†è§£å¤šè·¯å¹¶è¡Œæ¢ç´¢ã€å¤šè§†è§’åˆ†æåç”±èšåˆè€…ç»¼åˆç»“è®ºçš„æµç¨‹
- æŒæ¡ LangGraph çš„æ‰‡å‡º/æ‰‡å…¥ï¼ˆå¤šèŠ‚ç‚¹å¹¶è¡Œä¸æ±‡èšï¼‰
- å­¦ä¼šç”¨ ModelScopeChat åšçº¯æ–‡æœ¬ä¸ç»“æ„åŒ–è¾“å‡ºï¼ˆCIO ç»¼åˆæŠ¥å‘Šï¼‰
- èƒ½è¿è¡Œã€ŒæŠ•èµ„å§”å‘˜ä¼šã€æ¼”ç¤ºï¼šä¸‰è·¯åˆ†æå¸ˆ + CIO ç»¼åˆ

æ ¸å¿ƒæ¦‚å¿µé€Ÿè§ˆï¼š
- å¹¶è¡Œæ¢ç´¢ï¼šåŒä¸€é—®é¢˜ç”±å¤šä¸ªç‹¬ç«‹æ™ºèƒ½ä½“ï¼ˆä¸åŒäººè®¾ï¼‰åŒæ—¶åˆ†æï¼Œå¾—åˆ°å¤šä»½æŠ¥å‘Š
- é›†æˆå†³ç­–ï¼šèšåˆæ™ºèƒ½ä½“ï¼ˆå¦‚ CIOï¼‰ç»¼åˆå¤šä»½æŠ¥å‘Šï¼Œäº§å‡ºæœ€ç»ˆç»“æ„åŒ–ç»“è®º
- é€‚ç”¨åœºæ™¯ï¼šå¤æ‚æ¨ç†ã€äº‹å®æ ¸æŸ¥ã€é«˜ stakes å†³ç­–æ”¯æŒ

è¿è¡Œå‰å‡†å¤‡ï¼š
- é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` å¹¶é…ç½®ï¼š
  - `MODELSCOPE_API_KEY`ï¼ˆå¿…éœ€ï¼‰
  - `MODELSCOPE_BASE_URL`ã€`MODELSCOPE_MODEL_ID`ï¼ˆå¯é€‰ï¼Œæœ‰é»˜è®¤ï¼‰
  - ä»…ä½¿ç”¨ä¸»æ¨¡å‹ï¼ˆMODELSCOPE_MODEL_IDï¼Œé»˜è®¤ DeepSeek-V3.2ï¼‰ï¼Œä¸è€ƒè™‘å¤‡ç”¨æ¨¡å‹
  - å¯é€‰ Tavilyï¼š`TAVILY_API_KEY`ï¼ˆåˆ†æå¸ˆæ£€ç´¢ç”¨ï¼›æœªé…ç½®æ—¶ç”¨æ¨¡æ‹Ÿæ£€ç´¢ï¼‰

å¦‚ä½•è¿è¡Œï¼š
- ç›´æ¥è¿è¡Œé»˜è®¤ç¤ºä¾‹ï¼š`python 13_ensemble_cn.py`
- è‡ªå®šä¹‰é—®é¢˜ï¼š`python 13_ensemble_cn.py --request "è‹±ä¼Ÿè¾¾ 2026 ä¸­é•¿æœŸæ˜¯å¦å€¼å¾—æŠ•èµ„ï¼Ÿ"`

é˜…è¯»å»ºè®®ï¼š
- å…ˆçœ‹ã€ŒçŠ¶æ€ä¸ç»“æ„åŒ–æ¨¡å‹ã€ï¼Œå†çœ‹ã€Œåˆ†æå¸ˆèŠ‚ç‚¹ã€ä¸ã€ŒCIO èšåˆèŠ‚ç‚¹ã€ï¼Œæœ€åçœ‹å›¾æ„å»ºä¸å…¥å£ã€‚
"""

import os
import asyncio
import argparse
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
from openai import OpenAI
from openai import RateLimitError, APIError

# =========================
# 1) æ•°æ®ç»“æ„ä¸æ¨¡å‹å®šä¹‰ï¼ˆPydantic v2ï¼‰
# =========================

class FinalRecommendation(BaseModel):
    """CIO ç»¼åˆåçš„æœ€ç»ˆæŠ•èµ„å»ºè®®ï¼ˆç»“æ„åŒ–ï¼‰ã€‚"""
    final_recommendation: str = Field(description="æœ€ç»ˆæŠ•èµ„ç»“è®ºï¼Œé¡»ä¸º Strong Buy / Buy / Hold / Sell / Strong Sell ä¹‹ä¸€")
    confidence_score: float = Field(description="ä¿¡å¿ƒåˆ†æ•°ï¼Œ1.0ï½10.0")
    synthesis_summary: str = Field(description="ç»¼åˆå„åˆ†æå¸ˆè§‚ç‚¹çš„æ‘˜è¦ï¼Œå«å…±è¯†ä¸åˆ†æ­§")
    identified_opportunities: List[str] = Field(description="ä¸»è¦æœºä¼šæˆ–çœ‹å¤šè¦ç‚¹ï¼ˆåˆ—è¡¨ï¼‰")
    identified_risks: List[str] = Field(description="ä¸»è¦é£é™©æˆ–çœ‹ç©ºè¦ç‚¹ï¼ˆåˆ—è¡¨ï¼‰")


class EnsembleState(TypedDict):
    query: str
    analyses: Dict[str, str]
    final_recommendation: Optional[Any]


# =========================
# 2) LLM ä¸æ§åˆ¶å°
# =========================

console = Console()
DEBUG: bool = False


def _search_or_mock(query: str) -> str:
    """Tavily æ£€ç´¢æˆ–æ¨¡æ‹Ÿæ£€ç´¢ç»“æœã€‚"""
    if os.environ.get("TAVILY_API_KEY"):
        try:
            from langchain_tavily import TavilySearch
            search = TavilySearch(max_results=3)
            docs = search.invoke(query)
            return docs if isinstance(docs, str) else "\n\n".join(getattr(d, "content", str(d)) for d in (docs if isinstance(docs, list) else [docs]))
        except Exception as e:
            if DEBUG:
                console.print(f"[dim]Tavily æ£€ç´¢å¼‚å¸¸: {e}[/dim]")
    return f"[æ¨¡æ‹Ÿæ£€ç´¢] ä¸ã€Œ{query[:50]}ã€ç›¸å…³çš„è¿‘æœŸå¸‚åœºä¸åŸºæœ¬é¢æ‘˜è¦ï¼ˆæœªé…ç½® TAVILY_API_KEY æ—¶ä½¿ç”¨ï¼‰ã€‚"


class ModelScopeChat:
    """
    ModelScope çš„ OpenAI å…¼å®¹æ¥å£ï¼šinvokeã€with_structured_outputã€‚
    ä»…ä½¿ç”¨ä¸»æ¨¡å‹ï¼ˆMODELSCOPE_MODEL_IDï¼Œé»˜è®¤ deepseek-ai/DeepSeek-V3.2ï¼‰ï¼Œä¸è€ƒè™‘å¤‡ç”¨æ¨¡å‹ï¼›æ—  API å¯†é’¥æ—¶è¿”å›æ¨¡æ‹Ÿå“åº”ã€‚
    """
    def __init__(self, base_url: str = None, api_key: str = None, model: str = None, temperature: float = 0.3, extra_body: Optional[dict] = None):
        self.base_url = base_url or os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
        self.api_key = api_key or os.environ.get("MODELSCOPE_API_KEY")
        self.model = model or os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
        self.temperature = temperature
        self.extra_body = extra_body or {"enable_thinking": True, "trust_request_chat_template": True, "response_format": {"type": "json_object"}}
        if not self.api_key:
            console.print("[bold yellow]âš ï¸ æœªè®¾ç½® MODELSCOPE_API_KEYï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”[/bold yellow]")
            self.client = None
        else:
            self.client = OpenAI(base_url=self.base_url, api_key=self.api_key)

    def invoke(self, prompt: str) -> str:
        if not self.client:
            return "ï¼ˆæœªé…ç½® APIï¼Œæ­¤ä¸ºæ¨¡æ‹Ÿåˆ†ææ–‡æœ¬ã€‚ï¼‰"
        extra = dict(self.extra_body) if self.extra_body else {}
        try:
            resp = self.client.chat.completions.create(
                model=self.model, messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature, stream=False, extra_body=extra,
            )
            return (resp.choices[0].message.content or "").strip()
        except (RateLimitError, APIError) as e:
            if "balance" in str(e).lower() or "403" in str(e) or "insufficient" in str(e).lower():
                raise RuntimeError(
                    "ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥ï¼šè´¦æˆ·ä½™é¢ä¸è¶³(403)ï¼Œè¯·å……å€¼æˆ–æ£€æŸ¥ MODELSCOPE_API_KEY / MODELSCOPE_MODEL_ID é…ç½®ã€‚"
                ) from e
            raise

    def with_structured_output(self, pyd_model: type[BaseModel]):
        import json, re
        class _Wrap:
            def __init__(self, outer): self.outer = outer
            def invoke(self, prompt: str) -> BaseModel:
                schema = pyd_model.model_json_schema()
                props = schema.get("properties", {})
                req = schema.get("required", [])
                schema_txt = "\n".join(f"- {k}: {v.get('type','string')}" for k,v in props.items()) or "- æŒ‰æ¨¡å‹å­—æ®µ"
                req_txt = ", ".join(req) if req else "æ‰€æœ‰å­—æ®µ"
                system = f"ä½ åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸¥æ ¼åŒ¹é…ä»¥ä¸‹å­—æ®µä¸ç±»å‹ï¼š\n{schema_txt}\nå¿…é¡»åŒ…å«ï¼š{req_txt}\nä¸è¦è¾“å‡ºè§£é‡Šæˆ–ä»£ç å—ã€‚"
                messages = [{"role": "system", "content": system}, {"role": "user", "content": prompt}]
                if not self.outer.client:
                    return pyd_model(final_recommendation="Hold", confidence_score=5.0, synthesis_summary="ï¼ˆæ¨¡æ‹Ÿï¼‰", identified_opportunities=[], identified_risks=[])
                try:
                    r = self.outer.client.chat.completions.create(
                        model=self.outer.model, messages=messages,
                        temperature=self.outer.temperature, stream=False, extra_body=self.outer.extra_body,
                    )
                except (RateLimitError, APIError) as e:
                    if "balance" in str(e).lower() or "403" in str(e) or "insufficient" in str(e).lower():
                        raise RuntimeError(
                            "ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥ï¼šè´¦æˆ·ä½™é¢ä¸è¶³(403)ï¼Œè¯·å……å€¼æˆ–æ£€æŸ¥ MODELSCOPE_API_KEY / MODELSCOPE_MODEL_ID é…ç½®ã€‚"
                        ) from e
                    raise
                raw = (r.choices[0].message.content or "").strip()
                try:
                    data = json.loads(raw)
                except Exception:
                    m = re.search(r"\{[\s\S]*\}", raw)
                    data = json.loads(m.group(0)) if m else {}
                return pyd_model.model_validate(data)
        return _Wrap(self)


def init_llm() -> ModelScopeChat:
    base_url = os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
    api_key = os.environ.get("MODELSCOPE_API_KEY", "")
    model_id = os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
    extra = {"enable_thinking": True, "trust_request_chat_template": True, "response_format": {"type": "json_object"}}
    return ModelScopeChat(base_url=base_url, api_key=api_key, model=model_id, temperature=0.3, extra_body=extra)


# =========================
# 3) åˆ†æå¸ˆèŠ‚ç‚¹ï¼ˆå¹¶è¡Œï¼‰ä¸ CIO èšåˆèŠ‚ç‚¹
# =========================

def _run_one_analyst(llm: ModelScopeChat, persona: str, agent_name: str, query: str) -> str:
    """å•è·¯åˆ†æå¸ˆï¼šæ£€ç´¢ + LLM åˆ†æï¼Œè¿”å›åˆ†ææ–‡æœ¬ã€‚"""
    console.print(f"--- ğŸ‘¨â€ğŸ’» è°ƒç”¨ {agent_name} ---")
    search_result = _search_or_mock(query)
    prompt = (
        f"ä½ æ˜¯ä¸€åä¸“ä¸šé‡‘èåˆ†æå¸ˆã€‚ä½ çš„äººè®¾ï¼š{persona}\n"
        f"è¯·ç»“åˆä»¥ä¸‹ç ”ç©¶æ‘˜è¦è¿›è¡Œåˆ†æï¼š\n{search_result}\n\n"
        f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\n"
        "è¯·ç»™å‡ºè¯¦ç»†æŠ•èµ„åˆ†æï¼Œå¹¶åœ¨æ–‡æœ«æ˜ç¡®å†™å‡ºã€Œå»ºè®®ã€ï¼ˆBuy/Hold/Sellï¼‰å’Œã€Œä¿¡å¿ƒåˆ†æ•°ã€ï¼ˆ1-10ï¼‰ã€‚"
    )
    return llm.invoke(prompt)


def run_all_analysts_node(llm: ModelScopeChat):
    """å¹¶è¡Œé€»è¾‘ï¼šç”¨ asyncio.to_thread åœ¨åç¨‹ä¸­å¹¶å‘è°ƒç”¨ä¸‰ä½åˆ†æå¸ˆï¼ˆåŒæ­¥ LLM ä¸å˜ï¼‰ã€‚"""
    personas = [
        ("çœ‹å¤šæˆé•¿åˆ†æå¸ˆï¼šå…³æ³¨ TAMã€æŠ€æœ¯æŠ¤åŸæ²³ä¸é•¿æœŸå¢é•¿ï¼Œæ·¡åŒ–çŸ­æœŸä¼°å€¼ã€‚", "BullishAnalyst"),
        ("è°¨æ…ä»·å€¼åˆ†æå¸ˆï¼šå…³æ³¨è´¢æŠ¥ã€ä¼°å€¼ã€å€ºåŠ¡ä¸ç«äº‰é£é™©ï¼Œè­¦æƒ•æ³¡æ²«ã€‚", "ValueAnalyst"),
        ("é‡åŒ–åˆ†æå¸ˆï¼šä»…åŸºäºæ•°æ®ä¸æŒ‡æ ‡ï¼ˆæ”¶å…¥å¢é€Ÿã€EPSã€ä¼°å€¼å€æ•°ã€æŠ€æœ¯æŒ‡æ ‡ï¼‰åšå®¢è§‚åˆ†æã€‚", "QuantAnalyst"),
    ]

    async def node(state: EnsembleState) -> Dict[str, Any]:
        query = state["query"]
        names = [name for _, name in personas]
        tasks = [
            asyncio.to_thread(_run_one_analyst, llm, persona, name, query)
            for persona, name in personas
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        analyses = {}
        for name, r in zip(names, results):
            analyses[name] = r if not isinstance(r, BaseException) else f"[åˆ†æå¼‚å¸¸] {r}"
        return {"analyses": analyses}

    return node


def start_analysis_node(state: EnsembleState) -> Dict[str, Any]:
    return {"analyses": {}}


def cio_synthesizer_node(llm: ModelScopeChat):
    def node(state: EnsembleState) -> Dict[str, Any]:
        console.print("--- ğŸ›ï¸ è°ƒç”¨é¦–å¸­æŠ•èµ„å®˜ï¼ˆCIOï¼‰ç»¼åˆå†³ç­– ---")
        all_analyses = "\n\n---\n\n".join(
            f"**{name} åˆ†æï¼š**\n{text}" for name, text in state["analyses"].items()
        )
        prompt = (
            f"ä½ æ˜¯ä¸€å®¶æŠ•èµ„åŸºé‡‘çš„é¦–å¸­æŠ•èµ„å®˜ï¼ˆCIOï¼‰ã€‚ä»¥ä¸‹æ˜¯å›¢é˜Ÿå¯¹åŒä¸€é—®é¢˜çš„å¤šä»½åˆ†ææŠ¥å‘Šã€‚\n"
            f"ç”¨æˆ·é—®é¢˜ï¼š{state['query']}\n\n"
            f"å›¢é˜ŸæŠ¥å‘Šï¼š\n{all_analyses}\n\n"
            "è¯·ç»¼åˆä»¥ä¸Šè§‚ç‚¹ï¼Œç»™å‡ºæœ€ç»ˆæŠ•èµ„å»ºè®®ï¼ˆStrong Buy/Buy/Hold/Sell/Strong Sellï¼‰ã€ä¿¡å¿ƒåˆ†æ•°(1-10)ã€ç»¼åˆæ‘˜è¦ã€ä¸»è¦æœºä¼šä¸ä¸»è¦é£é™©åˆ—è¡¨ã€‚"
        )
        structured_llm = llm.with_structured_output(FinalRecommendation)
        final_rec = structured_llm.invoke(prompt)
        return {"final_recommendation": final_rec}
    return node


# =========================
# 4) å·¥ä½œæµæ„å»ºä¸è¿è¡Œ
# =========================

def build_app(llm: ModelScopeChat):
    workflow = StateGraph(EnsembleState)
    workflow.add_node("start_analysis", start_analysis_node)
    workflow.add_node("run_all_analysts", run_all_analysts_node(llm))
    workflow.add_node("cio_synthesizer", cio_synthesizer_node(llm))
    workflow.set_entry_point("start_analysis")
    workflow.add_edge("start_analysis", "run_all_analysts")
    workflow.add_edge("run_all_analysts", "cio_synthesizer")
    workflow.add_edge("cio_synthesizer", END)
    return workflow.compile()


async def run_workflow_async(app, request: str) -> Dict[str, Any]:
    return await app.ainvoke({"query": request, "analyses": {}, "final_recommendation": None})


def run_workflow(app, request: str) -> Dict[str, Any]:
    return asyncio.run(run_workflow_async(app, request))


# =========================
# 5) CLI ä¸å…¥å£
# =========================

def parse_args():
    p = argparse.ArgumentParser(description="å¹¶è¡Œæ¢ç´¢ + é›†æˆå†³ç­–ï¼šæŠ•èµ„å§”å‘˜ä¼šæ¼”ç¤º")
    p.add_argument("--request", type=str, default="åŸºäºè¿‘æœŸæ–°é—»ã€è´¢åŠ¡è¡¨ç°ä¸å±•æœ›ï¼Œè‹±ä¼Ÿè¾¾ï¼ˆNVDAï¼‰åœ¨ 2026 å¹´ä¸‹åŠå¹´æ˜¯å¦å€¼å¾—é•¿æœŸæŠ•èµ„ï¼Ÿ", help="æŠ•èµ„åˆ†æé—®é¢˜")
    p.add_argument("--debug", action="store_true", help="å¼€å¯è°ƒè¯•è¾“å‡º")
    return p.parse_args()


def main():
    global DEBUG
    load_dotenv()
    args = parse_args()
    DEBUG = getattr(args, "debug", False)
    if not os.environ.get("MODELSCOPE_API_KEY"):
        console.print("[bold yellow]æœªè®¾ç½® MODELSCOPE_API_KEYï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”[/bold yellow]")
    llm = init_llm()
    app = build_app(llm)
    console.print(f"--- ğŸ“ˆ æŠ•èµ„å§”å‘˜ä¼šåˆ†æï¼š{args.request} ---")
    result = run_workflow(app, args.request)
    console.print("\n--- å„åˆ†æå¸ˆæŠ¥å‘Š ---")
    for name, analysis in result.get("analyses", {}).items():
        console.print(Panel(Markdown(analysis), title=f"[bold yellow]{name}[/bold yellow]", border_style="yellow"))
    rec = result.get("final_recommendation")
    if rec:
        console.print("\n--- CIO ç»¼åˆå»ºè®® ---")
        console.print(Panel(
            f"[bold]æœ€ç»ˆå»ºè®®ï¼š[/bold] {rec.final_recommendation}\n"
            f"[bold]ä¿¡å¿ƒåˆ†æ•°ï¼š[/bold] {rec.confidence_score}/10\n\n"
            f"[bold]ç»¼åˆæ‘˜è¦ï¼š[/bold]\n{rec.synthesis_summary}\n\n"
            f"[bold]ä¸»è¦æœºä¼šï¼š[/bold]\n" + "\n".join(f"* {x}" for x in rec.identified_opportunities) + "\n\n"
            f"[bold]ä¸»è¦é£é™©ï¼š[/bold]\n" + "\n".join(f"* {x}" for x in rec.identified_risks),
            title="[bold green]é¦–å¸­æŠ•èµ„å®˜ç»“è®º[/bold green]", border_style="green"
        ))


if __name__ == "__main__":
    main()
