# -*- coding: utf-8 -*-
"""
åæ€å¼å…ƒè®¤çŸ¥æ™ºèƒ½ä½“ï¼ˆReflexive Metacognitive Agentï¼‰æ¶æ„çš„å¯è¿è¡Œç¤ºä¾‹

å­¦ä¹ ç›®æ ‡ï¼ˆçœ‹å®Œä½ èƒ½åšåˆ°ä»€ä¹ˆï¼‰ï¼š
- ç†è§£ã€Œå…ˆå¯¹è¯·æ±‚åšå…ƒè®¤çŸ¥åˆ†æã€å†é€‰ç­–ç•¥ã€çš„å®‰å…¨å†³ç­–æµç¨‹
- æŒæ¡åŸºäºè‡ªæ¨¡å‹ï¼ˆçŸ¥è¯†åŸŸã€å·¥å…·ã€ç½®ä¿¡åº¦é˜ˆå€¼ï¼‰çš„ç­–ç•¥è·¯ç”±ï¼šç›´æ¥å›ç­” / ç”¨å·¥å…· / å‡çº§äººå·¥
- èƒ½è¿è¡Œ
    - åŒ»ç–—åˆ†è¯Šæ¼”ç¤ºï¼šç®€å•é—®é¢˜ç›´æ¥ç­”ã€è¯ç‰©ç›¸äº’ä½œç”¨ç”¨å·¥å…·ã€é«˜å±æˆ–è¶…åŸŸåˆ™å‡çº§
    - è½¦èˆ±æ™ºèƒ½åŠ©æ‰‹æ¼”ç¤ºï¼šç®€å•é—®é¢˜ç›´æ¥ç­”ã€è½¦èˆ±è®¾å¤‡ã€å¨±ä¹ã€poiã€å¤©æ°”ã€å¯¼èˆªã€éŸ³ä¹ã€ç”µè¯ã€è§†é¢‘ã€æ¸¸æˆã€æ–°é—»ã€è®¾ç½®ã€å¸®åŠ©ç”¨å·¥å…·ã€é«˜å±æˆ–è¶…åŸŸ(å¦‚è½¦èˆ±è®¾å¤‡æ•…éšœ,é«˜å±è½¦æ§æ“ä½œ)åˆ™å‡çº§

æ ¸å¿ƒæ¦‚å¿µé€Ÿè§ˆï¼š
- å…ƒè®¤çŸ¥åˆ†æï¼šåœ¨å›ç­”å‰å…ˆåˆ†æã€Œæˆ‘èƒ½å¦å®‰å…¨ä¸”å‡†ç¡®å›ç­”ã€ã€Œæ˜¯å¦éœ€å·¥å…·ã€ã€Œæ˜¯å¦åº”å‡çº§ã€
- è‡ªæ¨¡å‹ï¼šæ™ºèƒ½ä½“å¯¹è‡ªèº«çŸ¥è¯†åŸŸã€å¯ç”¨å·¥å…·ã€ç½®ä¿¡åº¦é˜ˆå€¼çš„æ˜¾å¼æè¿°
- é€‚ç”¨åœºæ™¯ï¼šåŒ»ç–—/æ³•å¾‹/é‡‘èç­‰é«˜é£é™©é¢†åŸŸï¼Œå¿…é¡»èƒ½è¯´ã€Œè¯·å’¨è¯¢ä¸“ä¸šäººå£«ã€

è¿è¡Œå‰å‡†å¤‡ï¼š
- é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` å¹¶é…ç½®ï¼š
  - `MODELSCOPE_API_KEY`ï¼ˆå¿…éœ€ï¼‰
  - `MODELSCOPE_BASE_URL`ã€`MODELSCOPE_MODEL_ID`ï¼ˆå¯é€‰ï¼Œæœ‰é»˜è®¤ï¼‰
  - ä»…ä½¿ç”¨ä¸»æ¨¡å‹ï¼ˆMODELSCOPE_MODEL_IDï¼Œé»˜è®¤ DeepSeek-V3.2ï¼‰ï¼Œä¸è€ƒè™‘å¤‡ç”¨æ¨¡å‹

å¦‚ä½•è¿è¡Œï¼š
- ç›´æ¥è¿è¡Œï¼ˆé»˜è®¤è§¦å‘ use_tool è¯ç‰©æŸ¥è¯¢ï¼‰ï¼š`python 17_reflexive_metacognitive_cn.py`
- ç›´æ¥å›ç­”ç¤ºä¾‹ï¼š`python 17_reflexive_metacognitive_cn.py --request "æ„Ÿå†’å’Œæµæ„Ÿçš„ç—‡çŠ¶æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"`
- å‡çº§äººå·¥ç¤ºä¾‹ï¼š`python 17_reflexive_metacognitive_cn.py --request "æˆ‘èƒ¸å£ç–¼å‘¼å¸å›°éš¾æ€ä¹ˆåŠï¼Ÿ"`

é˜…è¯»å»ºè®®ï¼š
- å…ˆçœ‹ã€Œè‡ªæ¨¡å‹ä¸å·¥å…·ã€ã€Œå…ƒè®¤çŸ¥åˆ†æç»“æ„ã€ï¼Œå†çœ‹å„ç­–ç•¥èŠ‚ç‚¹ä¸æ¡ä»¶è¾¹ã€‚
"""

import os
import argparse
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict
from rich.console import Console
from rich.panel import Panel
from openai import OpenAI
from openai import RateLimitError, APIError

# =========================
# 1) æ•°æ®ç»“æ„ä¸æ¨¡å‹å®šä¹‰ï¼ˆPydantic v2ï¼‰
# =========================

class AgentSelfModel(BaseModel):
    """æ™ºèƒ½ä½“è‡ªæ¨¡å‹ï¼šåç§°ã€è§’è‰²ã€çŸ¥è¯†åŸŸã€å¯ç”¨å·¥å…·ã€ç½®ä¿¡åº¦é˜ˆå€¼ã€‚"""
    name: str = Field(description="æ™ºèƒ½ä½“åç§°")
    role: str = Field(description="è§’è‰²æè¿°")
    knowledge_domain: List[str] = Field(description="æ“…é•¿çš„çŸ¥è¯†åŸŸåˆ—è¡¨")
    available_tools: List[str] = Field(description="å¯ç”¨å·¥å…·ååˆ—è¡¨")
    confidence_threshold: float = Field(default=0.6, description="ä½äºæ­¤ç½®ä¿¡åº¦é¡»å‡çº§")


class MetacognitiveAnalysis(BaseModel):
    """å…ƒè®¤çŸ¥åˆ†æç»“æœã€‚"""
    confidence: float = Field(description="ç½®ä¿¡åº¦ 0.0ï½1.0")
    strategy: str = Field(description="ç­–ç•¥ï¼šreason_directly / use_tool / escalate")#reason_directly: ç›´æ¥å›ç­”ï¼Œuse_tool: ä½¿ç”¨å·¥å…·ï¼Œescalate: å‡çº§äººå·¥
    reasoning: str = Field(description="é€‰æ‹©è¯¥ç­–ç•¥çš„ç†ç”±")#é€‰æ‹©è¯¥ç­–ç•¥çš„ç†ç”±
    tool_to_use: Optional[str] = Field(default=None, description="è‹¥ use_toolï¼Œå·¥å…·å")#å·¥å…·å
    tool_args: Optional[Dict[str, Any]] = Field(default=None, description="è‹¥ use_toolï¼Œå·¥å…·å‚æ•°")#å·¥å…·å‚æ•°


class AgentState(TypedDict):
    user_query: str#ç”¨æˆ·æŸ¥è¯¢
    self_model: AgentSelfModel#æ™ºèƒ½ä½“è‡ªæ¨¡å‹
    metacognitive_analysis: Optional[MetacognitiveAnalysis]#å…ƒè®¤çŸ¥åˆ†æç»“æœ
    tool_output: Optional[str]#å·¥å…·è¾“å‡º
    final_response: str#æœ€ç»ˆå›å¤


# =========================
# 2) å·¥å…·ä¸ LLM
# =========================

console = Console()
DEBUG: bool = False


class DrugInteractionChecker:
    """æ¨¡æ‹Ÿè¯ç‰©ç›¸äº’ä½œç”¨æŸ¥è¯¢å·¥å…·ã€‚"""
    def check(self, drug_a: str, drug_b: str) -> str:
        """æŸ¥è¯¢è¯ç‰©ç›¸äº’ä½œç”¨ã€‚"""
        known = {
            frozenset(["ibuprofen", "lisinopril"]): "ä¸­ç­‰é£é™©ï¼šå¸ƒæ´›èŠ¬å¯èƒ½å‡å¼±èµ–è¯ºæ™®åˆ©é™å‹æ•ˆæœï¼Œéœ€ç›‘æµ‹è¡€å‹ã€‚",
            frozenset(["aspirin", "warfarin"]): "é«˜é£é™©ï¼šå¢åŠ å‡ºè¡€é£é™©ï¼Œåº”é¿å…è”ç”¨ï¼Œé™¤éåŒ»ç”ŸæŒ‡å¯¼ã€‚",
        }
        key = frozenset([drug_a.lower().strip(), drug_b.lower().strip()])
        return known.get(key, "æœªå‘ç°å·²çŸ¥æ˜¾è‘—ç›¸äº’ä½œç”¨ã€‚ä½†ä»è¯·å’¨è¯¢è¯å¸ˆæˆ–åŒ»ç”Ÿã€‚")


drug_tool = DrugInteractionChecker()


class ModelScopeChat:
    """
j    ModelScope çš„ OpenAI å…¼å®¹æ¥å£ï¼šinvokeã€with_structured_outputã€‚
    ä»…ä½¿ç”¨ä¸»æ¨¡å‹ï¼ˆMODELSCOPE_MODEL_IDï¼Œé»˜è®¤ deepseek-ai/DeepSeek-V3.2ï¼‰ï¼Œä¸è€ƒè™‘å¤‡ç”¨æ¨¡å‹ï¼›æ—  API å¯†é’¥æ—¶è¿”å›æ¨¡æ‹Ÿå“åº”ã€‚
    """
    def __init__(self, base_url: str = None, api_key: str = None, model: str = None, temperature: float = 0, extra_body: Optional[dict] = None):
        self.base_url = base_url or os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
        self.api_key = api_key or os.environ.get("MODELSCOPE_API_KEY")
        self.model = model or os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
        self.temperature = temperature
        self.extra_body = extra_body or {"enable_thinking": True, "trust_request_chat_template": True, "response_format": {"type": "json_object"}}
        if not self.api_key:
            console.print("[bold yellow]âš ï¸ æœªè®¾ç½® MODELSCOPE_API_KEYï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”[/bold yellow]")
            self.client = None
        else:
            self.client = OpenAI(base_url=self.base_url, api_key=self.api_key)

    def invoke(self, prompt: str) -> str:
        if not self.client:
            return "ï¼ˆæœªé…ç½® APIï¼Œæ­¤ä¸ºæ¨¡æ‹Ÿå›å¤ã€‚è¯·å’¨è¯¢åŒ»ç”Ÿã€‚ï¼‰"
        extra = dict(self.extra_body) if self.extra_body else {}
        try:
            r = self.client.chat.completions.create(
                model=self.model, messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature, stream=False, extra_body=extra,
            )
            return (r.choices[0].message.content or "").strip()
        except (RateLimitError, APIError) as e:
            if "balance" in str(e).lower() or "403" in str(e) or "insufficient" in str(e).lower():
                raise RuntimeError(
                    "ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥ï¼šè´¦æˆ·ä½™é¢ä¸è¶³(403)ï¼Œè¯·å……å€¼æˆ–æ£€æŸ¥ MODELSCOPE_API_KEY / MODELSCOPE_MODEL_ID é…ç½®ã€‚"
                ) from e
            raise

    def with_structured_output(self, pyd_model: type[BaseModel]):
        import json, re
        class _Wrap:
            def __init__(self, outer): self.outer = outer
            def invoke(self, prompt: str) -> BaseModel:
                schema = pyd_model.model_json_schema()
                props = schema.get("properties", {})
                req = schema.get("required", [])
                schema_txt = "\n".join(f"- {k}: {v.get('type','string')}" for k, v in props.items()) or "- æŒ‰æ¨¡å‹å­—æ®µ"
                req_txt = ", ".join(req) if req else "æ‰€æœ‰å­—æ®µ"
                system = f"åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸¥æ ¼åŒ¹é…ï¼š\n{schema_txt}\nå¿…é¡»åŒ…å«ï¼š{req_txt}\nä¸è¦è§£é‡Šæˆ–ä»£ç å—ã€‚"
                messages = [{"role": "system", "content": system}, {"role": "user", "content": prompt}]
                if not self.outer.client:
                    return pyd_model(confidence=0.9, strategy="reason_directly", reasoning="æ¨¡æ‹Ÿ", tool_to_use=None, tool_args=None)
                try:
                    r = self.outer.client.chat.completions.create(
                        model=self.outer.model, messages=messages,
                        temperature=self.outer.temperature, stream=False, extra_body=self.outer.extra_body,
                    )
                except (RateLimitError, APIError) as e:
                    if "balance" in str(e).lower() or "403" in str(e) or "insufficient" in str(e).lower():
                        raise RuntimeError(
                            "ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥ï¼šè´¦æˆ·ä½™é¢ä¸è¶³(403)ï¼Œè¯·å……å€¼æˆ–æ£€æŸ¥ MODELSCOPE_API_KEY / MODELSCOPE_MODEL_ID é…ç½®ã€‚"
                        ) from e
                    raise
                raw = (r.choices[0].message.content or "").strip()
                try:
                    data = json.loads(raw)
                except Exception:
                    m = re.search(r"\{[\s\S]*\}", raw)
                    data = json.loads(m.group(0)) if m else {}
                # LLM æœ‰æ—¶è¿”å› tool_args="", tool_to_use=""ï¼Œéœ€æ¸…æ´—ä¸º None / æœ‰æ•ˆ dict
                if "tool_args" in data and (not isinstance(data["tool_args"], dict) or data["tool_args"] == ""):
                    data["tool_args"] = None
                if "tool_to_use" in data and data["tool_to_use"] == "":
                    data["tool_to_use"] = None
                return pyd_model.model_validate(data)
        return _Wrap(self)


def init_llm() -> ModelScopeChat:
    base_url = os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
    api_key = os.environ.get("MODELSCOPE_API_KEY", "")
    model_id = os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
    extra = {"enable_thinking": True, "trust_request_chat_template": True, "response_format": {"type": "json_object"}}
    return ModelScopeChat(base_url=base_url, api_key=api_key, model=model_id, temperature=0, extra_body=extra)


# =========================
# 3) å›¾èŠ‚ç‚¹ï¼šå…ƒè®¤çŸ¥åˆ†æã€ç›´æ¥å›ç­”ã€ç”¨å·¥å…·ã€ç»¼åˆã€å‡çº§;
#    å…¶ä¸­å…ƒè®¤çŸ¥åˆ†æç”¨äºåˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œé€‰æ‹©æœ€å®‰å…¨ã€æœ€åˆé€‚çš„ç­–ç•¥ã€‚ç›´æ¥å›ç­”ç”¨äºç›´æ¥å›ç­”ç”¨æˆ·æŸ¥è¯¢ã€‚ç”¨å·¥å…·ç”¨äºä½¿ç”¨å·¥å…·ã€‚ç»¼åˆç”¨äºç»¼åˆå·¥å…·ç»“æœå¹¶å›å¤ã€‚å‡çº§ç”¨äºå‡çº§äººå·¥ã€‚
# =========================

def metacognitive_analysis_node(llm: ModelScopeChat):
    """å…ƒè®¤çŸ¥åˆ†æèŠ‚ç‚¹ã€‚"""
    def node(state: AgentState) -> Dict[str, Any]:
        console.print(Panel("ğŸ¤” å…ƒè®¤çŸ¥åˆ†æä¸­â€¦", title="[yellow]Step: Self-Reflection[/yellow]", border_style="yellow"))
        sm = state["self_model"]
        """æ„å»ºå…ƒè®¤çŸ¥åˆ†ææç¤ºã€‚"""
        prompt = (
            "ä½ æ˜¯å…ƒè®¤çŸ¥æ¨ç†å¼•æ“ã€‚æ ¹æ®æ™ºèƒ½ä½“è‡ªæ¨¡å‹åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œé€‰æ‹©æœ€å®‰å…¨ã€æœ€åˆé€‚çš„ç­–ç•¥ã€‚\n\n"
            f"è‡ªæ¨¡å‹ï¼šåç§°={sm.name}ï¼Œè§’è‰²={sm.role}ï¼ŒçŸ¥è¯†åŸŸ={sm.knowledge_domain}ï¼Œå¯ç”¨å·¥å…·={sm.available_tools}ã€‚\n\n"
            "ç­–ç•¥è§„åˆ™ï¼š1) escalateï¼šæ¶‰åŠæ€¥ç—‡ã€è¶…å‡ºçŸ¥è¯†åŸŸæˆ–ä»»ä½•ä¸ç¡®å®šæ—¶é€‰æ­¤é¡¹ã€‚2) use_toolï¼šé—®é¢˜æ˜ç¡®éœ€è¦æŸå·¥å…·æ—¶ï¼ˆå¦‚è¯ç‰©ç›¸äº’ä½œç”¨ç”¨ drug_interaction_checkerï¼‰ã€‚"
            "3) reason_directlyï¼šä»…å½“é«˜ç½®ä¿¡ã€ä½é£é™©ä¸”å®Œå…¨åœ¨çŸ¥è¯†åŸŸå†…æ—¶é€‰æ­¤é¡¹ã€‚\n\n"
            f"ç”¨æˆ·æŸ¥è¯¢ï¼š\"{state['user_query']}\"\n\n"
            "è¾“å‡º JSONï¼šconfidence(0-1), strategy(ä¸‰è€…ä¹‹ä¸€), reasoning, tool_to_use(å¯é€‰), tool_args(å¯é€‰ï¼Œå¦‚ {\"drug_a\":\"x\",\"drug_b\":\"y\"})ã€‚"
        )
        structured = llm.with_structured_output(MetacognitiveAnalysis)#ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºè·å–å…ƒè®¤çŸ¥åˆ†æç»“æœ
        analysis = structured.invoke(prompt)
        console.print(Panel(f"ç½®ä¿¡åº¦ï¼š{analysis.confidence:.2f}\nç­–ç•¥ï¼š{analysis.strategy}\nç†ç”±ï¼š{analysis.reasoning}", title="å…ƒè®¤çŸ¥ç»“æœ"))
        return {"metacognitive_analysis": analysis}
    return node


def reason_directly_node(llm: ModelScopeChat):
    """ç›´æ¥å›ç­”èŠ‚ç‚¹ã€‚"""
    def node(state: AgentState) -> Dict[str, Any]:
        console.print(Panel("âœ… ç­–ç•¥ï¼šç›´æ¥å›ç­”", title="[green]Reason Directly[/green]", border_style="green"))
        """æ„å»ºç›´æ¥å›ç­”æç¤ºã€‚"""
        prompt = f"ä½ æ˜¯{state['self_model'].role}ã€‚è¯·å¯¹ä»¥ä¸‹é—®é¢˜ç»™å‡ºæœ‰å¸®åŠ©ã€ä¸å…·å¤„æ–¹æ€§çš„å›ç­”ï¼Œå¹¶æé†’ç”¨æˆ·ä½ ä¸æ˜¯åŒ»ç”Ÿã€‚\n\né—®é¢˜ï¼š{state['user_query']}"
        resp = llm.invoke(prompt)#ä½¿ç”¨LLMç›´æ¥å›ç­”ç”¨æˆ·æŸ¥è¯¢
        return {"final_response": resp}
    return node


def call_tool_node(state: AgentState) -> Dict[str, Any]:
    """ä½¿ç”¨å·¥å…·èŠ‚ç‚¹ã€‚"""
    console.print(Panel("ğŸ› ï¸ ç­–ç•¥ï¼šä½¿ç”¨å·¥å…·", title="[cyan]Use Tool[/cyan]", border_style="cyan"))
    analysis = state["metacognitive_analysis"]#è·å–å…ƒè®¤çŸ¥åˆ†æç»“æœ
    if analysis.tool_to_use == "drug_interaction_checker" and analysis.tool_args:
        out = drug_tool.check(#ä½¿ç”¨è¯ç‰©ç›¸äº’ä½œç”¨æŸ¥è¯¢å·¥å…·æŸ¥è¯¢è¯ç‰©ç›¸äº’ä½œç”¨ 
            analysis.tool_args.get("drug_a", ""),
            analysis.tool_args.get("drug_b", ""),
        )
        return {"tool_output": out}
    return {"tool_output": "å·¥å…·æœªæ‰¾åˆ°æˆ–å‚æ•°ç¼ºå¤±ã€‚"}


def synthesize_tool_response_node(llm: ModelScopeChat):
    """ç»¼åˆå·¥å…·ç»“æœå¹¶å›å¤èŠ‚ç‚¹ã€‚"""
    def node(state: AgentState) -> Dict[str, Any]:
        console.print(Panel("ğŸ“ ç»¼åˆå·¥å…·ç»“æœå¹¶å›å¤", title="[cyan]Synthesize[/cyan]", border_style="cyan"))
        """æ„å»ºç»¼åˆå·¥å…·ç»“æœå¹¶å›å¤æç¤ºã€‚"""
        prompt = (
            f"ä½ æ˜¯{state['self_model'].role}ã€‚ä½ å·²é€šè¿‡å·¥å…·è·å¾—ä¿¡æ¯ï¼Œè¯·æ¸…æ™°ã€æœ‰å¸®åŠ©åœ°å‘ˆç°ç»™ç”¨æˆ·ï¼Œå¹¶åŠ¡å¿…æé†’å…¶å’¨è¯¢åŒ»ç–—ä¸“ä¸šäººå£«ã€‚\n\n"
            f"åŸé—®é¢˜ï¼š{state['user_query']}\nå·¥å…·è¾“å‡ºï¼š{state['tool_output']}"
        )
        resp = llm.invoke(prompt)#ä½¿ç”¨LLMç»¼åˆå·¥å…·ç»“æœå¹¶å›å¤
        return {"final_response": resp}
    return node


def escalate_to_human_node(state: AgentState) -> Dict[str, Any]:
    """å‡çº§äººå·¥èŠ‚ç‚¹ã€‚"""
    console.print(Panel("ğŸš¨ ç­–ç•¥ï¼šå‡çº§äººå·¥", title="[bold red]Escalate[/bold red]", border_style="red"))
    return {"final_response": "æˆ‘æ˜¯ AI åŠ©æ‰‹ï¼Œæ— æ³•å°±æ­¤ç±»é—®é¢˜æä¾›ä¸“ä¸šæ„è§ã€‚è¯¥é—®é¢˜è¶…å‡ºæˆ‘çš„çŸ¥è¯†èŒƒå›´æˆ–æ¶‰åŠå¯èƒ½ä¸¥é‡ç—‡çŠ¶ï¼Œè¯·ç«‹å³å’¨è¯¢åˆæ ¼åŒ»ç–—äººå‘˜ã€‚"}


def route_strategy(state: AgentState) -> str:
    """è·¯ç”±ç­–ç•¥èŠ‚ç‚¹ã€‚"""
    return state["metacognitive_analysis"].strategy#è¿”å›å…ƒè®¤çŸ¥åˆ†æç»“æœçš„ç­–ç•¥


# =========================
# 4) å·¥ä½œæµæ„å»ºä¸è¿è¡Œ
# =========================

def build_app(llm: ModelScopeChat):
    """æ„å»ºå·¥ä½œæµã€‚"""
    workflow = StateGraph(AgentState)#åˆ›å»ºçŠ¶æ€å›¾
    workflow.add_node("analyze", metacognitive_analysis_node(llm))#æ·»åŠ å…ƒè®¤çŸ¥åˆ†æèŠ‚ç‚¹
    workflow.add_node("reason", reason_directly_node(llm))#æ·»åŠ ç›´æ¥å›ç­”èŠ‚ç‚¹
    workflow.add_node("call_tool", call_tool_node)#æ·»åŠ ä½¿ç”¨å·¥å…·èŠ‚ç‚¹
    workflow.add_node("synthesize", synthesize_tool_response_node(llm))#æ·»åŠ ç»¼åˆå·¥å…·ç»“æœå¹¶å›å¤èŠ‚ç‚¹
    workflow.add_node("escalate", escalate_to_human_node)#æ·»åŠ å‡çº§äººå·¥èŠ‚ç‚¹
    workflow.set_entry_point("analyze")#è®¾ç½®å…¥å£ç‚¹
    workflow.add_conditional_edges("analyze", route_strategy, {#æ·»åŠ æ¡ä»¶è¾¹ï¼šå…ƒè®¤çŸ¥åˆ†æ â†’ ç›´æ¥å›ç­” / ä½¿ç”¨å·¥å…· / å‡çº§äººå·¥
        "reason_directly": "reason",
        "use_tool": "call_tool",
        "escalate": "escalate",
    })
    workflow.add_edge("call_tool", "synthesize")#æ·»åŠ è¾¹ï¼šä½¿ç”¨å·¥å…· â†’ ç»¼åˆå·¥å…·ç»“æœå¹¶å›å¤ï¼šä½¿ç”¨å·¥å…· â†’ ç»¼åˆå·¥å…·ç»“æœå¹¶å›å¤
    workflow.add_edge("reason", END)#æ·»åŠ è¾¹ï¼šç›´æ¥å›ç­” â†’ ç»“æŸï¼šç›´æ¥å›ç­” â†’ ç»“æŸ
    workflow.add_edge("synthesize", END)#æ·»åŠ è¾¹ï¼šç»¼åˆå·¥å…·ç»“æœå¹¶å›å¤ â†’ ç»“æŸï¼šç»¼åˆå·¥å…·ç»“æœå¹¶å›å¤ â†’ ç»“æŸ
    workflow.add_edge("escalate", END)#æ·»åŠ è¾¹ï¼šå‡çº§äººå·¥ â†’ ç»“æŸï¼šå‡çº§äººå·¥ â†’ ç»“æŸ   
    return workflow.compile()#ç¼–è¯‘å·¥ä½œæµ







def run_agent(app, query: str, self_model: AgentSelfModel) -> Dict[str, Any]:
    """è¿è¡Œæ™ºèƒ½ä½“ã€‚"""
    return app.invoke({#è°ƒç”¨å·¥ä½œæµï¼Œquery ä¸ºç”¨æˆ·æŸ¥è¯¢ï¼Œself_model ä¸ºæ™ºèƒ½ä½“è‡ªæ¨¡å‹
        "user_query": query,
        "self_model": self_model,#é»˜è®¤åŒ»ç–—åˆ†è¯Šæ™ºèƒ½ä½“è‡ªæ¨¡å‹
        "metacognitive_analysis": None,#é»˜è®¤å…ƒè®¤çŸ¥åˆ†æç»“æœ
        "tool_output": None,#é»˜è®¤å·¥å…·è¾“å‡º
        "final_response": "",#é»˜è®¤æœ€ç»ˆå›å¤
    })


# =========================
# 5) CLI ä¸å…¥å£
# =========================

MEDICAL_SELF_MODEL = AgentSelfModel(#åŒ»ç–—åˆ†è¯Šæ™ºèƒ½ä½“è‡ªæ¨¡å‹
    name="TriageBot-3000",
    role="æä¾›åˆæ­¥åŒ»ç–—ä¿¡æ¯çš„ AI åŠ©æ‰‹",
    knowledge_domain=["æ„Ÿå†’", "æµæ„Ÿ", "è¿‡æ•", "å¤´ç—›", "åŸºç¡€æ€¥æ•‘"],
    available_tools=["drug_interaction_checker"],
)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚"""
    p = argparse.ArgumentParser(description="åæ€å¼å…ƒè®¤çŸ¥æ™ºèƒ½ä½“ï¼šåŒ»ç–—åˆ†è¯Šæ¼”ç¤º")
    p.add_argument("--request", type=str, default="å¸ƒæ´›èŠ¬å’Œèµ–è¯ºæ™®åˆ©èƒ½ä¸€èµ·åƒå—ï¼Ÿ", help="ç”¨æˆ·é—®é¢˜ï¼ˆé»˜è®¤è§¦å‘ use_tool è¯ç‰©æŸ¥è¯¢ï¼‰")
    p.add_argument("--debug", action="store_true", help="è°ƒè¯•è¾“å‡º")
    return p.parse_args()


def main():
    global DEBUG
    load_dotenv()
    args = parse_args()
    DEBUG = getattr(args, "debug", False)
    if not os.environ.get("MODELSCOPE_API_KEY"):
        console.print("[bold yellow]æœªè®¾ç½® MODELSCOPE_API_KEYï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”[/bold yellow]")
    llm = init_llm()
    app = build_app(llm)
    console.print(f"--- ç”¨æˆ·é—®é¢˜ï¼š{args.request} ---")
    result = run_agent(app, args.request, MEDICAL_SELF_MODEL)#è¿è¡Œæ™ºèƒ½ä½“ï¼Œargs.request ä¸ºç”¨æˆ·æŸ¥è¯¢ï¼ŒMEDICAL_SELF_MODEL ä¸ºåŒ»ç–—åˆ†è¯Šæ™ºèƒ½ä½“è‡ªæ¨¡å‹
    console.print("\n--- æœ€ç»ˆå›å¤ ---")
    console.print(Panel(result.get("final_response", ""), title="[bold green]å›å¤[/bold green]", border_style="green"))


if __name__ == "__main__":
    main()
