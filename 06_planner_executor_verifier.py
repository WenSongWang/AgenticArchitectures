# -*- coding: utf-8 -*-
"""
è§„åˆ’â†’æ‰§è¡Œâ†’éªŒè¯ï¼ˆPlannerâ†’Executorâ†’Verifierï¼‰æ¶æ„çš„å¯è¿è¡Œç¤ºä¾‹

å­¦ä¹ ç›®æ ‡ï¼ˆçœ‹å®Œä½ èƒ½åšåˆ°ä»€ä¹ˆï¼‰ï¼š
- ç†è§£PEVæ¶æ„å¦‚ä½•é€šè¿‡"è§„åˆ’â†’æ‰§è¡Œâ†’éªŒè¯"ä¸‰æ­¥å·¥ä½œæµå®ç°é”™è¯¯æ£€æµ‹ä¸è‡ªä¿®æ­£
- æŒæ¡LangGraph1.0ä¸­æ¡ä»¶è¾¹ï¼ˆconditional edgesï¼‰çš„ä½¿ç”¨ï¼Œå®ç°æ ¹æ®éªŒè¯ç»“æœåŠ¨æ€è°ƒæ•´å·¥ä½œæµ
- å­¦ä¼šè®¾è®¡å¹¶å®ç°ä¸€ä¸ªéªŒè¯å™¨ï¼ˆVerifierï¼‰èŠ‚ç‚¹ï¼Œèƒ½å¤Ÿæ£€æµ‹æ‰§è¡Œç»“æœçš„é”™è¯¯
- èƒ½æŠŠè„šæœ¬ä½œä¸ºå‘½ä»¤è¡Œç¨‹åºè¿è¡Œï¼Œå¹¶è§‚å¯ŸPEVæ¶æ„å¦‚ä½•ä»é”™è¯¯ä¸­æ¢å¤

æ ¸å¿ƒæ¦‚å¿µé€Ÿè§ˆï¼š
- PEVæ¶æ„ï¼šPlannerï¼ˆè§„åˆ’ï¼‰â†’ Executorï¼ˆæ‰§è¡Œï¼‰â†’ Verifierï¼ˆéªŒè¯ï¼‰çš„ä¸‰æ­¥å·¥ä½œæµ
- æ¡ä»¶è¾¹ï¼šæ ¹æ®Verifierçš„éªŒè¯ç»“æœå†³å®šä¸‹ä¸€æ­¥æ˜¯é‡æ–°è§„åˆ’è¿˜æ˜¯ç»§ç»­æ‰§è¡Œ
- é”™è¯¯æ£€æµ‹ä¸æ¢å¤ï¼šVerifierèƒ½å¤Ÿå‘ç°æ‰§è¡Œå¤±è´¥ï¼Œå¹¶è§¦å‘é‡æ–°è§„åˆ’ä»¥ä¿®å¤é—®é¢˜

è¿è¡Œå‰å‡†å¤‡ï¼š
- é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` å¹¶é…ç½®ï¼š
  - `LANGCHAIN_API_KEY`ï¼ˆç”¨äº LangSmith è¿½è¸ªï¼Œå¯é€‰ï¼‰
  - å¦‚ä½¿ç”¨ ModelScope æ¥å…¥ï¼š`MODELSCOPE_BASE_URL`ã€`MODELSCOPE_API_KEY`ã€`MODELSCOPE_MODEL_ID`

å¦‚ä½•è¿è¡Œï¼š
- ç›´æ¥è¿è¡Œé»˜è®¤ç¤ºä¾‹ï¼š`python 06_planner_executor_verifier.py`
- æ›´æ¢ä»»åŠ¡è¯·æ±‚ï¼š`python 06_planner_executor_verifier.py --request "æŸ¥è¯¢è‹¹æœå…¬å¸çš„ç ”å‘æ”¯å‡ºå’Œå‘˜å·¥æ•°é‡ï¼Œè®¡ç®—äººå‡ç ”å‘æ”¯å‡º"`
- å¼€å¯è¯¦ç»†è°ƒè¯•æ—¥å¿—ï¼š`python 06_planner_executor_verifier.py --debug`

é˜…è¯»å»ºè®®ï¼š
- å…ˆä»"æ•°æ®ç»“æ„ä¸æ¨¡å‹å®šä¹‰"å¼€å§‹ï¼Œç†è§£PEVæ¶æ„çš„çŠ¶æ€ç®¡ç†
- é‡ç‚¹å…³æ³¨"æ ¸å¿ƒèŠ‚ç‚¹å®ç°"ä¸­çš„éªŒè¯å™¨èŠ‚ç‚¹ï¼Œå­¦ä¹ å¦‚ä½•è®¾è®¡é”™è¯¯æ£€æµ‹é€»è¾‘
- æœ€åçœ‹"å·¥ä½œæµæ„å»º"ï¼Œç†è§£æ¡ä»¶è¾¹å¦‚ä½•å®ç°åŠ¨æ€è·¯ç”±
"""

import os
import json
import argparse
import re
from typing import List, TypedDict, Optional
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from rich.console import Console
from rich.syntax import Syntax
from openai import OpenAI, RateLimitError, APIError
import logging
from rich.logging import RichHandler

# =========================
# 1) æ•°æ®ç»“æ„ä¸æ¨¡å‹å®šä¹‰ï¼ˆPydantic v2ï¼‰
# =========================
class Plan(BaseModel):
    """è®¡åˆ’æ¨¡å‹ï¼šå®šä¹‰éœ€è¦æ‰§è¡Œçš„æ­¥éª¤åˆ—è¡¨"""
    steps: List[str] = Field(description="éœ€è¦æ‰§è¡Œçš„å·¥å…·è°ƒç”¨æ­¥éª¤åˆ—è¡¨")

class VerificationResult(BaseModel):
    """éªŒè¯ç»“æœæ¨¡å‹ï¼šå®šä¹‰éªŒè¯å™¨çš„è¾“å‡º"""
    is_failure: bool = Field(description="æ‰§è¡Œç»“æœæ˜¯å¦å¤±è´¥")
    reason: str = Field(description="å¤±è´¥åŸå› æˆ–éªŒè¯é€šè¿‡çš„è¯´æ˜")

class PlannerExecutorVerifierState(TypedDict):
    """è§„åˆ’â†’æ‰§è¡Œâ†’éªŒè¯æ¶æ„çš„å·¥ä½œæµçŠ¶æ€ï¼šåœ¨èŠ‚ç‚¹é—´ä¼ é€’çš„å…±äº«æ•°æ®"""
    user_request: str                      # ç”¨æˆ·çš„åŸå§‹è¯·æ±‚
    plan: Optional[List[str]]             # Plannerç”Ÿæˆçš„æ‰§è¡Œè®¡åˆ’
    intermediate_steps: List[str]         # å·²æ‰§è¡Œçš„æ­¥éª¤ç»“æœ
    verification_result: Optional[dict]   # Verifierçš„éªŒè¯ç»“æœ
    final_answer: Optional[str]           # æœ€ç»ˆç­”æ¡ˆ

# =========================
# 2) LLM ä¸æ§åˆ¶å°åˆå§‹åŒ–
# =========================
console = Console()
DEBUG: bool = False  # é»˜è®¤å…³é—­è°ƒè¯•æ¨¡å¼
STREAM_TOKENS: bool = False  # æ˜¯å¦å¯ç”¨ä»¤ç‰Œæµè¾“å‡º
MAX_STEPS: int = 10  # æœ€å¤§æ‰§è¡Œæ­¥éª¤æ•°

# é…ç½®æ—¥å¿—
logger = logging.getLogger("pev")
handler = RichHandler(console=console, rich_tracebacks=True, markup=True)
handler.setFormatter(logging.Formatter("%(message)s"))
logger.handlers = [handler]
logger.propagate = False
logger.setLevel(logging.DEBUG if DEBUG else logging.INFO)

class ModelScopeChat:
    """
    ModelScopeçš„OpenAIå…¼å®¹æ¥å£é€‚é…å™¨ï¼š
    - æä¾›invoke(prompt)åŸºæœ¬è°ƒç”¨
    - æä¾›with_structured_output(PydanticModel)çš„ç»“æ„åŒ–è¾“å‡ºåŒ…è£…
    - æ”¯æŒAPIé”™è¯¯æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹
    """
    def __init__(self, base_url: str, api_key: str, model: str, fallback_model: Optional[str] = None, temperature: float = 0.2, extra_body: Optional[dict] = None):
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model = model
        self.fallback_model = fallback_model
        self.base_url = base_url
        self.temperature = temperature
        self.extra_body = extra_body or {}
        self.switched = False  # é˜²æ­¢æ— é™åˆ‡æ¢

    def invoke(self, prompt: str, stream_tokens: bool = False):
        try:
            if stream_tokens:
                resp_iter = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    stream=True,
                    extra_body=self.extra_body,
                )
                buffer = []
                for chunk in resp_iter:
                    delta = getattr(chunk.choices[0], "delta", None)
                    token = getattr(delta, "content", "") if delta else ""
                    if token:
                        buffer.append(token)
                        console.print(token, end="")
                return "".join(buffer)
            else:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    stream=False,
                    extra_body=self.extra_body,
                )
                return resp.choices[0].message.content
        except (RateLimitError, APIError) as e:
            if not self.switched and self.fallback_model:
                console.print(f"[bold yellow]âš ï¸ æ¨¡å‹ {self.model} è¯·æ±‚å¤±è´¥: {str(e)}ï¼Œå°è¯•åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹ {self.fallback_model}[/bold yellow]")
                # åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹
                self.model = self.fallback_model
                self.switched = True
                # é‡æ–°å°è¯•è¯·æ±‚
                return self.invoke(prompt, stream_tokens)
            else:
                # å¦‚æœæ²¡æœ‰å¤‡é€‰æ¨¡å‹æˆ–å·²ç»åˆ‡æ¢è¿‡ï¼Œé‡æ–°æŠ›å‡ºå¼‚å¸¸
                raise

    def with_structured_output(self, pyd_model: type[BaseModel]):
        class _StructuredWrapper:
            def __init__(self, outer: "ModelScopeChat"):
                self.outer = outer

            def invoke(self, prompt: str) -> BaseModel:
                # é€šè¿‡ç³»ç»Ÿæç¤ºçº¦æŸä»…è¾“å‡ºJSON
                schema = pyd_model.model_json_schema()
                props = schema.get("properties", {})
                required = schema.get("required", [])
                schema_text_lines = []
                for k, v in props.items():
                    t = v.get("type", "string")
                    schema_text_lines.append(f"- {k}: {t}")
                schema_text = "\n".join(schema_text_lines) or "- è¯·æŒ‰æ¨¡å‹å®šä¹‰ç”Ÿæˆå­—æ®µ"
                required_text = ", ".join(required) if required else "æ‰€æœ‰å­—æ®µ"
                system_msg = (
                    "ä½ æ˜¯ä¸€ä¸ªç»“æ„åŒ–è¾“å‡ºç”Ÿæˆå™¨ã€‚åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸¥æ ¼åŒ¹é…ä»¥ä¸‹å­—æ®µä¸ç±»å‹ï¼š\n"
                    f"{schema_text}\n"
                    f"å¿…é¡»åŒ…å«å­—æ®µï¼š{required_text}\n"
                    "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæˆ–å¤šä½™æ–‡æœ¬ï¼ˆä¾‹å¦‚ä»£ç å—æ ‡è®°ã€å‰åç¼€ï¼‰ã€‚"
                )
                if DEBUG:
                    logger.debug("ğŸ”§ ç”ŸæˆåŠ¨æ€ç³»ç»Ÿæç¤ºï¼ˆåŒ…å«å­—æ®µä¸ç±»å‹è¦æ±‚ï¼‰")
                    logger.debug("ç»“æ„åŒ–è¾“å‡ºæç¤ºï¼ˆç³»ç»Ÿæ¶ˆæ¯ï¼‰ï¼š\n" + system_msg)
                messages = [
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt},
                ]
                try:
                    resp = self.outer.client.chat.completions.create(
                        model=self.outer.model,
                        messages=messages,
                        temperature=self.outer.temperature,
                        stream=False,
                        extra_body=self.outer.extra_body,
                    )
                except (RateLimitError, APIError) as e:
                    if not self.outer.switched and self.outer.fallback_model:
                        console.print(f"[bold yellow]âš ï¸ æ¨¡å‹ {self.outer.model} è¯·æ±‚å¤±è´¥: {str(e)}ï¼Œå°è¯•åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹ {self.outer.fallback_model}[/bold yellow]")
                        # åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹
                        self.outer.model = self.outer.fallback_model
                        self.outer.switched = True
                        # é‡æ–°å°è¯•è¯·æ±‚
                        resp = self.outer.client.chat.completions.create(
                            model=self.outer.model,
                            messages=messages,
                            temperature=self.outer.temperature,
                            stream=False,
                            extra_body=self.outer.extra_body,
                        )
                    else:
                        # å¦‚æœæ²¡æœ‰å¤‡é€‰æ¨¡å‹æˆ–å·²ç»åˆ‡æ¢è¿‡ï¼Œé‡æ–°æŠ›å‡ºå¼‚å¸¸
                        raise
                
                content = resp.choices[0].message.content or ""
                def _extract_json(s: str) -> str:
                    m = re.search(r'(\{[\s\S]*\}|\[[\s\S]*\])', s)
                    return m.group(1) if m else "{}"
                
                raw = content.strip()
                if DEBUG:
                    console.print("[bold blue]ğŸ“¥ æ”¶åˆ°æ¨¡å‹è¿”å›ï¼Œå°è¯•è§£æä¸º JSON[/bold blue]")
                try:
                    data = json.loads(raw)
                except Exception:
                    data = json.loads(_extract_json(raw))
                
                if DEBUG:
                    console.print("[bold cyan]æ¨¡å‹åŸå§‹è¿”å›ï¼ˆæˆªæ–­å±•ç¤ºï¼‰[/bold cyan]:")
                    preview = json.dumps(data, ensure_ascii=False)[:400]
                    console.print(preview + ("..." if len(preview) == 400 else ""))
                
                # éªŒè¯å¹¶è¿”å›ç»“æœ
                try:
                    parsed = pyd_model.model_validate(data)
                    if DEBUG:
                        console.print(f"[bold green]âœ… ç»“æ„åŒ–è§£ææˆåŠŸ[/bold green]ï¼š{pyd_model.__name__}")
                    return parsed
                except Exception as e:
                    console.print(f"[bold red]âŒ ç»“æ„åŒ–è§£æå¤±è´¥: {e}[/bold red]")
                    raise

        return _StructuredWrapper(self)


def init_llm() -> ModelScopeChat:
    """
    åˆå§‹åŒ– ModelScope LLMï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰ã€‚
    - å¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼š
      MODELSCOPE_BASE_URLï¼ˆé»˜è®¤ï¼šhttps://api-inference.modelscope.cn/v1ï¼‰
      MODELSCOPE_API_KEY
      MODELSCOPE_MODEL_IDï¼ˆé»˜è®¤ï¼šdeepseek-ai/DeepSeek-V3.2ï¼‰
      MODELSCOPE_MODEL_ID_R1ï¼ˆå¤‡é€‰æ¨¡å‹ï¼Œå¯é€‰ï¼‰
    """
    base_url = os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
    api_key = os.environ.get("MODELSCOPE_API_KEY", "")
    model_id = os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
    fallback_model = os.environ.get("MODELSCOPE_MODEL_ID_R1")
    
    # ä¸ºé¿å…â€œæœªä¿¡ä»»çš„ chat templateâ€é”™è¯¯ï¼Œå¢åŠ ä¿¡ä»»å‚æ•°ï¼›å¹¶è¯·æ±‚ JSON è¾“å‡ºæ ¼å¼
    extra = {
        "enable_thinking": True,
        "trust_request_chat_template": True,
        "response_format": {"type": "json_object"},
    }
    
    return ModelScopeChat(
        base_url=base_url, 
        api_key=api_key, 
        model=model_id, 
        fallback_model=fallback_model,
        temperature=0.2, 
        extra_body=extra
    )

# =========================
# 2.5) å·¥å…·å®šä¹‰
# =========================
def flaky_web_search(query: str) -> str:
    """
    æ¨¡æ‹Ÿä¸€ä¸ªä¸ç¨³å®šçš„ç½‘ç»œæœç´¢å·¥å…·ï¼Œç”¨äºæ¼”ç¤ºPEVæ¶æ„çš„é”™è¯¯æ£€æµ‹èƒ½åŠ›ã€‚
    - å½“æŸ¥è¯¢åŒ…å«"employee count"æ—¶ï¼Œæ¨¡æ‹ŸAPIå¤±è´¥
    - å…¶ä»–æŸ¥è¯¢åˆ™è¿”å›æˆåŠŸç»“æœ
    """
    console.print(f"--- TOOL: Searching for '{query}'... ---")
    if "employee count" or "å‘˜å·¥æ•°é‡" in query.lower():
        console.print("--- TOOL: [bold red]Simulating API failure![/bold red] ---")
        return "Error: Could not retrieve data. The API endpoint is currently unavailable."
    else:
        # æ¨¡æ‹Ÿæœç´¢ç»“æœ
        mock_results = {
            "apple r&d spend last fiscal year": "Apple's R&D spend in the last fiscal year (2023) was approximately $29 billion.",
            "apple annual revenue": "Apple's annual revenue in 2023 was approximately $383 billion.",
            "apple market share": "Apple's global smartphone market share in 2023 was approximately 18%.",
        }
        return mock_results.get(query.lower(), f"Search results for: {query}...")

# =========================
# 3) æ ¸å¿ƒèŠ‚ç‚¹å®ç°
# =========================
def make_planner_node(llm: "ModelScopeChat"):
    """è§„åˆ’èŠ‚ç‚¹ï¼šæ ¹æ®ç”¨æˆ·è¯·æ±‚ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"""
    planner_llm = llm.with_structured_output(Plan)

    def _node(state: PlannerExecutorVerifierState) -> dict:## è§„åˆ’èŠ‚ç‚¹  
        console.print("--- [bold cyan]PLANNER: ç”Ÿæˆä»»åŠ¡è®¡åˆ’[/bold cyan] ---")
        if DEBUG:
            console.print(f"[bold]è¾“å…¥è¯·æ±‚ï¼š[/bold]{state['user_request']}")
        
        # æ ¹æ®éªŒè¯ç»“æœè°ƒæ•´è§„åˆ’ç­–ç•¥
        if state.get("verification_result") and state["verification_result"]["is_failure"]:
            # å¦‚æœéªŒè¯å¤±è´¥ï¼Œåœ¨è§„åˆ’æ—¶è€ƒè™‘å¤±è´¥åŸå› 
            prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è§„åˆ’å¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·è¯·æ±‚å’Œä¹‹å‰çš„å¤±è´¥åŸå› ï¼Œåˆ¶å®šä¸€ä¸ªæ–°çš„æ‰§è¡Œè®¡åˆ’ã€‚
            
            ç”¨æˆ·è¯·æ±‚ï¼š{state['user_request']}
            ä¹‹å‰çš„å¤±è´¥åŸå› ï¼š{state['verification_result']['reason']}
            
            è¯·å°†è¯·æ±‚åˆ†è§£ä¸ºä¸€ç³»åˆ—éœ€è¦ä½¿ç”¨'flaky_web_search'å·¥å…·çš„æŸ¥è¯¢æ­¥éª¤ã€‚
            æ¯ä¸ªæ­¥éª¤åº”è¯¥æ˜¯ä¸€ä¸ªæ˜ç¡®çš„æŸ¥è¯¢è¯­å¥ã€‚
            å°è¯•ç»•è¿‡ä¹‹å‰å¤±è´¥çš„æŸ¥è¯¢æ–¹å¼ã€‚
            """
        else:
            # æ­£å¸¸è§„åˆ’
            prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è§„åˆ’å¸ˆã€‚è¯·å°†ç”¨æˆ·è¯·æ±‚åˆ†è§£ä¸ºä¸€ç³»åˆ—éœ€è¦ä½¿ç”¨'flaky_web_search'å·¥å…·çš„æŸ¥è¯¢æ­¥éª¤ã€‚
            
            ç”¨æˆ·è¯·æ±‚ï¼š{state['user_request']}
            
            è¯·å°†è¯·æ±‚åˆ†è§£ä¸ºä¸€ç³»åˆ—éœ€è¦ä½¿ç”¨'flaky_web_search'å·¥å…·çš„æŸ¥è¯¢æ­¥éª¤ã€‚
            æ¯ä¸ªæ­¥éª¤åº”è¯¥æ˜¯ä¸€ä¸ªæ˜ç¡®çš„æŸ¥è¯¢è¯­å¥ã€‚
            """
        
        plan = planner_llm.invoke(prompt)
        if DEBUG:
            console.print(f"[bold green]ç”Ÿæˆè®¡åˆ’å®Œæˆ[/bold green]ï¼š{plan.steps}")
        
        return {"plan": plan.steps}

    return _node


def make_executor_node(llm: "ModelScopeChat"):
    """æ‰§è¡ŒèŠ‚ç‚¹ï¼šæ‰§è¡Œè®¡åˆ’ä¸­çš„ä¸‹ä¸€æ­¥"""
    def _node(state: PlannerExecutorVerifierState) -> dict:
        console.print("--- [bold green]EXECUTOR: æ‰§è¡Œä¸‹ä¸€æ­¥[/bold green] ---")
        if not state["plan"]:
            return {"intermediate_steps": state["intermediate_steps"]}
        
        next_step = state["plan"][0]
        if DEBUG:
            console.print(f"[bold]æ‰§è¡Œæ­¥éª¤ï¼š[/bold]{next_step}")
            
        result = flaky_web_search(next_step)
        
        # æ›´æ–°çŠ¶æ€ï¼šç§»é™¤å·²æ‰§è¡Œçš„æ­¥éª¤ï¼Œæ·»åŠ æ‰§è¡Œç»“æœ
        return {
            "plan": state["plan"][1:], 
            "intermediate_steps": state["intermediate_steps"] + [result]
        }

    return _node


def make_verifier_node(llm: "ModelScopeChat"):
    """éªŒè¯èŠ‚ç‚¹ï¼šéªŒè¯æ‰§è¡Œç»“æœæ˜¯å¦æˆåŠŸ"""
    verifier_llm = llm.with_structured_output(VerificationResult)

    def _node(state: PlannerExecutorVerifierState) -> dict:
        console.print("--- [bold yellow]VERIFIER: éªŒè¯æ‰§è¡Œç»“æœ[/bold yellow] ---")
        
        # è·å–æœ€åä¸€ä¸ªæ‰§è¡Œç»“æœ
        last_result = state["intermediate_steps"][-1] if state["intermediate_steps"] else ""
        if DEBUG:
            console.print(f"[bold]æœ€åæ‰§è¡Œç»“æœï¼š[/bold]{last_result}")
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªç»“æœéªŒè¯å™¨ã€‚è¯·æ£€æŸ¥ä»¥ä¸‹æ‰§è¡Œç»“æœæ˜¯å¦æˆåŠŸå®Œæˆäº†æŸ¥è¯¢ä»»åŠ¡ï¼š
        
        æ‰§è¡Œç»“æœï¼š{last_result}
        
        å¦‚æœç»“æœåŒ…å«"Error"æˆ–"API failure"ç­‰é”™è¯¯ä¿¡æ¯ï¼Œåˆ™éªŒè¯å¤±è´¥ã€‚
        å¦åˆ™ï¼ŒéªŒè¯é€šè¿‡ã€‚
        """
        
        verification_result = verifier_llm.invoke(prompt)
        if DEBUG:
            console.print(f"[bold]éªŒè¯ç»“æœï¼š[/bold]{'å¤±è´¥' if verification_result.is_failure else 'é€šè¿‡'}")
            console.print(f"[bold]å¤±è´¥åŸå› ï¼š[/bold]{verification_result.reason}")
        
        if verification_result.is_failure:
            console.print("--- [bold yellow]VERIFIER: æ£€æµ‹åˆ°æ‰§è¡Œå¤±è´¥ï¼Œè§¦å‘é‡æ–°è§„åˆ’[/bold yellow] ---")
        
        return {"verification_result": verification_result.model_dump()}

    return _node


def make_synthesizer_node(llm: "ModelScopeChat"):
    """æ±‡æ€»èŠ‚ç‚¹ï¼šæ ¹æ®æ‰€æœ‰æ‰§è¡Œç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    def _node(state: PlannerExecutorVerifierState) -> dict:
        console.print("--- [bold magenta]SYNTHESIZER: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ[/bold magenta] ---")
        
        context = "\n".join(state["intermediate_steps"])
        prompt = f"""
        è¯·æ ¹æ®ä»¥ä¸‹æœç´¢ç»“æœï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š
        
        ç”¨æˆ·é—®é¢˜ï¼š{state['user_request']}
        æœç´¢ç»“æœï¼š
        {context}
        
        è¯·æä¾›ä¸€ä¸ªæ¸…æ™°ã€ç®€æ´çš„æœ€ç»ˆç­”æ¡ˆã€‚
        """
        
        if DEBUG:
            console.print(f"[bold]æ±‡æ€»ä¸Šä¸‹æ–‡ï¼š[/bold]{context}")
        
        answer = llm.invoke(prompt, stream_tokens=STREAM_TOKENS)
        
        return {"final_answer": answer}

    return _node

# =========================
# 4) æ„å»ºä¸è¿è¡Œ LangGraph å·¥ä½œæµ
# =========================
def build_app(llm: "ModelScopeChat"):
    """æ„å»ºè§„åˆ’â†’æ‰§è¡Œâ†’éªŒè¯å·¥ä½œæµï¼šä½¿ç”¨æ¡ä»¶è¾¹å®ç°åŠ¨æ€è·¯ç”±"""
    graph_builder = StateGraph(PlannerExecutorVerifierState)
    
    # æ·»åŠ èŠ‚ç‚¹
    graph_builder.add_node("planner", make_planner_node(llm))        # è§„åˆ’èŠ‚ç‚¹
    graph_builder.add_node("executor", make_executor_node(llm))      # æ‰§è¡ŒèŠ‚ç‚¹
    graph_builder.add_node("verifier", make_verifier_node(llm))      # éªŒè¯èŠ‚ç‚¹
    graph_builder.add_node("synthesize", make_synthesizer_node(llm))  # æ±‡æ€»èŠ‚ç‚¹
    
    # è®¾ç½®å…¥å£ç‚¹
    graph_builder.set_entry_point("planner")
    
    # æ·»åŠ è¾¹
    graph_builder.add_edge("planner", "executor")  # è§„åˆ’å®Œæˆåæ‰§è¡Œç¬¬ä¸€æ­¥
    graph_builder.add_edge("executor", "verifier")  # æ‰§è¡Œå®ŒæˆåéªŒè¯ç»“æœ
    
    # æ·»åŠ æ¡ä»¶è¾¹ï¼šæ ¹æ®éªŒè¯ç»“æœå†³å®šä¸‹ä¸€æ­¥
    def route_after_verification(state: PlannerExecutorVerifierState) -> str:
        """
        è·¯ç”±å‡½æ•°ï¼šæ ¹æ®å½“å‰çŠ¶æ€å†³å®šå·¥ä½œæµçš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
        
        å‚æ•°ï¼š
            state: å½“å‰å·¥ä½œæµçŠ¶æ€
            - state["plan"]: å‰©ä½™çš„æ‰§è¡Œè®¡åˆ’ï¼ˆæ­¥éª¤åˆ—è¡¨ï¼‰
            - state["verification_result"]: éªŒè¯ç»“æœï¼ˆåŒ…å«is_failureå­—æ®µï¼‰
            
        è¿”å›å€¼ï¼š
            ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„åç§°ï¼š"executor"ã€"planner" æˆ– "synthesize"
        """
        # æƒ…å†µ1ï¼šè¿˜æœ‰æœªæ‰§è¡Œçš„æ­¥éª¤ä¸”å½“å‰æ­¥éª¤éªŒè¯é€šè¿‡
        if state["plan"] and not state["verification_result"]["is_failure"]:
            console.print(f"[bold yellow]è·¯ç”±å†³ç­–ï¼š[/bold yellow]è¿˜æœ‰{len(state['plan'])}ä¸ªæ­¥éª¤å¾…æ‰§è¡Œï¼Œä¸”å½“å‰æ­¥éª¤éªŒè¯é€šè¿‡ â†’ ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªæ­¥éª¤")
            return "executor"
        # æƒ…å†µ2ï¼šå½“å‰æ­¥éª¤éªŒè¯å¤±è´¥
        elif state["verification_result"]["is_failure"]:
            console.print("[bold yellow]è·¯ç”±å†³ç­–ï¼š[/bold yellow]å½“å‰æ­¥éª¤éªŒè¯å¤±è´¥ â†’ è¿”å›è§„åˆ’å™¨é‡æ–°è§„åˆ’")
            return "planner"
        # æƒ…å†µ3ï¼šæ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆä¸”éªŒè¯é€šè¿‡
        else:
            console.print("[bold yellow]è·¯ç”±å†³ç­–ï¼š[/bold yellow]æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆä¸”éªŒè¯é€šè¿‡ â†’ è¿›å…¥æ±‡æ€»é˜¶æ®µ")
            return "synthesize"
    
    # æ·»åŠ æ¡ä»¶è¾¹ï¼šè¿™æ˜¯PEVæ¶æ„çš„æ ¸å¿ƒæ§åˆ¶é€»è¾‘
    graph_builder.add_conditional_edges(
        "verifier",                  # æ¡ä»¶è¾¹çš„èµ·å§‹èŠ‚ç‚¹ï¼šéªŒè¯å™¨èŠ‚ç‚¹
        route_after_verification,    # è·¯ç”±å‡½æ•°ï¼šæ ¹æ®çŠ¶æ€å†³å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹
        {                             # è·¯ç”±æ˜ å°„ï¼šå°†è·¯ç”±å‡½æ•°è¿”å›å€¼æ˜ å°„åˆ°å®é™…èŠ‚ç‚¹åç§°
            "executor": "executor",  # è¿”å›"executor" â†’ ä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ˜¯æ‰§è¡Œå™¨
            "planner": "planner",    # è¿”å›"planner" â†’ ä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ˜¯è§„åˆ’å™¨
            "synthesize": "synthesize"  # è¿”å›"synthesize" â†’ ä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ˜¯æ±‡æ€»å™¨
        }
    )
    
    # æ±‡æ€»å®Œæˆåç»“æŸ
    graph_builder.add_edge("synthesize", END)
    
    if DEBUG:
        console.print("[bold cyan]å·¥ä½œæµæ„å»ºå®Œæˆ[/bold cyan]")
        console.print("å·¥ä½œæµè·¯çº¿ï¼šplanner â†’ executor â†’ verifier â†’ (æ¡ä»¶åˆ†æ”¯)")
        console.print("æ¡ä»¶åˆ†æ”¯ï¼šéªŒè¯é€šè¿‡ä¸”æœ‰å‰©ä½™æ­¥éª¤ â†’ executor")
        console.print("æ¡ä»¶åˆ†æ”¯ï¼šéªŒè¯å¤±è´¥ â†’ planner")
        console.print("æ¡ä»¶åˆ†æ”¯ï¼šéªŒè¯é€šè¿‡ä¸”æ— å‰©ä½™æ­¥éª¤ â†’ synthesize â†’ END")
    
    return graph_builder.compile()


def run_workflow(app, user_request: str) -> PlannerExecutorVerifierState:
    """æ‰§è¡Œå·¥ä½œæµå¹¶è¿”å›æœ€ç»ˆçŠ¶æ€"""
    initial_input = {
        "user_request": user_request,
        "plan": None,
        "intermediate_steps": [],
        "verification_result": None,
        "final_answer": None
    }
    
    console.print(f"[bold cyan]ğŸš€ å¯åŠ¨è§„åˆ’â†’æ‰§è¡Œâ†’éªŒè¯å·¥ä½œæµï¼š[/bold cyan] '{user_request}'")
    
    final_state: Optional[PEVState] = None
    step = 0
    
    for state_update in app.stream(initial_input, stream_mode="values"):
        final_state = state_update
        step += 1
        
        if DEBUG:
            console.print(f"[bold]æ­¥éª¤ {step}[/bold] å½“å‰çŠ¶æ€ï¼š")
            console.print(f"  - å‰©ä½™è®¡åˆ’ï¼š{state_update.get('plan', 'æ— ')}")
            console.print(f"  - å·²æ‰§è¡Œæ­¥éª¤æ•°ï¼š{len(state_update.get('intermediate_steps', []))}")
            if state_update.get('verification_result'):
                console.print(f"  - éªŒè¯ç»“æœï¼š{'å¤±è´¥' if state_update['verification_result']['is_failure'] else 'é€šè¿‡'}")
            if state_update.get('final_answer'):
                console.print(f"  - æœ€ç»ˆç­”æ¡ˆï¼š{state_update['final_answer'][:100]}...")
    
    console.print("[bold green]âœ… PEVå·¥ä½œæµå®Œæˆ[/bold green]")
    
    return final_state or initial_input

# =========================
# 5) è¾“å‡ºè¾…åŠ©ï¼šæ‰“å°æ‰§è¡Œç»“æœ
# =========================
def print_execution_results(state: PlannerExecutorVerifierState) -> None:
    """æ‰“å°æ‰§è¡Œç»“æœçš„è¾…åŠ©å‡½æ•°"""
    console.print("--- ### æ‰§è¡Œè¿‡ç¨‹æ€»ç»“ ### ---")
    
    # æ‰“å°ç”¨æˆ·è¯·æ±‚
    console.print(f"[bold]ç”¨æˆ·è¯·æ±‚ï¼š[/bold]{state['user_request']}")
    
    # æ‰“å°æ‰§è¡Œæ­¥éª¤
    console.print("[bold]æ‰§è¡Œæ­¥éª¤ï¼š[/bold]")
    for i, result in enumerate(state['intermediate_steps']):
        console.print(f"  {i+1}. {result}")
    
    # æ‰“å°æœ€ç»ˆç­”æ¡ˆ
    if state.get('final_answer'):
        console.print("\n[bold]æœ€ç»ˆç­”æ¡ˆï¼š[/bold]")
        console.print(state['final_answer'])

# =========================
# 6) CLI ä¸å…¥å£
# =========================
def parse_args() -> argparse.Namespace:
    """å‘½ä»¤è¡Œå‚æ•°è§£æ"""
    parser = argparse.ArgumentParser(
        description="è§„åˆ’â†’æ‰§è¡Œâ†’éªŒè¯æ¶æ„ï¼šå¸¦é”™è¯¯æ£€æµ‹ä¸è‡ªä¿®æ­£çš„æ™ºèƒ½ä½“å·¥ä½œæµ"
    )
    
    parser.add_argument(
        "--request",
        type=str,
        default="æŸ¥è¯¢è‹¹æœå…¬å¸ä¸Šä¸€è´¢å¹´çš„ç ”å‘æ”¯å‡ºå’Œå‘˜å·¥æ•°é‡ï¼Œè®¡ç®—äººå‡ç ”å‘æ”¯å‡º",
        help="ç”¨æˆ·è¯·æ±‚ï¼ˆé»˜è®¤ï¼šæŸ¥è¯¢è‹¹æœå…¬å¸çš„ç ”å‘æ”¯å‡ºå’Œå‘˜å·¥æ•°é‡ï¼‰",
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¼€å¯è°ƒè¯•æ¨¡å¼ï¼Œè¾“å‡ºè¯¦ç»†æ—¥å¿—",
    )
    
    parser.add_argument(
        "--stream",
        action="store_true",
        help="å¯ç”¨ä»¤ç‰Œæµè¾“å‡º",
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    global DEBUG, STREAM_TOKENS
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    DEBUG = args.debug
    STREAM_TOKENS = args.stream
    
    # æ›´æ–°æ—¥å¿—çº§åˆ«
    logger.setLevel(logging.DEBUG if DEBUG else logging.INFO)
    
    # åˆå§‹åŒ–LLM
    console.print("--- [bold blue]åˆå§‹åŒ–LLM[/bold blue] ---")
    llm = init_llm()
    
    # æ„å»ºå·¥ä½œæµ
    console.print("--- [bold blue]æ„å»ºå·¥ä½œæµ[/bold blue] ---")
    app = build_app(llm)
    
    # æ‰§è¡Œå·¥ä½œæµ
    console.print("--- [bold blue]æ‰§è¡Œå·¥ä½œæµ[/bold blue] ---")
    final_state = run_workflow(app, args.request)
    
    # æ‰“å°ç»“æœ
    console.print("--- [bold blue]è¾“å‡ºç»“æœ[/bold blue] ---")
    print_execution_results(final_state)


if __name__ == "__main__":
    main()