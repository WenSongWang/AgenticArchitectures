# -*- coding: utf-8 -*-
"""
å·¥å…·ä½¿ç”¨ï¼ˆTool Useï¼‰æ¶æ„çš„å¯è¿è¡Œç¤ºä¾‹
 
å­¦ä¹ ç›®æ ‡ï¼ˆçœ‹å®Œä½ èƒ½åšåˆ°ä»€ä¹ˆï¼‰ï¼š
- äº†è§£â€œå·¥å…·ä½¿ç”¨â€æ™ºèƒ½ä½“å¦‚ä½•æŠŠå¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºâ€œè§„åˆ’â†’æ‰§è¡Œâ†’æ±‡æ€»â€
- ç†è§£ LangGraph å¦‚ä½•æŠŠå¤šæ­¥é€»è¾‘ç¼–æ’æˆâ€œæœ‰çŠ¶æ€çš„å·¥ä½œæµâ€
- å­¦ä¼šç”¨ Pydantic v2 çº¦æŸ LLM è¾“å‡ºä¸ºç»“æ„åŒ–æ•°æ®ï¼ˆæ›´ç¨³ã€æ›´å¥½ç”¨ï¼‰
- èƒ½æŠŠè„šæœ¬ä½œä¸ºå‘½ä»¤è¡Œç¨‹åºè¿è¡Œï¼Œå¹¶è§‚å¯Ÿæ¯ä¸€æ­¥çš„ç»“æ„åŒ–ä¸­é—´ç»“æœ
 
æ ¸å¿ƒæ¦‚å¿µé€Ÿè§ˆï¼š
- StateGraphï¼šæœ‰çŠ¶æ€çš„â€œæµç¨‹å›¾â€ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ªå‡½æ•°ï¼ŒèŠ‚ç‚¹ä¹‹é—´æŒ‰è¾¹è¿æ¥é¡ºåºæ‰§è¡Œ
- å·¥å…·æ³¨å†Œè¡¨ï¼ˆTOOLS_REGISTRYï¼‰ï¼šæŠŠå¯ç”¨çš„æœ¬åœ°å·¥å…·ç»Ÿä¸€å£°æ˜ï¼Œä¾¿äº LLM è¿›è¡Œâ€œè§„åˆ’â€ä¸â€œè°ƒç”¨â€
- ä¸‰æ­¥èŠ‚ç‚¹ï¼šè§„åˆ’ï¼ˆPlannerï¼‰â†’ æ‰§è¡Œï¼ˆExecutorï¼‰â†’ æ±‡æ€»ï¼ˆSummarizerï¼‰
 
è¿è¡Œå‰å‡†å¤‡ï¼š
- é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` å¹¶é…ç½®ï¼š
  - `LANGCHAIN_API_KEY`ï¼ˆç”¨äº LangSmith è¿½è¸ªï¼Œå¯é€‰ï¼‰
  - å¦‚ä½¿ç”¨ ModelScope æ¥å…¥ï¼š`MODELSCOPE_BASE_URL`ã€`MODELSCOPE_API_KEY`ã€`MODELSCOPE_MODEL_ID`
  - å¦‚ä½¿ç”¨é«˜å¾·MCPæœåŠ¡ï¼š
    - `AMAP_KEY`ï¼šåœ¨é«˜å¾·å¼€æ”¾å¹³å°ç”³è¯·çš„APIå¯†é’¥ï¼ˆhttps://console.amap.com/ï¼‰
    - `AMAP_MCP_URL`ï¼šé«˜å¾·MCPæœåŠ¡å™¨åœ°å€ï¼ˆå¯é€‰ï¼Œé»˜è®¤ï¼šhttps://mcp.amap.com/mcpï¼‰
 
å¦‚ä½•è¿è¡Œï¼š
- ç›´æ¥è¿è¡Œé»˜è®¤ç¤ºä¾‹ï¼š`python 02_tool_use.py`
- æ›´æ¢ä»»åŠ¡è¯·æ±‚ï¼š`python 02_tool_use.py --request "è¯·è®¡ç®—è¡¨è¾¾å¼ 2+3*4ï¼Œå¹¶åˆ—å‡ºå½“å‰ç›®å½•æ–‡ä»¶ã€‚"`
- å¼€å¯è¯¦ç»†æ•™å­¦æ—¥å¿—ï¼š`python 02_tool_use.py --debug`
 
é˜…è¯»å»ºè®®ï¼š
- å…ˆä»â€œæ•°æ®æ¨¡å‹â€å’Œâ€œä¸‰ä¸ªèŠ‚ç‚¹å‡½æ•°â€å¼€å§‹ç†è§£ï¼Œå†çœ‹â€œbuild_appâ€å’Œâ€œrun_workflowâ€å¦‚ä½•æŠŠæ‰€æœ‰éƒ¨ä»¶ä¸²èµ·æ¥
- æœ¬ç¤ºä¾‹çš„å·¥å…·ä¸ºå®‰å…¨æœ¬åœ°å‡½æ•°ï¼Œä¾¿äºå­¦ä¹ â€œå·¥å…·è°ƒç”¨â€ä¸â€œç»“æ„åŒ–ç¼–æ’â€çš„åŸºæœ¬å¥—è·¯
"""
import os
import json
import argparse
import sys
import platform
from typing import List, TypedDict, Optional, Dict, Any
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from rich.console import Console
from rich.syntax import Syntax
from openai import OpenAI, RateLimitError, APIError
import logging
from rich.logging import RichHandler

console = Console()
DEBUG: bool = True
STREAM_TOKENS: bool = False
MAX_STEPS: int = 10
ON_ERROR: str = "skip"
logger = logging.getLogger("tool_use")
handler = RichHandler(console=console, rich_tracebacks=True, markup=True)
handler.setFormatter(logging.Formatter("%(message)s"))
logger.handlers = [handler]
logger.propagate = False
logger.setLevel(logging.DEBUG if DEBUG else logging.INFO)

class ModelScopeChat:
    """
    ModelScope çš„ OpenAI å…¼å®¹æ¥å£é€‚é…å™¨ï¼š
    - æä¾› invoke(prompt) åŸºæœ¬è°ƒç”¨
    - æä¾› with_structured_output(PydanticModel) çš„ç»“æ„åŒ–è¾“å‡ºåŒ…è£…
 
åˆå­¦è€…ç†è§£è¦ç‚¹ï¼š
- ä¸ºä»€ä¹ˆéœ€è¦â€œé€‚é…å™¨â€ï¼Ÿå› ä¸ºæˆ‘ä»¬çš„å·¥ä½œæµä¾èµ–â€œç»“æ„åŒ–è¾“å‡ºâ€ï¼Œè€Œä¸å°‘æœåŠ¡é»˜è®¤åªè¿”å›çº¯æ–‡æœ¬ã€‚
- é€‚é…å™¨ä¼šå°½é‡è¦æ±‚æ¨¡å‹â€œåªè¾“å‡º JSONâ€ï¼Œå†è§£æä¸º Pydantic v2 æ¨¡å‹ï¼›è¿™æ ·åç»­èŠ‚ç‚¹å°±èƒ½ç¨³ç¨³åœ°æ‹¿åˆ°å­—æ®µï¼Œè€Œä¸æ˜¯æ‚ä¹±çš„æ–‡æœ¬ã€‚
    """
    def __init__(self, base_url: str, api_key: str, model: str, fallback_model: Optional[str] = None, temperature: float = 0.2, extra_body: Optional[dict] = None):
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model = model
        self.fallback_model = fallback_model
        self.base_url = base_url
        self.temperature = temperature
        self.extra_body = extra_body or {}
        self.switched = False
    def invoke(self, prompt: str, stream_tokens: bool = False):
        try:
            if stream_tokens:
                resp_iter = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    stream=True,
                    extra_body=self.extra_body,
                )
                buffer = []
                import sys as _sys
                for chunk in resp_iter:
                    delta = getattr(chunk.choices[0], "delta", None)
                    token = getattr(delta, "content", "") if delta else ""
                    if token:
                        buffer.append(token)
                        _sys.stdout.write(token)
                        _sys.stdout.flush()
                return "".join(buffer)
            else:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    stream=False,
                    extra_body=self.extra_body,
                )
                return resp.choices[0].message.content
        except (RateLimitError, APIError) as e:
            if not self.switched and self.fallback_model:
                if DEBUG:
                    console.print(f"[bold yellow]âš ï¸ ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥ï¼š{e}ï¼Œå°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹[/bold yellow]")
                self.model = self.fallback_model
                self.switched = True
                # é‡è¯•è¯·æ±‚
                if stream_tokens:
                    resp_iter = self.client.chat.completions.create(
                        model=self.model,
                        messages=[{"role": "user", "content": prompt}],
                        temperature=self.temperature,
                        stream=True,
                        extra_body=self.extra_body,
                    )
                    buffer = []
                    import sys as _sys
                    for chunk in resp_iter:
                        delta = getattr(chunk.choices[0], "delta", None)
                        token = getattr(delta, "content", "") if delta else ""
                        if token:
                            buffer.append(token)
                            _sys.stdout.write(token)
                            _sys.stdout.flush()
                    return "".join(buffer)
                else:
                    resp = self.client.chat.completions.create(
                        model=self.model,
                        messages=[{"role": "user", "content": prompt}],
                        temperature=self.temperature,
                        stream=False,
                        extra_body=self.extra_body,
                    )
                    return resp.choices[0].message.content
            else:
                raise
    def with_structured_output(self, pyd_model: type[BaseModel]):
        class _StructuredWrapper:
            def __init__(self, outer: "ModelScopeChat"):
                self.outer = outer
            def invoke(self, prompt: str) -> BaseModel:
                # é€šè¿‡ç³»ç»Ÿæç¤ºçº¦æŸä»…è¾“å‡º JSONï¼ˆå°½é‡æé«˜è§£ææˆåŠŸç‡ï¼‰ï¼Œå¹¶æ˜ç¡®å­—æ®µ/ç±»å‹
                schema = pyd_model.model_json_schema()
                props = schema.get("properties", {})
                required = schema.get("required", [])
                schema_text_lines = []
                for k, v in props.items():
                    t = v.get("type", "string")
                    schema_text_lines.append(f"- {k}: {t}")
                schema_text = "\n".join(schema_text_lines) or "- fields"
                required_text = ", ".join(required) if required else "all"
                system_msg = (
                    "åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸¥æ ¼åŒ¹é…ä»¥ä¸‹å­—æ®µä¸ç±»å‹ï¼š\n"
                    f"{schema_text}\n"
                    f"å¿…é¡»åŒ…å«å­—æ®µï¼š{required_text}\n"
                    "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæˆ–å¤šä½™æ–‡æœ¬ã€‚"
                )
                messages = [
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt},
                ]
                try:
                    if STREAM_TOKENS:
                        content_iter = self.outer.client.chat.completions.create(
                            model=self.outer.model,
                            messages=messages,
                            temperature=self.outer.temperature,
                            stream=True,
                            extra_body=self.outer.extra_body,
                        )
                        import sys as _sys
                        _sys.stdout.write("\nğŸ“¡ æ­£åœ¨æ¥æ”¶ç»“æ„åŒ– JSON...\n")
                        _sys.stdout.flush()
                        parts = []
                        for chunk in content_iter:
                            delta = getattr(chunk.choices[0], "delta", None)
                            token = getattr(delta, "content", "") if delta else ""
                            if token:
                                parts.append(token)
                                _sys.stdout.write(token)
                                _sys.stdout.flush()
                        content = "".join(parts)
                    else:
                        resp = self.outer.client.chat.completions.create(
                            model=self.outer.model,
                            messages=messages,
                            temperature=self.outer.temperature,
                            stream=False,
                            extra_body=self.outer.extra_body,
                        )
                        content = resp.choices[0].message.content or ""
                except (RateLimitError, APIError) as e:
                    if not self.outer.switched and self.outer.fallback_model:
                        if DEBUG:
                            console.print(f"[bold yellow]âš ï¸ ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥ï¼š{e}ï¼Œå°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹[/bold yellow]")
                        self.outer.model = self.outer.fallback_model
                        self.outer.switched = True
                        # é‡è¯•è¯·æ±‚
                        if STREAM_TOKENS:
                            content_iter = self.outer.client.chat.completions.create(
                                model=self.outer.model,
                                messages=messages,
                                temperature=self.outer.temperature,
                                stream=True,
                                extra_body=self.outer.extra_body,
                            )
                            import sys as _sys
                            _sys.stdout.write("\nğŸ“¡ æ­£åœ¨æ¥æ”¶ç»“æ„åŒ– JSON...\n")
                            _sys.stdout.flush()
                            parts = []
                            for chunk in content_iter:
                                delta = getattr(chunk.choices[0], "delta", None)
                                token = getattr(delta, "content", "") if delta else ""
                                if token:
                                    parts.append(token)
                                    _sys.stdout.write(token)
                                    _sys.stdout.flush()
                            content = "".join(parts)
                        else:
                            resp = self.outer.client.chat.completions.create(
                                model=self.outer.model,
                                messages=messages,
                                temperature=self.outer.temperature,
                                stream=False,
                                extra_body=self.outer.extra_body,
                            )
                            content = resp.choices[0].message.content or ""
                    else:
                        raise
                import json as _json, re
                from pydantic import ValidationError
                def _extract_json(s: str) -> str:
                    m = re.search(r'(\{[\s\S]*\}|\[[\s\S]*\])', s)
                    return m.group(1) if m else "{}"
                raw = content.strip()
                try:
                    data = _json.loads(raw)
                except Exception:
                    data = _json.loads(_extract_json(raw))
                try:
                    parsed = pyd_model.model_validate(data)
                    return parsed
                except ValidationError:
                    mappings_applied = False
                    # å…œåº•å­—æ®µæ˜ å°„ï¼šå°½é‡æŠŠå¸¸è§åˆ«åæ˜ å°„åˆ°ç›®æ ‡æ¨¡å‹å­—æ®µ
                    if pyd_model.__name__ == "ToolPlan":
                        if "planned_calls" not in data and "tools" in data:
                            data["planned_calls"] = data.pop("tools")
                            mappings_applied = True
                        if "planned_calls" in data and isinstance(data["planned_calls"], list):
                            normalized = []
                            for item in data["planned_calls"]:
                                if not isinstance(item, dict):
                                    continue
                                tname = item.get("tool_name") or item.get("name") or item.get("tool")
                                args = item.get("arguments") or item.get("args") or item.get("parameters") or {}
                                reason = item.get("reason") or item.get("why") or "auto"
                                assign_to = item.get("assign_to") or item.get("assign") or item.get("save_as") or item.get("var") or None
                                normalized.append({"tool_name": tname, "arguments": args, "reason": reason, "assign_to": assign_to})
                            data["planned_calls"] = normalized
                            mappings_applied = True
                    if pyd_model.__name__ == "FinalAnswer":
                        if "answer" not in data and "final" in data:
                            data["answer"] = data.pop("final")
                            mappings_applied = True
                        if "sources" not in data and "refs" in data:
                            data["sources"] = data.pop("refs")
                            mappings_applied = True
                    parsed = pyd_model.model_validate(data if mappings_applied else data)
                    return parsed
        return _StructuredWrapper(self)

def init_llm() -> ModelScopeChat:
    """
    åˆå§‹åŒ– ModelScope LLMï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰ã€‚
    - å¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼š
      MODELSCOPE_BASE_URLï¼ˆé»˜è®¤ï¼šhttps://api-inference.modelscope.cn/v1ï¼‰
      MODELSCOPE_API_KEY
      MODELSCOPE_MODEL_IDï¼ˆé»˜è®¤ï¼šdeepseek-ai/DeepSeek-V3.2ï¼‰
      MODELSCOPE_MODEL_ID_R1ï¼ˆå¤‡ç”¨æ¨¡å‹ï¼Œå¯é€‰ï¼‰
    - é¢å¤–å‚æ•°ï¼šenable_thinking å¯é€‰ï¼›å¼ºåˆ¶ JSON è¾“å‡º
    - å½“ä¸»æ¨¡å‹è¯·æ±‚å¤±è´¥æ—¶ï¼Œä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹ï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
    """
    base_url = os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
    api_key = os.environ.get("MODELSCOPE_API_KEY", "")
    model_id = os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
    fallback_model_id = os.environ.get("MODELSCOPE_MODEL_ID_R1")
    extra = {
        "enable_thinking": True,
        "trust_request_chat_template": True,
        "response_format": {"type": "json_object"},
    }
    return ModelScopeChat(base_url=base_url, api_key=api_key, model=model_id, fallback_model=fallback_model_id, temperature=0.2, extra_body=extra)

class ToolCall(BaseModel):
    """å•æ¬¡å·¥å…·è°ƒç”¨çš„è®¡åˆ’é¡¹ï¼šé€‰ç”¨å·¥å…·ã€ä¼ å…¥å‚æ•°ã€è°ƒç”¨ç†ç”±"""
    tool_name: str = Field(description="å·¥å…·åç§°")
    arguments: Dict[str, Any] = Field(description="å·¥å…·å…¥å‚å¯¹è±¡")
    reason: str = Field(description="è°ƒç”¨è¯¥å·¥å…·çš„åŸå› ")
    assign_to: Optional[str] = Field(default=None, description="å°†è¯¥å·¥å…·è¾“å‡ºä¿å­˜åˆ°ä¸Šä¸‹æ–‡ä¸­çš„å˜é‡åï¼Œä¾›åç»­æ­¥éª¤å¼•ç”¨")

class ToolPlan(BaseModel):
    """æ•´ä½“å·¥å…·è°ƒç”¨è®¡åˆ’ï¼šæŒ‰é¡ºåºæ’åˆ—å¤šä¸ª ToolCallï¼Œå¹¶ç»™å‡ºè®¡åˆ’æ‘˜è¦"""
    planned_calls: List[ToolCall] = Field(description="æŒ‰é¡ºåºè®¡åˆ’è°ƒç”¨çš„å·¥å…·åˆ—è¡¨")
    plan_summary: str = Field(description="æ•´ä½“è®¡åˆ’æ‘˜è¦")

class ToolResult(BaseModel):
    """å•æ¬¡å·¥å…·è°ƒç”¨çš„ç»“æ„åŒ–ç»“æœï¼šè®°å½•å·¥å…·åä¸è¾“å‡º"""
    tool_name: str = Field(description="å·¥å…·åç§°")
    output: Any = Field(description="å·¥å…·è¾“å‡º")

class ToolExecutionResults(BaseModel):
    """æ‰§è¡Œå¤šä¸ªå·¥å…·è°ƒç”¨åçš„æ±‡æ€»ç»“æœï¼šç»“æœåˆ—è¡¨ä¸æ‰§è¡Œè¿‡ç¨‹æ‘˜è¦"""
    results: List[ToolResult] = Field(description="å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨")
    execution_summary: str = Field(description="æ‰§è¡Œè¿‡ç¨‹æ‘˜è¦")
    context: Dict[str, Any] = Field(default_factory=dict, description="ç´¯è®¡çš„ä¸Šä¸‹æ–‡å˜é‡å­—å…¸ï¼ˆå«æ¯æ­¥ assign_to çš„è¾“å‡ºï¼‰")

class FinalAnswer(BaseModel):
    """æœ€ç»ˆå›ç­”ï¼šç»¼åˆå·¥å…·ç»“æœåçš„ç­”æ¡ˆä¸å¼•ç”¨æ¥æº"""
    answer: str = Field(description="ç»¼åˆå·¥å…·ç»“æœåçš„æœ€ç»ˆå›ç­”")
    sources: List[str] = Field(description="å¼•ç”¨çš„å·¥å…·æˆ–æ¥æºæ ‡è¯†")

class ToolUseState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€ï¼šåœ¨èŠ‚ç‚¹é—´ä¼ é€’çš„å…±äº«æ•°æ®"""
    user_request: str
    tool_plan: Optional[dict]
    tool_results: Optional[dict]
    final_answer: Optional[dict]

def _safe_eval(expr: str) -> float:
    """
    å®‰å…¨ç®—æœ¯è¡¨è¾¾å¼æ±‚å€¼ï¼š
    - ä½¿ç”¨ AST é™åˆ¶å¯ç”¨æ“ä½œç¬¦ï¼Œé¿å…ä»»æ„ä»£ç æ‰§è¡Œé£é™©
    - æ”¯æŒ +, -, *, /, ** ä»¥åŠä¸€å…ƒè´Ÿå·
    """
    import ast, operator as op
    allowed = {
        ast.Add: op.add,
        ast.Sub: op.sub,
        ast.Mult: op.mul,
        ast.Div: op.truediv,
        ast.Pow: op.pow,
        ast.USub: op.neg,
    }
    def _eval(node):
        if isinstance(node, ast.Num):
            return node.n
        if isinstance(node, ast.BinOp) and type(node.op) in allowed:
            return allowed[type(node.op)](_eval(node.left), _eval(node.right))
        if isinstance(node, ast.UnaryOp) and type(node.op) in allowed:
            return allowed[type(node.op)](_eval(node.operand))
        raise ValueError("unsupported expression")
    tree = ast.parse(expr, mode="eval")
    return float(_eval(tree.body))

def tool_calc(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šè®¡ç®—ç®—æœ¯è¡¨è¾¾å¼çš„å€¼ï¼ˆä½¿ç”¨ _safe_eval ä¿è¯å®‰å…¨ï¼‰"""
    expr = str(arguments.get("expression", ""))
    value = _safe_eval(expr)
    return {"expression": expr, "value": value}

def tool_python_info(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šè¿”å›å½“å‰ Python ç‰ˆæœ¬ã€å®ç°ä¸å¹³å°ä¿¡æ¯"""
    return {
        "version": sys.version.split()[0],
        "implementation": platform.python_implementation(),
        "platform": platform.platform(),
    }

def tool_string_length(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šè¿”å›å­—ç¬¦ä¸²é•¿åº¦ä¸å­—ç¬¦ç»Ÿè®¡ï¼Œçº¯å†…å­˜æ— æƒé™é£é™©"""
    s = str(arguments.get("text", ""))
    return {"text": s, "length": len(s)}

def tool_concat(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šæ‹¼æ¥ä¸¤ä¸ªå­—ç¬¦ä¸²ä¸ºä¸€ä¸ªç»“æœ"""
    a = str(arguments.get("a", ""))
    b = str(arguments.get("b", ""))
    return {"result": a + b}

def tool_repeat(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šå°†å­—ç¬¦ä¸²é‡å¤æŒ‡å®šæ¬¡æ•°"""
    text = str(arguments.get("text", ""))
    times = int(arguments.get("times", 1))
    times = max(0, min(times, 1000))
    return {"result": text * times}

def tool_upper(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šå°†å­—ç¬¦ä¸²è½¬ä¸ºå¤§å†™"""
    text = str(arguments.get("text", ""))
    return {"result": text.upper()}

def tool_normalize_text(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šæ ‡å‡†åŒ–æ–‡æœ¬ï¼ˆå°å†™ã€å»é™¤å¸¸è§æ ‡ç‚¹ã€å‹ç¼©ç©ºç™½ï¼‰"""
    import re
    text = str(arguments.get("text", ""))
    lower = text.lower()
    no_punct = re.sub(r"[^\w\s]", " ", lower)
    normalized = re.sub(r"\s+", " ", no_punct).strip()
    return {"normalized": normalized, "orig": text}

def tool_tokenize_words(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šæŒ‰ç©ºç™½åˆ†è¯ï¼Œè¿”å›è¯åˆ—è¡¨"""
    text = str(arguments.get("text", ""))
    tokens = [t for t in text.split() if t]
    return {"tokens": tokens}

def tool_remove_stopwords(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šç§»é™¤å¸¸è§è‹±æ–‡åœç”¨è¯ï¼ˆç¤ºä¾‹ç”¨ï¼Œå°é›†åˆï¼‰ï¼Œä¿æŒä¸­æ–‡è¯ä¸å˜"""
    tokens = list(arguments.get("tokens", []))
    stop = {"the","a","an","and","or","of","to","in","on","for","with","is","are","be","this","that","it"}
    cleaned = [t for t in tokens if t.lower() not in stop]
    return {"tokens": cleaned}

def tool_keyword_extract(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šåŸºäºè¯é¢‘çš„ç®€æ˜“å…³é”®è¯æå–ï¼Œè¿”å› top_k å…³é”®è¯ä¸è®¡æ•°"""
    tokens = list(arguments.get("tokens", []))
    top_k = int(arguments.get("top_k", 5))
    from collections import Counter
    cnt = Counter(tokens)
    top = cnt.most_common(max(1, top_k))
    return {"keywords": [{"term": t, "count": c} for t, c in top]}

def tool_word_count(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šç»Ÿè®¡å­—ç¬¦ä¸²çš„å•è¯æ•°é‡ï¼ˆä»¥ç©ºç™½åˆ†éš”ï¼‰ï¼Œçº¯å†…å­˜å®‰å…¨"""
    s = str(arguments.get("text", ""))
    words = [w for w in s.split() if w]
    return {"text": s, "word_count": len(words)}

def tool_current_time(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šè¿”å›å½“å‰æœ¬åœ°æ—¶é—´å­—ç¬¦ä¸²ï¼Œä¸æ¶‰åŠå¤–éƒ¨è®¿é—®"""
    from datetime import datetime
    fmt = str(arguments.get("format", "%Y-%m-%d %H:%M:%S"))
    return {"now": datetime.now().strftime(fmt), "format": fmt}

def tool_title_case(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šæ ‡é¢˜åŒ–æ–‡æœ¬ï¼ˆè‹±æ–‡ç¤ºä¾‹ï¼Œä¸­æ–‡åŸºæœ¬ä¿æŒä¸å˜ï¼‰"""
    text = str(arguments.get("text", ""))
    return {"title": text.title()}

def tool_slugify(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šç”Ÿæˆ URL å‹å¥½çš„ slugï¼ˆç®€åŒ–è§„åˆ™ï¼‰"""
    import re
    text = str(arguments.get("text", ""))
    lower = text.lower().strip()
    slug = re.sub(r"[^\w\s-]", "", lower)
    slug = re.sub(r"\s+", "-", slug)
    slug = re.sub(r"-{2,}", "-", slug).strip("-")
    return {"slug": slug}

def tool_render_report(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šå°†ç®¡çº¿å„æ­¥ç»“æœæ¸²æŸ“ä¸º Markdown æŠ¥å‘Šï¼Œä¾¿äºå±•ç¤ºå­¦ä¹ æˆæœ"""
    original = str(arguments.get("original", ""))
    normalized = str(arguments.get("normalized", ""))
    keywords = list(arguments.get("keywords", []))
    sentiment = dict(arguments.get("sentiment", {}))
    now = str(arguments.get("time", ""))
    import re
    # è‡ªåŠ¨ç”Ÿæˆæ ‡é¢˜ä¸ slugï¼ˆè‹¥æœªæä¾›ï¼‰
    title = str(arguments.get("title", "")).strip()
    slug = str(arguments.get("slug", "")).strip()
    if not title:
        # å–æ ‡å‡†åŒ–æ–‡æœ¬çš„å‰è‹¥å¹²è¯ä½œä¸ºæ ‡é¢˜
        head = " ".join(normalized.split()[:8]) if normalized else "Processed Text Report"
        title = head.title()
    if not slug:
        lower = normalized.lower().strip()
        slug = re.sub(r"[^\w\s-]", "", lower)
        slug = re.sub(r"\s+", "-", slug)
        slug = re.sub(r"-{2,}", "-", slug).strip("-") or "processed-text-report"
    lines = []
    lines.append(f"# {title}")
    lines.append(f"- Time: {now}")
    lines.append(f"- Slug: `{slug}`")
    lines.append("")
    lines.append("## Original")
    lines.append(f"{original}")
    lines.append("")
    lines.append("## Normalized")
    lines.append(f"{normalized}")
    lines.append("")
    lines.append("## Keywords")
    if keywords:
        lines.append("\n".join([f"- {k.get('term')} ({k.get('count')})" for k in keywords]))
    else:
        lines.append("- (none)")
    lines.append("")
    lines.append("## Sentiment")
    if sentiment:
        lines.append(f"- Label: {sentiment.get('label')}")
        lines.append(f"- Score: {sentiment.get('score')}")
    else:
        lines.append("- (n/a)")
    return {"markdown": "\n".join(lines)}

def tool_amap_mcp(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šè°ƒç”¨é«˜å¾·MCPæœåŠ¡ï¼Œè·å–åœ°å›¾ç›¸å…³æ•°æ®
    
    é«˜å¾·MCPæœåŠ¡å™¨æ”¯æŒçš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
    - åœ°ç†ç¼–ç ï¼ˆåœ°å€è½¬åæ ‡ï¼‰
    - é€†åœ°ç†ç¼–ç ï¼ˆåæ ‡è½¬åœ°å€ï¼‰
    - è·¯çº¿è§„åˆ’
    - å…´è¶£ç‚¹æŸ¥è¯¢
    - åœ°å›¾æ•°æ®æŸ¥è¯¢ç­‰
    
    å‚æ•°ç¤ºä¾‹ï¼š
    {"service": "geocode", "parameters": {"address": "åŒ—äº¬å¸‚æœé˜³åŒº"}}
    {"service": "regeo", "parameters": {"location": "116.407413,39.904211"}}
    
    è¿”å›ç¤ºä¾‹ï¼š
    {"status": "success", "data": {"geocodes": [...]}, "url": "è¯·æ±‚URL"}
    """
    import os, requests
    
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    amap_mcp_url = os.environ.get("AMAP_MCP_URL", "https://mcp.amap.com/mcp")
    amap_key = os.environ.get("AMAP_KEY", "")
    
    if not amap_key:
        return {"error": "AMAP_KEY æœªåœ¨ç¯å¢ƒå˜é‡ä¸­é…ç½®ï¼Œè¯·å‰å¾€ https://console.amap.com/ ç”³è¯·"}
    
    # è§£æMCPæœåŠ¡å‚æ•°
    service = arguments.get("service")
    parameters = arguments.get("parameters", {})
    
    if not service:
        return {"error": "ç¼ºå°‘å¿…è¦å‚æ•° 'service'ï¼Œè¯·æŒ‡å®šè¦è°ƒç”¨çš„é«˜å¾·MCPæœåŠ¡"}
    
    # æ„å»ºè¯·æ±‚å‚æ•°ï¼šå°†API keyå’ŒæœåŠ¡å‚æ•°åˆå¹¶
    params = {"key": amap_key, "service": service, **parameters}
    
    try:
        # å‘é€è¯·æ±‚åˆ°é«˜å¾·MCPæœåŠ¡å™¨ï¼ˆä½¿ç”¨POSTæ–¹æ³•ï¼Œé«˜å¾·MCPæœåŠ¡é€šå¸¸è¦æ±‚POSTè¯·æ±‚ï¼‰
        response = requests.post(amap_mcp_url, json=params, timeout=15)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        
        # è§£æå“åº”
        result = response.json()
        
        # æ£€æŸ¥é«˜å¾·APIè¿”å›çš„çŠ¶æ€ç 
        if result.get("status") != "1":
            error_info = result.get("info", "æœªçŸ¥é”™è¯¯")
            error_code = result.get("infocode", "0")
            return {"status": "error", "error": f"é«˜å¾·APIé”™è¯¯: {error_info} (é”™è¯¯ç : {error_code})", "url": response.url}
        
        return {"status": "success", "data": result, "url": response.url}
    except requests.exceptions.RequestException as e:
        return {"status": "error", "error": f"è¯·æ±‚å¤±è´¥: {str(e)}", "url": amap_mcp_url}
    except ValueError as e:
        return {"status": "error", "error": f"å“åº”è§£æå¤±è´¥: {str(e)}", "url": f"{amap_mcp_url}?{'&'.join([f'{k}={v}' for k, v in params.items()])}"}

def tool_sentiment_simple(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·ï¼šåŸºäºç®€æ˜“è¯å…¸çš„æƒ…æ„Ÿåˆ¤æ–­ï¼ˆè‹±æ–‡ç¤ºä¾‹ï¼‰"""
    text = str(arguments.get("text", ""))
    pos = {"good","great","excellent","love","happy","awesome","nice","cool","fast","smart"}
    neg = {"bad","terrible","awful","hate","sad","slow","stupid","bug","issue"}
    import re
    tokens = re.findall(r"\w+", text.lower())
    p = sum(1 for t in tokens if t in pos)
    n = sum(1 for t in tokens if t in neg)
    score = float(p - n)
    label = "positive" if score > 0 else ("negative" if score < 0 else "neutral")
    return {"score": score, "label": label}

TOOLS_REGISTRY = {
    # å·¥å…·æ³¨å†Œè¡¨ï¼šç»Ÿä¸€å£°æ˜å¯ç”¨çš„æœ¬åœ°å·¥å…·ï¼Œä¾¿äºâ€œè§„åˆ’èŠ‚ç‚¹â€å¼•ç”¨
    "normalize_text": tool_normalize_text,
    "tokenize_words": tool_tokenize_words,
    "keyword_extract": tool_keyword_extract,
    "current_time": tool_current_time,
    "render_report": tool_render_report,
    "amap_mcp": tool_amap_mcp,  # é«˜å¾·MCPåœ°å›¾æœåŠ¡å·¥å…·ï¼ˆåŸºäºModel Context Protocolï¼‰
}

def make_planner_node(llm: "ModelScopeChat"):
    """è§„åˆ’èŠ‚ç‚¹ï¼šè®© LLM é€‰æ‹©å¹¶ç¼–æ’å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œè¾“å‡ºç»“æ„åŒ–è®¡åˆ’ï¼ˆToolPlanï¼‰"""
    planner_llm = llm.with_structured_output(ToolPlan)
    def _node(state: ToolUseState) -> dict:
        if DEBUG:
            console.print("[bold]è§„åˆ’å·¥å…·è°ƒç”¨[/bold]")
        tool_catalog = json.dumps(
            {
                "available_tools": [
                    {"name": "normalize_text", "args": {"text": "åŸå§‹æ–‡æœ¬"}},
                    {"name": "tokenize_words", "args": {"text": "è¦åˆ†è¯çš„æ–‡æœ¬"}},
                    {"name": "keyword_extract", "args": {"tokens": "è¯åˆ—è¡¨", "top_k": "å…³é”®è¯æ•°é‡ï¼ˆæ•´æ•°ï¼‰"}},
                    {"name": "current_time", "args": {"format": "å¯é€‰ï¼Œæ—¶é—´æ ¼å¼ï¼Œé»˜è®¤ '%Y-%m-%d %H:%M:%S'"}},
                    {"name": "render_report", "args": {"original": "åŸæ–‡", "normalized": "æ ‡å‡†åŒ–æ–‡æœ¬", "keywords": "å…³é”®è¯åˆ—è¡¨", "time": "æ—¶é—´å­—ç¬¦ä¸²"}},
                    {"name": "amap_mcp", "args": {"service": "é«˜å¾·MCPæœåŠ¡åç§°ï¼ˆå¦‚ï¼šgeocode, regeo, route, poiï¼‰", "parameters": "æœåŠ¡å‚æ•°å¯¹è±¡ï¼ˆæ ¹æ®ä¸åŒæœåŠ¡ç±»å‹ï¼‰"}},  # é«˜å¾·MCPåœ°å›¾æœåŠ¡ï¼ˆåŸºäºModel Context Protocolï¼‰
                ]
            },
            ensure_ascii=False,
            indent=2,
        )
        prompt = (
            "ä½ æ˜¯å·¥å…·è§„åˆ’åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·è¯·æ±‚é€‰æ‹©å¹¶ç¼–æ’å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œè¿”å› JSON è®¡åˆ’ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1) æ¯ä¸ªæ­¥éª¤å¯ä½¿ç”¨ assign_to å°†è¾“å‡ºä¿å­˜ä¸ºå˜é‡åï¼Œä»¥ä¾¿åç»­æ­¥éª¤é€šè¿‡ $å˜é‡å æˆ– $å˜é‡å.å­—æ®µ å¼•ç”¨ã€‚\n"
            "2) åˆå§‹ä¸Šä¸‹æ–‡åŒ…å«å˜é‡ï¼š$requestï¼ˆå³ç”¨æˆ·è¯·æ±‚æ–‡æœ¬ï¼‰ã€‚\n"
            "3) ä»…ä½¿ç”¨ç»™å®šçš„å¯ç”¨å·¥å…·ï¼Œå¿…è¦æ—¶å¤šæ­¥ä¸²è”ï¼Œç¡®ä¿åç»­æ­¥éª¤å‚æ•°å¯ä»å‰åºå˜é‡ä¸­å–å€¼ã€‚\n\n"
            "ç¤ºä¾‹ï¼ˆæ–‡æœ¬å¤„ç†ç®¡çº¿ï¼Œç²¾ç®€ç‰ˆï¼‰ï¼š\n"
            "{\n"
            "  \"planned_calls\": [\n"
            "    {\"tool_name\": \"normalize_text\", \"arguments\": {\"text\": \"$request\"}, \"reason\": \"æ ‡å‡†åŒ–åŸæ–‡\", \"assign_to\": \"normalized\"},\n"
            "    {\"tool_name\": \"tokenize_words\", \"arguments\": {\"text\": \"$normalized.normalized\"}, \"reason\": \"åˆ†è¯\", \"assign_to\": \"tokens\"},\n"
            "    {\"tool_name\": \"keyword_extract\", \"arguments\": {\"tokens\": \"$tokens.tokens\", \"top_k\": 5}, \"reason\": \"æå–å…³é”®è¯\", \"assign_to\": \"keywords\"},\n"
            "    {\"tool_name\": \"current_time\", \"arguments\": {\"format\": \"%Y-%m-%d %H:%M\"}, \"reason\": \"è®°å½•æ—¶é—´\", \"assign_to\": \"time\"},\n"
            "    {\"tool_name\": \"render_report\", \"arguments\": {\"original\": \"$normalized.orig\", \"normalized\": \"$normalized.normalized\", \"keywords\": \"$keywords.keywords\", \"time\": \"$time.now\"}, \"reason\": \"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š\", \"assign_to\": \"report\"}\n"
            "  ],\n"
            "  \"plan_summary\": \"æ ‡å‡†åŒ–â†’åˆ†è¯â†’å…³é”®è¯â†’æ—¶é—´â†’æŠ¥å‘Š\"\n"
            "}\n\n"
            f"å·¥å…·ç›®å½•ï¼š\n{tool_catalog}\n\n"
            f"ç”¨æˆ·è¯·æ±‚ï¼š\n{state['user_request']}\n"
        )
        from rich.console import Console as _Console
        _status_console = _Console()
        with _status_console.status("è§„åˆ’ä¸­...", spinner="dots"):
            plan = planner_llm.invoke(prompt)
        return {"tool_plan": plan.model_dump()}
    return _node

def make_plan_check_node():
    def _node(state: ToolUseState) -> dict:
        plan = state.get("tool_plan") or {}
        calls = list(plan.get("planned_calls", []))
        warnings = []
        # æ­¥æ•°é™åˆ¶
        if len(calls) > MAX_STEPS:
            warnings.append(f"planned_calls exceeded MAX_STEPS={MAX_STEPS}, truncating")
            calls = calls[:MAX_STEPS]
        # å·¥å…·ä¸å˜é‡æ ¡éªŒã€assign_to å…œåº•ä¸å»é‡
        seen_vars = {"request"}
        used_names = set()
        cleaned = []
        for idx, item in enumerate(calls):
            name = item.get("tool_name")
            args = item.get("arguments", {}) or {}
            assign_to = item.get("assign_to") or f"{name}_{idx}"
            if not name or name not in TOOLS_REGISTRY:
                warnings.append(f"unknown tool '{name}' at step {idx}, skipping")
                continue
            base_assign = assign_to
            suffix = 1
            while assign_to in used_names:
                assign_to = f"{base_assign}_{suffix}"
                suffix += 1
            used_names.add(assign_to)
            # å˜é‡å¼•ç”¨åŸºç¡€æ ¡éªŒï¼ˆåªæ ¡éªŒåŸºå˜é‡æ˜¯å¦å·²å®šä¹‰ï¼‰
            def _base_vars(v):
                if isinstance(v, str) and v.startswith("$"):
                    return v[1:].split(".")[0]
                return None
            missing_refs = []
            for v in args.values():
                b = _base_vars(v)
                if b and b not in seen_vars:
                    missing_refs.append(b)
            if missing_refs:
                warnings.append(f"step {idx} references undefined vars: {sorted(set(missing_refs))}, keeping step but may fail")
            cleaned.append({"tool_name": name, "arguments": args, "reason": item.get("reason", "auto"), "assign_to": assign_to})
            seen_vars.add(assign_to)
        plan_checked = {
            "planned_calls": cleaned,
            "plan_summary": (plan.get("plan_summary") or "") + ((" | warnings: " + "; ".join(warnings)) if warnings else "")
        }
        if DEBUG and warnings:
            console.print("[bold yellow]è§„åˆ’æ£€æŸ¥è­¦å‘Š[/bold yellow]: " + "; ".join(warnings))
        return {"tool_plan": plan_checked}
    return _node

def make_executor_node():
    """æ‰§è¡ŒèŠ‚ç‚¹ï¼šä¾æ¬¡æ‰§è¡Œå·¥å…·è®¡åˆ’ï¼Œè¿”å›ç»“æ„åŒ–çš„æ‰§è¡Œç»“æœï¼ˆToolExecutionResultsï¼‰"""
    def _resolve_value(value: Any, ctx: Dict[str, Any]) -> Any:
        # å­—ç¬¦ä¸²å¼•ç”¨è§£æï¼š"$var" æˆ– "$var.field"
        if isinstance(value, str) and value.startswith("$"):
            path = value[1:]
            parts = path.split(".")
            base = ctx.get(parts[0], None)
            if base is None:
                return None
            cur = base
            for p in parts[1:]:
                if isinstance(cur, dict):
                    cur = cur.get(p, None)
                else:
                    cur = getattr(cur, p, None) if hasattr(cur, p) else None
            return cur
        return value
    def _resolve_args(args: Dict[str, Any], ctx: Dict[str, Any]) -> Dict[str, Any]:
        def _resolve(obj: Any) -> Any:
            if isinstance(obj, dict):
                return {k: _resolve(v) for k, v in obj.items()}
            if isinstance(obj, list):
                return [_resolve(v) for v in obj]
            return _resolve_value(obj, ctx)
        return _resolve(args or {})
    def _node(state: ToolUseState) -> dict:
        if DEBUG:
            console.print("[bold]æ‰§è¡Œå·¥å…·è®¡åˆ’[/bold]")
        planned = state["tool_plan"]["planned_calls"] if state.get("tool_plan") else []
        results: List[Dict[str, Any]] = []
        context: Dict[str, Any] = {"request": state["user_request"]}
        errors = 0
        for call in planned:
            name = call.get("tool_name")
            args = call.get("arguments", {}) or {}
            assign_to = call.get("assign_to") or f"{name}"
            func = TOOLS_REGISTRY.get(name)
            if func is None:
                results.append({"tool_name": name, "output": {"error": "unknown tool"}})
                errors += 1
                continue
            try:
                resolved_args = _resolve_args(args, context)
                out = func(resolved_args)
                results.append({"tool_name": name, "output": out})
                context[assign_to] = out
            except Exception as e:
                results.append({"tool_name": name, "output": {"error": str(e)}})
                errors += 1
                if ON_ERROR == "stop":
                    break
                elif ON_ERROR == "skip":
                    continue
                elif ON_ERROR == "fallback":
                    continue
        summary = "done" + (f" with {errors} error(s), policy={ON_ERROR}" if errors else "")
        payload = ToolExecutionResults(
            results=[ToolResult(tool_name=r["tool_name"], output=r["output"]) for r in results],
            execution_summary=summary,
            context=context,
        )
        return {"tool_results": payload.model_dump()}
    return _node

def make_summarizer_node(llm: "ModelScopeChat"):
    """æ±‡æ€»èŠ‚ç‚¹ï¼šç»¼åˆå·¥å…·æ‰§è¡Œç»“æœï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”ä¸å¼•ç”¨æ¥æºï¼ˆFinalAnswerï¼‰"""
    summarizer_llm = llm.with_structured_output(FinalAnswer)
    def _node(state: ToolUseState) -> dict:
        if DEBUG:
            console.print("[bold]æ±‡æ€»ç”Ÿæˆç­”æ¡ˆ[/bold]")
        req = state["user_request"]
        tool_results_json = json.dumps(state.get("tool_results", {}), ensure_ascii=False, indent=2)
        context_json = json.dumps(state.get("tool_results", {}).get("context", {}), ensure_ascii=False, indent=2)
        prompt = (
            "ä½ æ˜¯ç»“æœæ±‡æ€»å™¨ã€‚æ ¹æ®ç”¨æˆ·è¯·æ±‚ä¸å·¥å…·æ‰§è¡Œç»“æœï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”ä¸å¼•ç”¨æ¥æºåˆ—è¡¨ã€‚\n"
            "è‹¥å·¥å…·ç»“æœå«é”™è¯¯ï¼Œéœ€è¯´æ˜å¹¶ç»™å‡ºå¯è¡Œçš„æ›¿ä»£å»ºè®®ã€‚\n\n"
            f"ç”¨æˆ·è¯·æ±‚ï¼š\n{req}\n\n"
            f"å·¥å…·ç»“æœï¼š\n{tool_results_json}\n\n"
            f"ä¸Šä¸‹æ–‡å˜é‡ï¼š\n{context_json}\n"
        )
        from rich.console import Console as _Console
        _status_console = _Console()
        with _status_console.status("æ±‡æ€»ä¸­...", spinner="dots"):
            ans = summarizer_llm.invoke(prompt)
        return {"final_answer": ans.model_dump()}
    return _node

def build_app(llm: "ModelScopeChat"):
    """
    æ„å»ºçº¿æ€§å·¥ä½œæµï¼šplanner â†’ executor â†’ summarizer â†’ END
    æ•™å­¦è¯´æ˜ï¼š
    - æˆ‘ä»¬ä½¿ç”¨ LangGraph çš„ StateGraph æ¥å®šä¹‰â€œèŠ‚ç‚¹ + è¾¹â€çš„æœ‰çŠ¶æ€æµç¨‹å›¾
    - æ¯ä¸ªèŠ‚ç‚¹ï¼ˆplanner/executor/summarizerï¼‰éƒ½æ˜¯ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°ï¼Œæ¥æ”¶å¹¶è¿”å›çŠ¶æ€ï¼ˆToolUseState çš„éƒ¨åˆ†å­—æ®µï¼‰
    - set_entry_point æŒ‡å®šæµç¨‹çš„èµ·ç‚¹ï¼›add_edge ç”¨äºæŒ‡å®šèŠ‚ç‚¹ä¹‹é—´çš„æ‰§è¡Œé¡ºåº
    - compile å°†æ„å»ºå™¨â€œç¼–è¯‘â€ä¸ºå¯æ‰§è¡Œçš„åº”ç”¨å¯¹è±¡ï¼ˆappï¼‰ï¼Œéšåå¯ç”¨ app.stream è¿›è¡Œæµå¼æ‰§è¡Œ
    """
    graph_builder = StateGraph(ToolUseState)  # å£°æ˜çŠ¶æ€ç±»å‹ï¼Œä¾¿äº IDE ä¸è¯»è€…ç†è§£èŠ‚ç‚¹é—´ä¼ é€’çš„å­—æ®µç»“æ„
    # æ·»åŠ  3 ä¸ªæ ¸å¿ƒèŠ‚ç‚¹ï¼šè§„åˆ’ â†’ æ‰§è¡Œ â†’ æ±‡æ€»
    graph_builder.add_node("planner", make_planner_node(llm))       # è®© LLM ç”Ÿæˆç»“æ„åŒ–çš„å·¥å…·è°ƒç”¨è®¡åˆ’ï¼ˆToolPlanï¼‰
    graph_builder.add_node("plan_check", make_plan_check_node())    # è§„åˆ’è‡ªæ£€ä¸è§„èŒƒåŒ–ï¼Œé™ä½æ‰§è¡Œé˜¶æ®µé£é™©
    graph_builder.add_node("executor", make_executor_node())        # ä¾æ¬¡æ‰§è¡Œè®¡åˆ’ï¼Œç´¯ç§¯ä¸Šä¸‹æ–‡å­—å…¸ï¼ˆcontextï¼‰
    graph_builder.add_node("summarizer", make_summarizer_node(llm)) # æ ¹æ®æ‰§è¡Œç»“æœä¸ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆå›ç­”ï¼ˆFinalAnswerï¼‰
    # è®¾ç½®æµç¨‹å…¥å£ä¸è¾¹ï¼ˆçº¿æ€§ç¼–æ’ï¼‰
    graph_builder.set_entry_point("planner")        # å…¥å£ï¼šå…ˆåšâ€œè§„åˆ’â€
    graph_builder.add_edge("planner", "plan_check") # è¾¹ï¼šè§„åˆ’ â†’ è§„åˆ’è‡ªæ£€
    graph_builder.add_edge("plan_check", "executor")# è¾¹ï¼šè‡ªæ£€ â†’ æ‰§è¡Œ
    graph_builder.add_edge("executor", "summarizer")# è¾¹ï¼šæ‰§è¡Œ â†’ æ±‡æ€»
    graph_builder.add_edge("summarizer", END)      # ç»“æŸï¼šæ±‡æ€»åç»“æŸï¼ˆè¿”å›æœ€ç»ˆçŠ¶æ€ï¼‰
    # compile å°†å›¾è½¬ä¸ºå¯è¿è¡Œå¯¹è±¡ï¼›ä½ å¯ä»¥ç”¨ app.invoke/app.stream æ‰§è¡Œ
    return graph_builder.compile()

def run_workflow(app, user_request: str) -> ToolUseState:
    """
    æ‰§è¡Œå·¥ä½œæµå¹¶è¿”å›æœ€ç»ˆçŠ¶æ€ï¼›æ‰“å°çŠ¶æ€å­—æ®µå˜åŒ–ä¾¿äºå­¦ä¹ ç†è§£
    æ•™å­¦è¯´æ˜ï¼š
    - initial_input æ˜¯å·¥ä½œæµçš„åˆå§‹çŠ¶æ€ï¼ŒåªåŒ…å«ç”¨æˆ·è¯·æ±‚ï¼ˆuser_requestï¼‰
    - app.stream ä¼šæŒ‰æˆ‘ä»¬åœ¨å›¾ä¸­å®šä¹‰çš„â€œèŠ‚ç‚¹é¡ºåºâ€æ‰§è¡Œï¼Œå¹¶åœ¨æ¯ä¸€æ­¥è¿”å›å½“å‰çš„â€œçŠ¶æ€å¢é‡â€
    - stream_mode="values" è¡¨ç¤ºä»…è¿”å›çŠ¶æ€å­—å…¸ï¼ˆä¸åŒ…å«èŠ‚ç‚¹åç­‰é¢å¤–ä¿¡æ¯ï¼‰ï¼Œä¾¿äºç›´è§‚æŸ¥çœ‹å­—æ®µå˜åŒ–
    - final_state æŒæœ‰æœ€åä¸€æ­¥çš„å®Œæ•´çŠ¶æ€ï¼Œé€šå¸¸ä¼šåŒ…å« tool_planã€tool_resultsã€final_answer ç­‰å­—æ®µ
    """
    initial_input = {"user_request": user_request}  # åˆå§‹çŠ¶æ€ï¼šä»…å«ç”¨æˆ·è¯·æ±‚æ–‡æœ¬
    final_state: Optional[ToolUseState] = None
    # é€æ­¥æ‰§è¡Œï¼šplanner â†’ executor â†’ summarizer â†’ END
    for state_update in app.stream(initial_input, stream_mode="values"):
        final_state = state_update  # è¦†ç›–ä¸ºå½“å‰æ­¥éª¤çš„æœ€æ–°çŠ¶æ€ï¼›å¾ªç¯ç»“æŸåå³ä¸ºæœ€ç»ˆçŠ¶æ€
        if DEBUG:
            # æ‰“å°è¯¥æ­¥éª¤äº§ç”Ÿ/æ›´æ–°çš„çŠ¶æ€å­—æ®µåï¼Œå¸®åŠ©æ–°æ‰‹è§‚å¯Ÿâ€œæ•°æ®æ˜¯å¦‚ä½•é€æ­¥ä¸°å¯Œèµ·æ¥çš„â€
            console.print(f"[bold]çŠ¶æ€æ›´æ–°ï¼š[/bold]{list(state_update.keys())}")
    # è‹¥å› å¼‚å¸¸ä¸­æ–­ï¼Œåˆ™å…œåº•è¿”å› initial_inputï¼›æ­£å¸¸åˆ™è¿”å›å®Œæ•´ final_state
    return final_state or initial_input

def print_outputs(state: ToolUseState) -> None:
    """æ‰“å°å·¥å…·è®¡åˆ’ã€æ‰§è¡Œç»“æœä¸æœ€ç»ˆå›ç­”ï¼Œä¾¿äºç›´è§‚æ•™å­¦å±•ç¤º"""
    console.print("--- ### å·¥å…·è®¡åˆ’ ---")
    plan = state.get("tool_plan", {})
    if plan:
        console.print(json.dumps(plan, ensure_ascii=False, indent=2))
    console.print("--- ### æ‰§è¡Œç»“æœ ---")
    results = state.get("tool_results", {})
    if results:
        console.print(json.dumps(results, ensure_ascii=False, indent=2))
    console.print("--- ### æœ€ç»ˆå›ç­” ---")
    final_ans = state.get("final_answer", {}).get("answer", "")
    if final_ans:
        console.print(final_ans)

def parse_args() -> argparse.Namespace:
    """å‘½ä»¤è¡Œå‚æ•°è§£æï¼šæ”¯æŒè¯·æ±‚æ–‡æœ¬ä¸æ•™å­¦æ—¥å¿—å¼€å…³"""
    parser = argparse.ArgumentParser(description="å·¥å…·ä½¿ç”¨ï¼ˆTool Useï¼‰æ¶æ„ï¼šè§„åˆ’â†’æ‰§è¡Œâ†’æ±‡æ€»çš„å¯è¿è¡Œè„šæœ¬")
    parser.add_argument(
        "--request",
        type=str,
        default="è¯·å¯¹è¿™æ®µè¯åšç®€å•æ–‡æœ¬ç®¡çº¿ï¼š'LangGraph makes it easier to build stateful AI workflows.' æ ‡å‡†åŒ–ã€åˆ†è¯ã€æå–5ä¸ªå…³é”®è¯ï¼Œæœ€åç»“åˆå½“å‰æ—¶é—´æ¸²æŸ“ä¸º Markdown æŠ¥å‘Šã€‚",
        help="ç”¨æˆ·è¯·æ±‚ï¼ˆå»ºè®®å†™æ˜æ–‡æœ¬ä¸ç®€åŒ–çš„ç®¡çº¿æ­¥éª¤ï¼‰",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¼€å¯è¯¦ç»†æ—¥å¿—",
    )
    parser.add_argument(
        "--stream",
        action="store_true",
        help="å®æ—¶æ‰“å°æ¨¡å‹ä»¤ç‰Œï¼ˆç»“æ„åŒ– JSON ä¹Ÿå°†è¾¹æ¥æ”¶è¾¹å±•ç¤ºï¼‰",
    )
    parser.add_argument(
        "--max-steps",
        type=int,
        default=10,
        help="é™åˆ¶è§„åˆ’å¯æ‰§è¡Œçš„æœ€å¤§æ­¥éª¤æ•°ï¼ˆè¿‡é•¿çš„è®¡åˆ’å°†è¢«æˆªæ–­ï¼‰",
    )
    parser.add_argument(
        "--on-error",
        type=str,
        choices=["stop", "skip", "fallback"],
        default="skip",
        help="æ‰§è¡Œé˜¶æ®µçš„é”™è¯¯ç­–ç•¥ï¼šstop=åœæ­¢ï¼Œskip=è·³è¿‡ç»§ç»­ï¼Œfallback=ç»§ç»­å¹¶ç”±æ±‡æ€»è¯´æ˜",
    )
    return parser.parse_args()

def main():
    """è„šæœ¬å…¥å£ï¼šåˆå§‹åŒ–ç¯å¢ƒå˜é‡ä¸ LLMï¼Œæ„å»ºå·¥ä½œæµå¹¶è¿è¡Œ"""
    load_dotenv()
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_PROJECT"] = "Agentic Architecture - Tool Use (ModelScope)"
    args = parse_args()
    global DEBUG
    DEBUG = bool(args.debug)
    global STREAM_TOKENS
    STREAM_TOKENS = bool(args.stream)
    global MAX_STEPS, ON_ERROR
    try:
        MAX_STEPS = int(args.max_steps)
    except Exception:
        MAX_STEPS = 10
    ON_ERROR = str(args.on_error or "skip")
    if not os.environ.get("MODELSCOPE_API_KEY"):
        console.print("[bold red]MODELSCOPE_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨é¡¹ç›®æ ¹ç›®å½•é…ç½® .env[/bold red]")
    if not os.environ.get("LANGCHAIN_API_KEY"):
        console.print("[bold yellow]æç¤ºï¼šæœªè®¾ç½® LANGCHAIN_API_KEYï¼ŒLangSmith è¿½è¸ªå°†ä¸å¯ç”¨[/bold yellow]")
    llm = init_llm()
    if DEBUG:
        console.print("[bold cyan]æ¨ç†æœåŠ¡é…ç½®[/bold cyan]:")
        console.print(f"base_url={llm.base_url}")
        console.print(f"model_id={llm.model}")
    app = build_app(llm)
    final_state = run_workflow(app, args.request)
    print_outputs(final_state)

if __name__ == "__main__":
    main()
