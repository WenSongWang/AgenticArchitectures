# -*- coding: utf-8 -*-
"""
åæ€ï¼ˆReflectionï¼‰æ¶æ„çš„å¯è¿è¡Œç¤ºä¾‹
 
å­¦ä¹ ç›®æ ‡ï¼ˆçœ‹å®Œä½ èƒ½åšåˆ°ä»€ä¹ˆï¼‰ï¼š
- è¯»æ‡‚ä»€ä¹ˆæ˜¯â€œåæ€å¼â€æ™ºèƒ½ä½“ï¼šå…ˆç”Ÿæˆï¼Œå†è¯„å®¡ï¼Œæœ€åæ”¹å†™ï¼Œä½¿ç­”æ¡ˆæ›´å¯é 
- ç†è§£ LangGraph å¦‚ä½•æŠŠå¤šæ­¥é€»è¾‘ç¼–æ’æˆâ€œæœ‰çŠ¶æ€çš„å·¥ä½œæµâ€
- å­¦ä¼šç”¨ Pydantic v2 çº¦æŸ LLM è¾“å‡ºä¸ºç»“æ„åŒ–æ•°æ®ï¼ˆæ›´ç¨³ã€æ›´å¥½ç”¨ï¼‰
- èƒ½æŠŠè„šæœ¬ä½œä¸ºå‘½ä»¤è¡Œç¨‹åºè¿è¡Œï¼Œå¹¶æŒ‰éœ€ä¿å­˜æ”¹å†™åçš„ä»£ç 
 
æ ¸å¿ƒæ¦‚å¿µé€Ÿè§ˆï¼š
- StateGraphï¼šæœ‰çŠ¶æ€çš„â€œæµç¨‹å›¾â€ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ªå‡½æ•°ï¼ŒèŠ‚ç‚¹ä¹‹é—´æŒ‰è¾¹è¿æ¥é¡ºåºæ‰§è¡Œ
- ç»“æ„åŒ–è¾“å‡ºï¼ˆwith_structured_outputï¼‰ï¼šè®© LLM æ ¹æ®æˆ‘ä»¬å®šä¹‰çš„â€œæ•°æ®æ¨¡å‹â€åå‡º JSONï¼Œå†è‡ªåŠ¨è§£ææˆ Python å¯¹è±¡
- ä¸‰æ­¥èŠ‚ç‚¹ï¼šç”Ÿæˆï¼ˆGeneratorï¼‰â†’ è¯„å®¡ï¼ˆCriticï¼‰â†’ æ”¹å†™ï¼ˆRefinerï¼‰
 
è¿è¡Œå‰å‡†å¤‡ï¼š
- é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` å¹¶é…ç½®ï¼š
  - `LANGCHAIN_API_KEY`ï¼ˆç”¨äº LangSmith è¿½è¸ªï¼Œå¯é€‰ï¼‰
  - å¦‚ä½¿ç”¨ ModelScope æ¥å…¥ï¼š`MODELSCOPE_BASE_URL`ã€`MODELSCOPE_API_KEY`ã€`MODELSCOPE_MODEL_ID`
 
å¦‚ä½•è¿è¡Œï¼š
- ç›´æ¥è¿è¡Œé»˜è®¤ç¤ºä¾‹ï¼š`python 01_reflection.py`
- æ›´æ¢ä»»åŠ¡è¯·æ±‚ï¼š`python 01_reflection.py --request "Write a Python function to sort a list."`
- ä¿å­˜æ”¹å†™åçš„ä»£ç ï¼š`python 01_reflection.py --save-refined refined.py`
 
é˜…è¯»å»ºè®®ï¼š
- å…ˆä»â€œæ•°æ®æ¨¡å‹â€å’Œâ€œä¸‰ä¸ªèŠ‚ç‚¹å‡½æ•°â€å¼€å§‹ç†è§£ï¼Œå†çœ‹â€œbuild_appâ€å’Œâ€œrun_workflowâ€å¦‚ä½•æŠŠæ‰€æœ‰éƒ¨ä»¶ä¸²èµ·æ¥
"""

import os
import json                                                                           
import argparse
from typing import List, TypedDict, Optional

from dotenv import load_dotenv

from pydantic import BaseModel, Field

# LangGraph
from langgraph.graph import StateGraph, END

# æ§åˆ¶å°ç¾åŒ–
from rich.console import Console
from rich.syntax import Syntax
 
from openai import OpenAI
import logging
from rich.logging import RichHandler

# =========================
# 1) æ•°æ®ç»“æ„ä¸æ¨¡å‹å®šä¹‰ï¼ˆPydantic v2ï¼‰
# =========================
class DraftCode(BaseModel):
    """åˆç¨¿ï¼šç”Ÿæˆçš„ä»£ç ä¸ç®€è¦è¯´æ˜"""
    code: str = Field(description="ä¸ºç”¨æˆ·è¯·æ±‚ç”Ÿæˆçš„ Python ä»£ç ")
    explanation: str = Field(description="ä»£ç å·¥ä½œåŸç†çš„ç®€è¦è¯´æ˜")


class Critique(BaseModel):
    """è¯„å®¡ï¼šç»“æ„åŒ–çš„ä»£ç æ‰¹åˆ¤ä¸æ”¹è¿›å»ºè®®"""
    has_errors: bool = Field(description="æ˜¯å¦å­˜åœ¨æ½œåœ¨é”™è¯¯æˆ–é€»è¾‘é—®é¢˜")
    is_efficient: bool = Field(description="å®ç°æ˜¯å¦é«˜æ•ˆã€æ˜¯å¦ç¬¦åˆæœ€ä½³å®è·µ")
    suggested_improvements: List[str] = Field(description="å¯æ‰§è¡Œçš„ã€å…·ä½“çš„æ”¹è¿›å»ºè®®")
    critique_summary: str = Field(description="è¯„å®¡æ‘˜è¦")


class RefinedCode(BaseModel):
    """æ”¹å†™ï¼šåŸºäºè¯„å®¡æ„è§çš„æœ€ç»ˆç‰ˆæœ¬ä»£ç """
    refined_code: str = Field(description="æ”¹è¿›åçš„æœ€ç»ˆ Python ä»£ç ")
    refinement_summary: str = Field(description="æ ¹æ®è¯„å®¡æ‰€åšæ”¹åŠ¨çš„è¯´æ˜")


class ReflectionState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€ï¼šåœ¨èŠ‚ç‚¹é—´ä¼ é€’çš„å…±äº«æ•°æ®"""
    # ç”¨æˆ·çš„åŸå§‹è¯·æ±‚æ–‡æœ¬ï¼Œç”¨äºé©±åŠ¨â€œç”Ÿæˆåˆç¨¿â€èŠ‚ç‚¹
    user_request: str
    # â€œç”Ÿæˆåˆç¨¿â€èŠ‚ç‚¹è¾“å‡ºçš„ç»“æ„åŒ–ç»“æœï¼Œé€šå¸¸åŒ…å« code ä¸ explanation
    draft: Optional[dict]
    # â€œè¯„å®¡â€èŠ‚ç‚¹è¾“å‡ºçš„ç»“æ„åŒ–è¯„å®¡ä¿¡æ¯ï¼ˆæ˜¯å¦æœ‰é”™è¯¯ã€æ•ˆç‡ã€å»ºè®®ã€æ‘˜è¦ï¼‰
    critique: Optional[dict]
    # â€œæ”¹å†™â€èŠ‚ç‚¹è¾“å‡ºçš„æœ€ç»ˆä»£ç ä¸æ”¹åŠ¨æ‘˜è¦ï¼ˆrefined_code ä¸ refinement_summaryï¼‰
    refined_code: Optional[dict]


# =========================
# 2) LLM ä¸æ§åˆ¶å°åˆå§‹åŒ–
# =========================
console = Console()
DEBUG: bool = True  # é€šè¿‡ --debug å¼€å¯æ›´è¯¦ç»†çš„æ—¥å¿—
logger = logging.getLogger("reflection")
handler = RichHandler(console=console, rich_tracebacks=True, markup=True)
handler.setFormatter(logging.Formatter("%(message)s"))
logger.handlers = [handler]
logger.propagate = False
logger.setLevel(logging.DEBUG if DEBUG else logging.INFO)

class ModelScopeChat:
    """
    ModelScope çš„ OpenAI å…¼å®¹æ¥å£é€‚é…å™¨ï¼š
    - æä¾› invoke(prompt) åŸºæœ¬è°ƒç”¨
    - æä¾› with_structured_output(PydanticModel) çš„ç»“æ„åŒ–è¾“å‡ºåŒ…è£…
 
åˆå­¦è€…ç†è§£è¦ç‚¹ï¼š
- ä¸ºä»€ä¹ˆéœ€è¦â€œé€‚é…å™¨â€ï¼Ÿå› ä¸ºæˆ‘ä»¬çš„å·¥ä½œæµä¾èµ–â€œç»“æ„åŒ–è¾“å‡ºâ€ï¼Œè€Œä¸å°‘æœåŠ¡é»˜è®¤åªè¿”å›çº¯æ–‡æœ¬ã€‚
- é€‚é…å™¨ä¼šå°½é‡è¦æ±‚æ¨¡å‹â€œåªè¾“å‡º JSONâ€ï¼Œå†è§£æä¸º Pydantic v2 æ¨¡å‹ï¼›è¿™æ ·åç»­èŠ‚ç‚¹å°±èƒ½ç¨³ç¨³åœ°æ‹¿åˆ°å­—æ®µï¼Œè€Œä¸æ˜¯æ‚ä¹±çš„æ–‡æœ¬ã€‚
    """
    def __init__(self, base_url: str, api_key: str, model: str, temperature: float = 0.2, extra_body: Optional[dict] = None):
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model = model
        self.base_url = base_url
        self.temperature = temperature
        self.extra_body = extra_body or {}

    def invoke(self, prompt: str):
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
            stream=False,
            extra_body=self.extra_body,
        )
        # éæµå¼è¿”å›ï¼šchoices[0].message.content
        return resp.choices[0].message.content

    def with_structured_output(self, pyd_model: type[BaseModel]):
        class _StructuredWrapper:
            def __init__(self, outer: "ModelScopeChat"):
                self.outer = outer

            def invoke(self, prompt: str) -> BaseModel:
                # é€šè¿‡ç³»ç»Ÿæç¤ºçº¦æŸä»…è¾“å‡º JSONï¼ˆå°½é‡æé«˜è§£ææˆåŠŸç‡ï¼‰ï¼Œå¹¶æ˜ç¡®å­—æ®µ/ç±»å‹
                schema = pyd_model.model_json_schema()
                props = schema.get("properties", {})
                required = schema.get("required", [])
                schema_text_lines = []
                for k, v in props.items():
                    t = v.get("type", "string")
                    schema_text_lines.append(f"- {k}: {t}")
                schema_text = "\n".join(schema_text_lines) or "- è¯·æŒ‰æ¨¡å‹å®šä¹‰ç”Ÿæˆå­—æ®µ"
                required_text = ", ".join(required) if required else "æ‰€æœ‰å­—æ®µ"
                system_msg = (
                    "ä½ æ˜¯ä¸€ä¸ªç»“æ„åŒ–è¾“å‡ºç”Ÿæˆå™¨ã€‚åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸¥æ ¼åŒ¹é…ä»¥ä¸‹å­—æ®µä¸ç±»å‹ï¼š\n"
                    f"{schema_text}\n"
                    f"å¿…é¡»åŒ…å«å­—æ®µï¼š{required_text}\n"
                    "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæˆ–å¤šä½™æ–‡æœ¬ï¼ˆä¾‹å¦‚ä»£ç å—æ ‡è®°ã€å‰åç¼€ï¼‰ã€‚"
                )
                if DEBUG:
                    logger.debug("ğŸ”§ ç”ŸæˆåŠ¨æ€ç³»ç»Ÿæç¤ºï¼ˆåŒ…å«å­—æ®µä¸ç±»å‹è¦æ±‚ï¼‰")
                    logger.debug("ç»“æ„åŒ–è¾“å‡ºæç¤ºï¼ˆç³»ç»Ÿæ¶ˆæ¯ï¼‰ï¼š\n" + system_msg)
                messages = [
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt},
                ]
                resp = self.outer.client.chat.completions.create(
                    model=self.outer.model,
                    messages=messages,
                    temperature=self.outer.temperature,
                    stream=False,
                    extra_body=self.outer.extra_body,
                )
                content = resp.choices[0].message.content or ""
                import json, re
                from pydantic import ValidationError
                def _extract_json(s: str) -> str:
                    m = re.search(r'(\{[\s\S]*\}|\[[\s\S]*\])', s)
                    return m.group(1) if m else "{}"
                raw = content.strip()
                if DEBUG:
                    console.print("[bold blue]ğŸ“¥ æ”¶åˆ°æ¨¡å‹è¿”å›ï¼Œå°è¯•è§£æä¸º JSON[/bold blue]")
                try:
                    data = json.loads(raw)
                except Exception:
                    data = json.loads(_extract_json(raw))
                if DEBUG:
                    console.print("[bold cyan]æ¨¡å‹åŸå§‹è¿”å›ï¼ˆæˆªæ–­å±•ç¤ºï¼‰[/bold cyan]:")
                    preview = json.dumps(data, ensure_ascii=False)[:400]
                    console.print(preview + ("..." if len(preview) == 400 else ""))
                # å…œåº•å­—æ®µæ˜ å°„ï¼šå°½é‡æŠŠå¸¸è§åˆ«åæ˜ å°„åˆ°ç›®æ ‡æ¨¡å‹å­—æ®µ
                try:
                    parsed = pyd_model.model_validate(data)
                    if DEBUG:
                        console.print(f"[bold green]âœ… ç»“æ„åŒ–è§£ææˆåŠŸ[/bold green]ï¼š{pyd_model.__name__}")
                    return parsed
                except ValidationError:
                    if DEBUG:
                        console.print("[bold yellow]âš ï¸ å­—æ®µä¸åŒ¹é…ï¼Œå°è¯•è‡ªåŠ¨æ˜ å°„å¸¸è§åˆ«å[/bold yellow]")
                    mappings_applied = []
                    # DraftCode: å¸¸è§è¿”å› 'function' å­—æ®µï¼Œæ˜ å°„åˆ° 'code'
                    if "code" not in data and "function" in data:
                        data["code"] = data.pop("function")
                        mappings_applied.append("function â†’ code")
                    if "explanation" not in data and "desc" in data:
                        data["explanation"] = data.pop("desc")
                        mappings_applied.append("desc â†’ explanation")
                    # RefinedCode: å°† 'code' æ˜ å°„ä¸º 'refined_code'
                    if "refined_code" not in data and "code" in data:
                        data["refined_code"] = data.pop("code")
                        mappings_applied.append("code â†’ refined_code")
                    # Critique: å¯èƒ½ç»™ 'summary' æ˜ å°„ä¸º 'critique_summary'
                    if "critique_summary" not in data and "summary" in data:
                        data["critique_summary"] = data.pop("summary")
                        mappings_applied.append("summary â†’ critique_summary")
                    if DEBUG and mappings_applied:
                        console.print("[bold cyan]å·²åº”ç”¨å­—æ®µæ˜ å°„ï¼š[/bold cyan] " + ", ".join(mappings_applied))
                    parsed = pyd_model.model_validate(data)
                    if DEBUG:
                        console.print(f"[bold green]âœ… ç»“æ„åŒ–è§£ææˆåŠŸ[/bold green]ï¼š{pyd_model.__name__}")
                    return parsed

        return _StructuredWrapper(self)


def init_llm() -> ModelScopeChat:
    """
    åˆå§‹åŒ– ModelScope LLMï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰ã€‚
    - å¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼š
      MODELSCOPE_BASE_URLï¼ˆé»˜è®¤ï¼šhttps://api-inference.modelscope.cn/v1ï¼‰
      MODELSCOPE_API_KEY
      MODELSCOPE_MODEL_IDï¼ˆé»˜è®¤ï¼šdeepseek-ai/DeepSeek-V3.2ï¼‰
    - é¢å¤–å‚æ•°ï¼šenable_thinking å¯é€‰
 
å°è´´å£«ï¼š
- ç›´æ¥åœ¨æµè§ˆå™¨è®¿é—® Base URL è¿”å› 404 æ­£å¸¸ï¼Œå®¢æˆ·ç«¯ä¼šåœ¨æ­¤åŸºç¡€ä¸Šæ‹¼æ¥å…·ä½“è·¯å¾„ï¼ˆå¦‚ /chat/completionsï¼‰ã€‚
- è¯·æŠŠçœŸå®çš„ Token å†™åœ¨ `.env` ä¸­ï¼Œä¸è¦ç¡¬ç¼–ç åˆ°è„šæœ¬ã€‚
    """
    base_url = os.environ.get("MODELSCOPE_BASE_URL", "https://api-inference.modelscope.cn/v1")
    api_key = os.environ.get("MODELSCOPE_API_KEY", "")
    model_id = os.environ.get("MODELSCOPE_MODEL_ID", "deepseek-ai/DeepSeek-V3.2")
    # ä¸ºé¿å…â€œæœªä¿¡ä»»çš„ chat templateâ€é”™è¯¯ï¼Œå¢åŠ ä¿¡ä»»å‚æ•°ï¼›å¹¶è¯·æ±‚ JSON è¾“å‡ºæ ¼å¼
    extra = {
        "enable_thinking": True,
        "trust_request_chat_template": True,
        "response_format": {"type": "json_object"},
    }
    return ModelScopeChat(base_url=base_url, api_key=api_key, model=model_id, temperature=0.2, extra_body=extra)


# =========================
# 3) ä¸‰ä¸ªæ ¸å¿ƒèŠ‚ç‚¹ï¼šç”Ÿæˆâ†’è¯„å®¡â†’æ”¹å†™
# =========================
def make_generator_node(llm: "ModelScopeChat"):
    """ç”ŸæˆèŠ‚ç‚¹ï¼šæ ¹æ®ç”¨æˆ·è¯·æ±‚äº§å‡ºç¬¬ä¸€ç‰ˆä»£ç ï¼ˆç»“æ„åŒ–è¾“å‡ºä¸º DraftCodeï¼‰"""
    generator_llm = llm.with_structured_output(DraftCode)

    def _node(state: ReflectionState) -> dict:
        console.print("--- 1. ç”Ÿæˆåˆç¨¿ ---")
        if DEBUG:
            console.print(f"[bold]è¾“å…¥è¯·æ±‚ï¼š[/bold]{state['user_request']}")
        prompt = f"""ä½ æ˜¯èµ„æ·± Python ç¨‹åºå‘˜ã€‚è¯·ä¸ºä¸‹é¢çš„è¯·æ±‚ç¼–å†™å‡½æ•°ï¼Œå¹¶ç»™å‡ºç®€è¦è¯´æ˜ï¼š

è¯·æ±‚ï¼š{state['user_request']}
"""
        draft = generator_llm.invoke(prompt)
        if DEBUG:
            console.print("[bold green]ç”Ÿæˆåˆç¨¿å®Œæˆï¼ˆç»“æ„åŒ–ï¼‰[/bold green]")
        return {"draft": draft.model_dump()}

    return _node


def make_critic_node(llm: "ModelScopeChat"):
    """è¯„å®¡èŠ‚ç‚¹ï¼šé’ˆå¯¹åˆç¨¿è¿›è¡Œç»“æ„åŒ–è¯„å®¡ï¼ˆç»“æ„åŒ–è¾“å‡ºä¸º Critiqueï¼‰"""
    critic_llm = llm.with_structured_output(Critique)

    def _node(state: ReflectionState) -> dict:
        console.print("--- 2. è¯„å®¡åˆç¨¿ ---")
        if DEBUG:
            console.print("[bold]å¾…è¯„å®¡ä»£ç ç‰‡æ®µï¼ˆå‰120å­—ç¬¦ï¼‰ï¼š[/bold]")
            console.print((state["draft"]["code"] or "")[:120] + "...")
        code_to_critique = state["draft"]["code"]
        prompt = f"""ä½ æ˜¯ä¸€åèµ„æ·± Python ä»£ç å®¡é˜…è€…ã€‚è¯·å¯¹ä¸‹æ–¹ä»£ç è¿›è¡Œä¸¥æ ¼è¯„å®¡ï¼Œå¹¶è¾“å‡ºç»“æ„åŒ–å»ºè®®ï¼š
1) æ˜¯å¦å­˜åœ¨æ½œåœ¨é”™è¯¯æˆ–æœªè¦†ç›–çš„è¾¹ç•Œï¼Ÿ
2) æ˜¯å¦éµå¾ªæœ€ä½³å®è·µå¹¶å…·å¤‡é«˜æ•ˆç‡ï¼Ÿ
3) ç»™å‡ºå¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®®æ¸…å•ã€‚

ä»£ç ï¼š
```python
{code_to_critique}
```
"""
        critique = critic_llm.invoke(prompt)
        if DEBUG:
            console.print("[bold green]è¯„å®¡å®Œæˆï¼ˆç»“æ„åŒ–ï¼‰[/bold green]")
        return {"critique": critique.model_dump()}

    return _node


def make_refiner_node(llm: "ModelScopeChat"):
    """æ”¹å†™èŠ‚ç‚¹ï¼šç»“åˆè¯„å®¡å»ºè®®é‡å†™ä»£ç ï¼ˆç»“æ„åŒ–è¾“å‡ºä¸º RefinedCodeï¼‰"""
    refiner_llm = llm.with_structured_output(RefinedCode)

    def _node(state: ReflectionState) -> dict:
        console.print("--- 3. æ”¹å†™ä»£ç  ---")
        if DEBUG:
            console.print("[bold]å°†æ ¹æ®ä»¥ä¸‹è¯„å®¡å»ºè®®è¿›è¡Œæ”¹å†™ï¼ˆå­—æ®µè§†å›¾ï¼‰ï¼š[/bold]")
            console.print(list(state["critique"].keys()))
        draft_code = state["draft"]["code"]
        critique_suggestions = json.dumps(state["critique"], ensure_ascii=False, indent=2)
        prompt = f"""ä½ æ˜¯ä¸€åèµ„æ·± Python ç¨‹åºå‘˜ã€‚è¯·åŸºäºè¯„å®¡æ„è§é‡å†™ä»£ç ï¼š

åŸå§‹ä»£ç ï¼š
```python
{draft_code}
```

è¯„å®¡å»ºè®®ï¼ˆç»“æ„åŒ–ï¼‰ï¼š
{critique_suggestions}

è¯·è¾“å‡ºæœ€ç»ˆæ”¹è¿›åçš„ä»£ç ä¸æ”¹åŠ¨æ‘˜è¦ã€‚
"""
        refined = refiner_llm.invoke(prompt)
        if DEBUG:
            console.print("[bold green]æ”¹å†™å®Œæˆï¼ˆç»“æ„åŒ–ï¼‰[/bold green]")
        return {"refined_code": refined.model_dump()}

    return _node


# =========================
# 4) æ„å»ºä¸è¿è¡Œ LangGraph å·¥ä½œæµ
# =========================
def build_app(llm: "ModelScopeChat"):
    """æ„å»ºçº¿æ€§å·¥ä½œæµï¼šentry â†’ generator â†’ critic â†’ refiner â†’ END"""
    graph_builder = StateGraph(ReflectionState)  # åˆ›å»ºâ€œæµç¨‹å›¾â€æ„å»ºå™¨ï¼Œå¹¶å£°æ˜çŠ¶æ€æ•°æ®çš„ç±»å‹

    graph_builder.add_node("generator", make_generator_node(llm))  # ç¬¬ 1 æ­¥ï¼šç”Ÿæˆåˆç¨¿ï¼ˆDraftï¼‰
    graph_builder.add_node("critic", make_critic_node(llm))        # ç¬¬ 2 æ­¥ï¼šè¯„å®¡åˆç¨¿ï¼ˆCritiqueï¼‰
    graph_builder.add_node("refiner", make_refiner_node(llm))      # ç¬¬ 3 æ­¥ï¼šæ”¹å†™ä»£ç ï¼ˆRefinedï¼‰

    graph_builder.set_entry_point("generator")      # æŒ‡å®šå…¥å£èŠ‚ç‚¹ä¸ºâ€œç”Ÿæˆåˆç¨¿â€
    graph_builder.add_edge("generator", "critic")   # æµç¨‹ï¼šç”Ÿæˆ â†’ è¯„å®¡
    graph_builder.add_edge("critic", "refiner")     # æµç¨‹ï¼šè¯„å®¡ â†’ æ”¹å†™
    graph_builder.add_edge("refiner", END)          # æµç¨‹ï¼šæ”¹å†™ â†’ ç»“æŸï¼ˆè¿”å›æœ€ç»ˆç»“æœï¼‰
    if DEBUG:
        console.print("[bold cyan]å·¥ä½œæµç¼–æ’ï¼š[/bold cyan] generator â†’ critic â†’ refiner â†’ END")

    return graph_builder.compile()  # ç¼–è¯‘æˆå¯æ‰§è¡Œçš„â€œåº”ç”¨â€ï¼Œä¾› run_workflow è°ƒç”¨


def run_workflow(app, user_request: str) -> ReflectionState:
    """æ‰§è¡Œå·¥ä½œæµå¹¶è¿”å›æœ€ç»ˆçŠ¶æ€"""
    initial_input = {"user_request": user_request}
    console.print(f"[bold cyan]ğŸš€ å¯åŠ¨åæ€å·¥ä½œæµï¼š[/bold cyan] '{user_request}'")
    final_state: Optional[ReflectionState] = None
    step = 0
    for state_update in app.stream(initial_input, stream_mode="values"):
        final_state = state_update  # æµå¼ç´¯ç§¯åˆ°æœ€ç»ˆçŠ¶æ€
        step += 1
        if DEBUG:
            console.print(f"[bold]æ­¥éª¤ {step}[/bold] å½“å‰çŠ¶æ€å­—æ®µï¼š{list(state_update.keys())}")
    console.print("[bold green]âœ… å·¥ä½œæµå®Œæˆ[/bold green]")
    return final_state or initial_input  # å…œåº•


# =========================
# 5) è¾“å‡ºè¾…åŠ©ï¼šæ‰“å°å‰åå¯¹æ¯”
# =========================
def print_before_after(state: ReflectionState) -> None:
    """æ‰“å°åˆç¨¿/è¯„å®¡/æ”¹å†™åçš„ä»£ç ï¼Œä¾¿äºç›´è§‚å¯¹æ¯”"""
    console.print("--- ### åˆç¨¿ï¼ˆDraftï¼‰ ---")
    explanation = state.get("draft", {}).get("explanation", "")
    if explanation:
        console.print(f"[bold]è¯´æ˜ï¼š[/bold]{explanation}")
    code = state.get("draft", {}).get("code", "")
    if code:
        console.print(Syntax(code, "python", theme="monokai", line_numbers=True))

    console.print("--- ### è¯„å®¡ï¼ˆCritiqueï¼‰ ---")
    critique = state.get("critique", {})
    if critique:
        console.print(json.dumps(critique, ensure_ascii=False, indent=2))

    console.print("--- ### æ”¹å†™åï¼ˆRefinedï¼‰ ---")
    refined = state.get("refined_code", {}).get("refined_code", "")
    if refined:
        console.print(Syntax(refined, "python", theme="monokai", line_numbers=True))


# =========================
# 6) CLI ä¸å…¥å£
# =========================
def parse_args() -> argparse.Namespace:
    """å‘½ä»¤è¡Œå‚æ•°è§£æ"""
    parser = argparse.ArgumentParser(
        description="åæ€ï¼ˆReflectionï¼‰æ¶æ„ï¼šç”Ÿæˆâ†’è¯„å®¡â†’æ”¹å†™çš„å¯è¿è¡Œè„šæœ¬"
    )
    parser.add_argument(
        "--request",
        type=str,
        default="Write a Python function to find the nth Fibonacci number.",
        help="ç”¨æˆ·è¯·æ±‚ï¼ˆé»˜è®¤ï¼šæ–æ³¢é‚£å¥‘å‡½æ•°ï¼‰",
    )
    parser.add_argument(
        "--save-refined",
        type=str,
        default="",
        help="å°†æ”¹å†™åçš„ä»£ç ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¼€å¯è¯¦ç»†æ•™å­¦æ—¥å¿—ï¼ˆç»“æ„åŒ–æç¤ºã€çŠ¶æ€å˜åŒ–ã€ç‰‡æ®µé¢„è§ˆï¼‰",
    )
    return parser.parse_args()


def main():
    load_dotenv()
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_PROJECT"] = "Agentic Architecture - Reflection (ModelScope)"

    if not os.environ.get("MODELSCOPE_API_KEY"):
        console.print("[bold red]MODELSCOPE_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨é¡¹ç›®æ ¹ç›®å½•é…ç½® .env[/bold red]")
    if not os.environ.get("LANGCHAIN_API_KEY"):
        console.print("[bold yellow]æç¤ºï¼šæœªè®¾ç½® LANGCHAIN_API_KEYï¼ŒLangSmith è¿½è¸ªå°†ä¸å¯ç”¨[/bold yellow]")

    args = parse_args()
    global DEBUG
    DEBUG = bool(args.debug)
    llm = init_llm()
    if DEBUG:
        console.print("[bold cyan]æ¨ç†æœåŠ¡é…ç½®[/bold cyan]:")
        console.print(f"base_url={llm.base_url}")
        console.print(f"model_id={llm.model}")
    app = build_app(llm)

    final_state = run_workflow(app, args.request)
    print_before_after(final_state)

    # å¯é€‰ï¼šä¿å­˜æ”¹å†™åçš„ä»£ç åˆ°æ–‡ä»¶
    if args.save_refined:
        refined_code = final_state.get("refined_code", {}).get("refined_code", "")
        if refined_code:
            with open(args.save_refined, "w", encoding="utf-8") as f:
                f.write(refined_code)
            console.print(f"[bold green]å·²ä¿å­˜æ”¹å†™ä»£ç è‡³ï¼š[/bold green]{args.save_refined}")
        else:
            console.print("[bold red]æœªæ‰¾åˆ°æ”¹å†™åçš„ä»£ç ï¼Œä¿å­˜å¤±è´¥[/bold red]")


if __name__ == "__main__":
    main()

